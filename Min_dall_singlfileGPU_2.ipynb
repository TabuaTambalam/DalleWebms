{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "「Min_dall_singlfileGPU.ipynb」的副本",
      "provenance": [],
      "collapsed_sections": [
        "7w_GA--q5BAL"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Min-dalle classes"
      ],
      "metadata": {
        "id": "7w_GA--q5BAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if not os.path.isfile('/content/once.txt'):\n",
        "  !pip install accelerate"
      ],
      "metadata": {
        "id": "F0tKk2pj68F1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d89a0789-a240-4044-e0fa-46a444fb8bd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.10.0-py3-none-any.whl (117 kB)\n",
            "\u001b[K     |████████████████████████████████| 117 kB 22.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from accelerate) (3.13)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from accelerate) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.21.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from accelerate) (5.4.8)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.12.0+cu113)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->accelerate) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->accelerate) (4.1.1)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/once.txt\n",
        "uwa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksJNr4attN92",
        "outputId": "2719f3bf-d45c-4c9d-c271-7e9e07c5ca00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/once.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_TOKEN_COUNT = 256\n",
        "ROW_STA=0\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from threading import Thread\n",
        "\n",
        "import torch\n",
        "from torch import LongTensor, FloatTensor,nn, BoolTensor\n",
        "from math import sqrt\n",
        "\n",
        "import torch.backends.cudnn, torch.backends.cuda\n",
        "import json\n",
        "import requests\n",
        "from typing import Iterator,List, Tuple, Dict\n",
        "from math import inf\n",
        "\n",
        "\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "torch.set_num_threads(os.cpu_count())\n",
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True\n",
        "\n",
        "MIN_DALLE_REPO = 'https://huggingface.co/kuprel/min-dalle/resolve/main/'\n",
        "\n",
        "\n",
        "BLANK_TOKEN = 6965"
      ],
      "metadata": {
        "id": "b622jYml4NI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ber encoder"
      ],
      "metadata": {
        "id": "2FQOIADUd6Kl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GLU(nn.Module):\n",
        "    def __init__(self, count_in_out, count_middle):\n",
        "        super().__init__()\n",
        "        self.gelu = nn.GELU()\n",
        "        self.ln0 = nn.LayerNorm(count_in_out)\n",
        "        self.ln1 = nn.LayerNorm(count_middle)\n",
        "        self.fc0 = nn.Linear(count_in_out, count_middle, bias=False)\n",
        "        self.fc1 = nn.Linear(count_in_out, count_middle, bias=False)\n",
        "        self.fc2 = nn.Linear(count_middle, count_in_out, bias=False)\n",
        "    \n",
        "    def forward(self, z: FloatTensor) -> FloatTensor:\n",
        "        z = self.ln0.forward(z)\n",
        "        w = self.fc0.forward(z)\n",
        "        w = self.gelu.forward(w)\n",
        "        v = self.fc1.forward(z)\n",
        "        z = self.ln1.forward(w * v)\n",
        "        z = self.fc2.forward(z)\n",
        "        return z\n",
        "\n",
        "\n",
        "class AttentionBase(nn.Module):\n",
        "    def __init__(self, head_count: int, embed_count: int):\n",
        "        super().__init__()\n",
        "        self.head_count = head_count\n",
        "        self.embed_count = embed_count\n",
        "\n",
        "        self.k_proj = nn.Linear(embed_count, embed_count, bias=False)\n",
        "        self.v_proj = nn.Linear(embed_count, embed_count, bias=False)\n",
        "        self.q_proj = nn.Linear(embed_count, embed_count, bias=False)\n",
        "        self.out_proj = nn.Linear(embed_count, embed_count, bias=False)\n",
        "        self.one = torch.ones((1, 1))\n",
        "        if torch.cuda.is_available(): self.one = self.one.cuda()\n",
        "    \n",
        "    def forward(\n",
        "        self,\n",
        "        keys: FloatTensor,\n",
        "        values: FloatTensor,\n",
        "        queries: FloatTensor,\n",
        "        attention_mask: BoolTensor\n",
        "    ) -> FloatTensor:\n",
        "        keys = keys.reshape(keys.shape[:2] + (self.head_count, -1))\n",
        "        values = values.reshape(values.shape[:2] + (self.head_count, -1))\n",
        "        queries = queries.reshape(queries.shape[:2] + (self.head_count, -1))\n",
        "        queries /= queries.shape[-1] ** 0.5\n",
        "\n",
        "        attention_bias = torch.where(\n",
        "            attention_mask,\n",
        "            self.one * 0,\n",
        "            self.one * (-torch.inf),\n",
        "        )\n",
        "        attention_weights: FloatTensor = torch.einsum(\n",
        "            'bqhc,bkhc->bhqk',\n",
        "            queries, \n",
        "            keys\n",
        "        )\n",
        "        attention_weights += attention_bias[:, None, None, :]\n",
        "        attention_weights = torch.softmax(attention_weights, -1)\n",
        "        attention_output: FloatTensor = torch.einsum(\n",
        "            \"bhqk,bkhc->bqhc\",\n",
        "            attention_weights, \n",
        "            values\n",
        "        )\n",
        "        shape = attention_output.shape[:2] + (self.embed_count,)\n",
        "        attention_output = attention_output.reshape(shape)\n",
        "        attention_output = self.out_proj.forward(attention_output)\n",
        "        return attention_output\n",
        "\n",
        "\n",
        "class EncoderSelfAttention(AttentionBase):\n",
        "    def forward(\n",
        "        self,\n",
        "        encoder_state: FloatTensor,\n",
        "        attention_mask: BoolTensor\n",
        "    ) -> FloatTensor:\n",
        "        keys = self.k_proj.forward(encoder_state)\n",
        "        values = self.v_proj.forward(encoder_state)\n",
        "        queries = self.q_proj.forward(encoder_state)\n",
        "        return super().forward(keys, values, queries, attention_mask)\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, embed_count: int, head_count: int, glu_embed_count: int):\n",
        "        super().__init__()\n",
        "        self.pre_self_attn_layer_norm = nn.LayerNorm(embed_count)\n",
        "        self.self_attn = EncoderSelfAttention(head_count, embed_count)\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(embed_count)\n",
        "        self.glu = GLU(embed_count, glu_embed_count)\n",
        "    \n",
        "    def forward(\n",
        "        self,\n",
        "        encoder_state: FloatTensor,\n",
        "        attention_mask: BoolTensor\n",
        "    ) -> FloatTensor:\n",
        "        residual = encoder_state\n",
        "        encoder_state = self.pre_self_attn_layer_norm.forward(encoder_state)\n",
        "        encoder_state = self.self_attn.forward(encoder_state, attention_mask)\n",
        "        encoder_state = self.self_attn_layer_norm.forward(encoder_state)\n",
        "        encoder_state = residual + encoder_state\n",
        "        residual = encoder_state\n",
        "        encoder_state = self.glu.forward(encoder_state)\n",
        "        encoder_state = residual + encoder_state\n",
        "        return encoder_state\n",
        "\n",
        "\n",
        "class DalleBartEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        layer_count: int,\n",
        "        embed_count: int,\n",
        "        attention_head_count: int,\n",
        "        text_vocab_count: int,\n",
        "        text_token_count: int,\n",
        "        glu_embed_count: int\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.text_vocab_count = text_vocab_count\n",
        "        self.embed_tokens = nn.Embedding(text_vocab_count, embed_count)\n",
        "        self.embed_positions = nn.Embedding(text_token_count, embed_count)\n",
        "        self.layers: List[EncoderLayer] = nn.ModuleList([\n",
        "            EncoderLayer(\n",
        "                embed_count = embed_count,\n",
        "                head_count = attention_head_count,\n",
        "                glu_embed_count = glu_embed_count\n",
        "            ) \n",
        "            for _ in range(layer_count)\n",
        "        ])\n",
        "        self.layernorm_embedding = nn.LayerNorm(embed_count)\n",
        "        self.final_ln = nn.LayerNorm(embed_count)\n",
        "        self.token_indices = torch.arange(text_token_count).to(torch.long)\n",
        "        if torch.cuda.is_available(): \n",
        "            self.token_indices = self.token_indices.cuda()\n",
        "\n",
        "    def forward(self, text_tokens: LongTensor) -> FloatTensor:\n",
        "        attention_mask = text_tokens.not_equal(1)\n",
        "        pose_tokens = self.token_indices[None][[0] * text_tokens.shape[0]]\n",
        "        text_tokens.clamp_(0, self.text_vocab_count - 1)\n",
        "        encoder_state = (\n",
        "            self.embed_tokens.forward(text_tokens) +\n",
        "            self.embed_positions.forward(pose_tokens)\n",
        "        )\n",
        "        encoder_state = self.layernorm_embedding.forward(encoder_state)\n",
        "        for layer in self.layers:\n",
        "            encoder_state = layer.forward(encoder_state, attention_mask)\n",
        "        encoder_state = self.final_ln.forward(encoder_state)\n",
        "        return encoder_state"
      ],
      "metadata": {
        "id": "p70me-854qoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bert Decoder"
      ],
      "metadata": {
        "id": "By5WhGVFd2Ht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderCrossAttention(AttentionBase):\n",
        "    def forward(\n",
        "        self,\n",
        "        decoder_state: FloatTensor,\n",
        "        encoder_state: FloatTensor,\n",
        "        attention_mask: BoolTensor\n",
        "    ) -> FloatTensor:\n",
        "        keys = self.k_proj.forward(encoder_state)\n",
        "        values = self.v_proj.forward(encoder_state)\n",
        "        queries = self.q_proj.forward(decoder_state)\n",
        "        return super().forward(keys, values, queries, attention_mask)\n",
        "\n",
        "\n",
        "class DecoderSelfAttention(AttentionBase):\n",
        "    def __init__(self, head_count: int, embed_count: int):\n",
        "        super().__init__(head_count, embed_count)\n",
        "        token_indices = torch.arange(IMAGE_TOKEN_COUNT)\n",
        "        if torch.cuda.is_available(): token_indices = token_indices.cuda()\n",
        "        self.token_indices = token_indices\n",
        "\n",
        "    def forward(\n",
        "        self, \n",
        "        decoder_state: FloatTensor,\n",
        "        attention_state: FloatTensor,\n",
        "        token_index: LongTensor\n",
        "    ) -> Tuple[FloatTensor, FloatTensor]:\n",
        "        keys = self.k_proj.forward(decoder_state)\n",
        "        values = self.v_proj.forward(decoder_state)\n",
        "        queries = self.q_proj.forward(decoder_state)\n",
        "        attn_mask = self.token_indices < token_index + 1\n",
        "        attn_mask = attn_mask[None][[0] * decoder_state.shape[0]]\n",
        "        attn_state_new = torch.cat([keys, values]).to(attention_state.dtype)\n",
        "        attention_state[:, token_index] = attn_state_new\n",
        "        batch_count = decoder_state.shape[0]\n",
        "        keys = attention_state[:batch_count]\n",
        "        values = attention_state[batch_count:]\n",
        "        decoder_state = super().forward(keys, values, queries, attn_mask)\n",
        "        return decoder_state, attention_state\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        head_count: int, \n",
        "        embed_count: int,\n",
        "        glu_embed_count: int\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.pre_self_attn_layer_norm = nn.LayerNorm(embed_count)\n",
        "        self.self_attn = DecoderSelfAttention(head_count, embed_count)\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(embed_count)\n",
        "        self.pre_encoder_attn_layer_norm = nn.LayerNorm(embed_count)\n",
        "        self.encoder_attn = DecoderCrossAttention(head_count, embed_count)\n",
        "        self.encoder_attn_layer_norm = nn.LayerNorm(embed_count)\n",
        "        self.glu = GLU(embed_count, glu_embed_count)\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        decoder_state: FloatTensor,\n",
        "        encoder_state: FloatTensor,\n",
        "        attention_state: FloatTensor,\n",
        "        attention_mask: BoolTensor,\n",
        "        token_index: LongTensor\n",
        "    ) -> Tuple[FloatTensor, FloatTensor]:\n",
        "        # Self Attention\n",
        "        residual = decoder_state\n",
        "        decoder_state = self.pre_self_attn_layer_norm.forward(decoder_state)\n",
        "        decoder_state, attention_state = self.self_attn.forward(\n",
        "            decoder_state,\n",
        "            attention_state,\n",
        "            token_index\n",
        "        )\n",
        "        decoder_state = self.self_attn_layer_norm.forward(decoder_state)\n",
        "        decoder_state = residual + decoder_state\n",
        "\n",
        "        # Cross Attention\n",
        "        residual = decoder_state\n",
        "        decoder_state = self.pre_encoder_attn_layer_norm.forward(decoder_state)\n",
        "        decoder_state = self.encoder_attn.forward(\n",
        "            decoder_state,\n",
        "            encoder_state,\n",
        "            attention_mask\n",
        "        )\n",
        "        decoder_state = self.encoder_attn_layer_norm.forward(decoder_state)\n",
        "        decoder_state = residual + decoder_state\n",
        "\n",
        "        # Feed forward\n",
        "        residual = decoder_state\n",
        "        decoder_state = self.glu.forward(decoder_state)\n",
        "        decoder_state = residual + decoder_state\n",
        "\n",
        "        return decoder_state, attention_state\n",
        "\n",
        "\n",
        "class DalleBartDecoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_vocab_count: int,\n",
        "        embed_count: int,\n",
        "        attention_head_count: int,\n",
        "        glu_embed_count: int,\n",
        "        layer_count: int,\n",
        "        start_token: int\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.layer_count = layer_count\n",
        "        self.embed_count = embed_count\n",
        "        self.image_vocab_count = image_vocab_count\n",
        "        self.embed_tokens = nn.Embedding(image_vocab_count + 1, embed_count)\n",
        "        self.embed_positions = nn.Embedding(IMAGE_TOKEN_COUNT, embed_count)\n",
        "        self.layers: List[DecoderLayer] = nn.ModuleList([\n",
        "            DecoderLayer(\n",
        "                attention_head_count,\n",
        "                embed_count,\n",
        "                glu_embed_count\n",
        "            ) \n",
        "            for _ in range(layer_count)\n",
        "        ])\n",
        "        self.layernorm_embedding = nn.LayerNorm(embed_count)\n",
        "        self.final_ln = nn.LayerNorm(embed_count)\n",
        "        self.lm_head = nn.Linear(embed_count, image_vocab_count + 1, bias=False)\n",
        "        self.zero_prob = torch.zeros([1])\n",
        "        self.token_indices = torch.arange(IMAGE_TOKEN_COUNT)\n",
        "        self.start_token = torch.tensor([start_token]).to(torch.long)\n",
        "        if torch.cuda.is_available():\n",
        "            self.zero_prob = self.zero_prob.cuda()\n",
        "            self.token_indices = self.token_indices.cuda()\n",
        "            self.start_token = self.start_token.cuda()\n",
        "\n",
        "\n",
        "    def decode_step(\n",
        "        self,\n",
        "        temperature: float,\n",
        "        top_k: int,\n",
        "        supercondition_factor: int,\n",
        "        attention_mask: BoolTensor,\n",
        "        encoder_state: FloatTensor,\n",
        "        attention_state: FloatTensor,\n",
        "        prev_tokens: LongTensor,\n",
        "        token_index: LongTensor\n",
        "    ) -> Tuple[FloatTensor, FloatTensor]:\n",
        "        image_count = encoder_state.shape[0] // 2\n",
        "        token_index_batched = token_index[[0] * image_count * 2]\n",
        "        prev_tokens = prev_tokens[list(range(image_count)) * 2]\n",
        "        prev_tokens.clamp_(0, self.image_vocab_count)\n",
        "        decoder_state = self.embed_tokens.forward(prev_tokens)\n",
        "        decoder_state += self.embed_positions.forward(token_index_batched)\n",
        "        decoder_state = self.layernorm_embedding.forward(decoder_state)\n",
        "        decoder_state = decoder_state[:, None]\n",
        "        for i in range(self.layer_count):\n",
        "            decoder_state, attention_state[i] = self.layers[i].forward(\n",
        "                decoder_state,\n",
        "                encoder_state,\n",
        "                attention_state[i],\n",
        "                attention_mask,\n",
        "                token_index\n",
        "            )\n",
        "        decoder_state = self.final_ln(decoder_state)\n",
        "        logits = self.lm_head(decoder_state)\n",
        "        a = supercondition_factor\n",
        "        logits: FloatTensor = (\n",
        "            logits[:image_count, -1] * (1 - a) + \n",
        "            logits[image_count:, -1] * a\n",
        "        )\n",
        "\n",
        "        top_logits, _ = logits.topk(top_k, dim=-1)\n",
        "        is_kept = logits >= top_logits[:, [-1]]\n",
        "        logits -= top_logits[:, [0]]\n",
        "        logits /= max(temperature, 1e-6)\n",
        "        probs = torch.where(is_kept, torch.exp(logits), self.zero_prob)\n",
        "        probs[:, 2 ** 14:] = 0              # vqgan vocab_count is only 2 ** 14\n",
        "        return probs, attention_state\n",
        "\n",
        "\n",
        "    def decode_row(\n",
        "        self,\n",
        "        row_index: int,\n",
        "        temperature: float,\n",
        "        top_k: int,\n",
        "        supercondition_factor: int,\n",
        "        encoder_state: FloatTensor,\n",
        "        attention_mask: BoolTensor,\n",
        "        attention_state: FloatTensor,\n",
        "        image_tokens_sequence: LongTensor\n",
        "    ) -> Tuple[FloatTensor, LongTensor]:\n",
        "        for col_index in range(ROW_STA,16):\n",
        "            i = 16 * row_index + col_index\n",
        "            probs, attention_state = self.decode_step(\n",
        "                temperature = temperature,\n",
        "                top_k = top_k,\n",
        "                supercondition_factor = supercondition_factor,\n",
        "                attention_mask = attention_mask,\n",
        "                encoder_state = encoder_state,\n",
        "                attention_state = attention_state,\n",
        "                prev_tokens = image_tokens_sequence[:, i],\n",
        "                token_index = self.token_indices[[i]]\n",
        "            )\n",
        "            image_tokens_sequence[:, i + 1] = torch.multinomial(probs, 1)[:, 0]\n",
        "\n",
        "        return attention_state, image_tokens_sequence\n",
        "\n",
        "    \n",
        "    def decode_initial(\n",
        "        self,\n",
        "        seed: int,\n",
        "        image_count: int,\n",
        "        text_tokens: LongTensor,\n",
        "        encoder_state: FloatTensor\n",
        "    ) -> Tuple[FloatTensor, FloatTensor, FloatTensor, LongTensor]:\n",
        "        expanded_indices = [0] * image_count + [1] * image_count\n",
        "        text_tokens = text_tokens[expanded_indices]\n",
        "        encoder_state = encoder_state[expanded_indices]\n",
        "        attention_mask = text_tokens.not_equal(1)\n",
        "\n",
        "        attention_state_shape = (\n",
        "            self.layer_count,\n",
        "            image_count * 4,\n",
        "            IMAGE_TOKEN_COUNT,\n",
        "            self.embed_count\n",
        "        )\n",
        "        attention_state = torch.zeros(attention_state_shape)\n",
        "        image_tokens_sequence = torch.full(\n",
        "            (image_count, IMAGE_TOKEN_COUNT + 1), \n",
        "            BLANK_TOKEN,\n",
        "            dtype=torch.long\n",
        "        )\n",
        "        if torch.cuda.is_available(): \n",
        "            attention_state = attention_state.cuda()\n",
        "            image_tokens_sequence = image_tokens_sequence.cuda()\n",
        "        \n",
        "        image_tokens_sequence[:, 0] = self.start_token[0]\n",
        "\n",
        "        if seed > 0: torch.manual_seed(seed)\n",
        "\n",
        "        return encoder_state, attention_mask, attention_state, image_tokens_sequence"
      ],
      "metadata": {
        "id": "P54OurB24cO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizer"
      ],
      "metadata": {
        "id": "0Lw-W7bUdxH9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jigj0B226DA"
      },
      "outputs": [],
      "source": [
        "class TextTokenizer:\n",
        "    def __init__(self, vocab: dict, merges: List[str]):\n",
        "        self.token_from_subword = vocab\n",
        "        pairs = [tuple(pair.split()) for pair in merges]\n",
        "        self.rank_from_pair = dict(zip(pairs, range(len(pairs))))\n",
        "\n",
        "    def tokenize(self, text: str, is_verbose: bool = False) -> List[int]:\n",
        "        sep_token = self.token_from_subword['</s>']\n",
        "        cls_token = self.token_from_subword['<s>']\n",
        "        unk_token = self.token_from_subword['<unk>']\n",
        "        text = text.lower().encode(\"ascii\", errors=\"ignore\").decode()\n",
        "        tokens = [\n",
        "            self.token_from_subword.get(subword, unk_token)\n",
        "            for word in text.split(\" \") if len(word) > 0\n",
        "            for subword in self.get_byte_pair_encoding(word, is_verbose)\n",
        "        ]\n",
        "        return [cls_token] + tokens + [sep_token]\n",
        "\n",
        "    def get_byte_pair_encoding(self, word: str, is_verbose: bool) -> List[str]:\n",
        "        def get_pair_rank(pair: Tuple[str, str]) -> int:\n",
        "            return self.rank_from_pair.get(pair, inf)\n",
        "\n",
        "        subwords = [chr(ord(\" \") + 256)] + list(word)\n",
        "        while len(subwords) > 1:\n",
        "            pairs = list(zip(subwords[:-1], subwords[1:]))\n",
        "            pair_to_merge = min(pairs, key=get_pair_rank)\n",
        "            if pair_to_merge not in self.rank_from_pair: break\n",
        "            i = pairs.index(pair_to_merge)\n",
        "            subwords = (\n",
        "                (subwords[:i] if i > 0 else []) + \n",
        "                [subwords[i] + subwords[i + 1]] + \n",
        "                (subwords[i + 2:] if i + 2 < len(subwords) else [])\n",
        "            )\n",
        "\n",
        "        if is_verbose: print(subwords)\n",
        "        return subwords\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MinDalle class"
      ],
      "metadata": {
        "id": "dAbq5eR3dvc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import init_empty_weights\n",
        "\n",
        "def get_keys_to_submodule(model: nn.Module) -> Dict[str, nn.Module]:\n",
        "    keys_to_submodule = {}\n",
        "    # iterate all submodules\n",
        "    for submodule_name, submodule in model.named_modules():\n",
        "        # iterate all paramters in each submobule\n",
        "        for param_name, param in submodule.named_parameters():\n",
        "            # param_name is organized as <name>.<subname>.<subsubname> ...\n",
        "            # the more we go deep in the model, the less \"subname\"s we have\n",
        "            splitted_param_name = param_name.split('.')\n",
        "            # if we have only one subname, then it means that we reach a \"leaf\" submodule, \n",
        "            # we cannot go inside it anymore. This is the actual parameter\n",
        "            is_leaf_param = len(splitted_param_name) == 1\n",
        "            if is_leaf_param:\n",
        "                # we recreate the correct key\n",
        "                key = f\"{submodule_name}.{param_name}\"\n",
        "                # we associate this key with this submodule\n",
        "                keys_to_submodule[key] = submodule\n",
        "                \n",
        "    return keys_to_submodule\n",
        "\n",
        "def load_state_dict_with_low_memory(model: nn.Module, state_dict):\n",
        "    # free up memory by placing the model in the `meta` device\n",
        "    keys_to_submodule = get_keys_to_submodule(model)\n",
        "    mste=model.state_dict()\n",
        "    for key, submodule in keys_to_submodule.items():\n",
        "        # get the valye from the state_dict\n",
        "        if key in state_dict:\n",
        "          val = state_dict[key]\n",
        "        else:\n",
        "          val = torch.ones(mste[key].shape, dtype= torch.float16)\n",
        "        # we need to substitute the parameter inside submodule, \n",
        "        # remember key is composed of <name>.<subname>.<subsubname>\n",
        "        # the actual submodule's parameter is stored inside the \n",
        "        # last subname. If key is `in_proj.weight`, the correct field if `weight`\n",
        "        param_name = key.split('.')[-1]\n",
        "        #param_dtype = getattr(submodule, param_name).dtype\n",
        "        #val = val.to(param_dtype)\n",
        "        # create a new parameter\n",
        "        new_val = torch.nn.Parameter(val)\n",
        "        setattr(submodule, param_name, new_val)\n",
        "\n",
        "\n",
        "def init2(mdl,path):\n",
        "  load_state_dict_with_low_memory(mdl, torch.load(path,map_location='cpu'))\n",
        "  #mdl.load_state_dict(params, strict=False)\n",
        "  mdl.eval()\n",
        "  \n",
        "\n",
        "\n",
        "class MinDalle:\n",
        "    def __init__(\n",
        "        self,\n",
        "        models_root: str = 'pretrained',\n",
        "        dtype: torch.dtype = torch.float32,\n",
        "        is_mega: bool = True, \n",
        "        is_reusable: bool = True,\n",
        "        is_verbose = True\n",
        "    ):\n",
        "        self.is_mega = is_mega\n",
        "        self.is_reusable = is_reusable\n",
        "        self.dtype = dtype\n",
        "        self.is_verbose = is_verbose\n",
        "        self.text_token_count = 64\n",
        "        self.layer_count = 24 if is_mega else 12\n",
        "        self.attention_head_count = 32 if is_mega else 16\n",
        "        self.embed_count = 2048 if is_mega else 1024\n",
        "        self.glu_embed_count = 4096 if is_mega else 2730\n",
        "        self.text_vocab_count = 50272 if is_mega else 50264\n",
        "        self.image_vocab_count = 16415 if is_mega else 16384\n",
        "\n",
        "        model_name = 'dalle_bart_{}'.format('mega' if is_mega else 'mini')\n",
        "        dalle_path = os.path.join(models_root, model_name)\n",
        "        vqgan_path = os.path.join(models_root, 'vqgan')\n",
        "        if not os.path.exists(dalle_path): os.makedirs(dalle_path)\n",
        "        if not os.path.exists(vqgan_path): os.makedirs(vqgan_path)\n",
        "        self.vocab_path = os.path.join(dalle_path, 'vocab.json')\n",
        "        self.merges_path = os.path.join(dalle_path, 'merges.txt')\n",
        "        self.encoder_params_path = os.path.join(dalle_path, 'encoder.pt')\n",
        "        self.decoder_params_path = os.path.join(dalle_path, 'decoder.pt')\n",
        "        self.detoker_params_path = os.path.join(vqgan_path, 'detoker.pt')\n",
        "\n",
        "        self.init_tokenizer()\n",
        "        \n",
        "\n",
        "\n",
        "    def download_tokenizer(self):\n",
        "        if self.is_verbose: print(\"downloading tokenizer params\")\n",
        "        suffix = '' if self.is_mega else '_mini'\n",
        "        vocab = requests.get(MIN_DALLE_REPO + 'vocab{}.json'.format(suffix))\n",
        "        merges = requests.get(MIN_DALLE_REPO + 'merges{}.txt'.format(suffix))\n",
        "        with open(self.vocab_path, 'wb') as f: f.write(vocab.content)\n",
        "        with open(self.merges_path, 'wb') as f: f.write(merges.content)\n",
        "\n",
        "\n",
        "    def download_encoder(self):\n",
        "        if self.is_verbose: print(\"downloading encoder params\")\n",
        "        suffix = '' if self.is_mega else '_mini'\n",
        "        urli=MIN_DALLE_REPO + 'encoder{}.pt'.format(suffix)\n",
        "        !wget -O {self.encoder_params_path} {urli}\n",
        "        #params = requests.get(MIN_DALLE_REPO + 'encoder{}.pt'.format(suffix))\n",
        "        #with open(self.encoder_params_path, 'wb') as f: f.write(params.content)\n",
        "\n",
        "\n",
        "    def download_decoder(self):\n",
        "        if self.is_verbose: print(\"downloading decoder params\")\n",
        "        suffix = '' if self.is_mega else '_mini'\n",
        "        urli=MIN_DALLE_REPO + 'decoder{}.pt'.format(suffix)\n",
        "        !wget -O {self.decoder_params_path} {urli}\n",
        "        #params = requests.get(MIN_DALLE_REPO + 'decoder{}.pt'.format(suffix))\n",
        "        #with open(self.decoder_params_path, 'wb') as f: f.write(params.content)\n",
        "    \n",
        "\n",
        "    def download_detokenizer(self):\n",
        "        if self.is_verbose: print(\"downloading detokenizer params\")\n",
        "        params = requests.get(MIN_DALLE_REPO + 'detoker.pt')\n",
        "        with open(self.detoker_params_path, 'wb') as f: f.write(params.content)\n",
        "\n",
        "\n",
        "    def init_tokenizer(self):\n",
        "        is_downloaded = os.path.exists(self.vocab_path)\n",
        "        is_downloaded &= os.path.exists(self.merges_path)\n",
        "        if not is_downloaded: self.download_tokenizer()\n",
        "        if self.is_verbose: print(\"intializing TextTokenizer\")\n",
        "        with open(self.vocab_path, 'r', encoding='utf8') as f:\n",
        "            vocab = json.load(f)\n",
        "        with open(self.merges_path, 'r', encoding='utf8') as f:\n",
        "            merges = f.read().split(\"\\n\")[1:-1]\n",
        "        self.tokenizer = TextTokenizer(vocab, merges)\n",
        "\n",
        "\n",
        "    def init_encoder(self):\n",
        "        is_downloaded = os.path.exists(self.encoder_params_path)\n",
        "        if not is_downloaded: self.download_encoder()\n",
        "        if self.is_verbose: print(\"initializing DalleBartEncoder\")\n",
        "        with init_empty_weights():\n",
        "          self.encoder = DalleBartEncoder(\n",
        "              attention_head_count = self.attention_head_count,\n",
        "              embed_count = self.embed_count,\n",
        "              glu_embed_count = self.glu_embed_count,\n",
        "              text_token_count = self.text_token_count,\n",
        "              text_vocab_count = self.text_vocab_count,\n",
        "              layer_count = self.layer_count\n",
        "          )\n",
        "        init2(self.encoder,self.encoder_params_path)\n",
        "        if torch.cuda.is_available(): self.encoder = self.encoder.cuda()\n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "    def init_decoder(self):\n",
        "        is_downloaded = os.path.exists(self.decoder_params_path)\n",
        "        if not is_downloaded: self.download_decoder()\n",
        "        if self.is_verbose: print(\"initializing DalleBartDecoder\")\n",
        "        with init_empty_weights():\n",
        "          self.decoder = DalleBartDecoder(\n",
        "              image_vocab_count = self.image_vocab_count,\n",
        "              attention_head_count = self.attention_head_count,\n",
        "              embed_count = self.embed_count,\n",
        "              glu_embed_count = self.glu_embed_count,\n",
        "              layer_count = self.layer_count,\n",
        "              start_token = self.image_vocab_count\n",
        "          )\n",
        "        init2(self.decoder,self.decoder_params_path)\n",
        "        if torch.cuda.is_available(): self.decoder = self.decoder.cuda()\n",
        "\n",
        "\n",
        "    def init_detokenizer(self):\n",
        "        is_downloaded = os.path.exists(self.detoker_params_path)\n",
        "        if not is_downloaded: self.download_detokenizer()\n",
        "        if self.is_verbose: print(\"initializing VQGanDetokenizer\")\n",
        "        self.detokenizer = VQGanDetokenizer().eval()\n",
        "        params = torch.load(self.detoker_params_path)\n",
        "        self.detokenizer.load_state_dict(params)\n",
        "        del params\n",
        "        if torch.cuda.is_available(): self.detokenizer = self.detokenizer.cuda()\n",
        "\n",
        "\n",
        "    def images_from_tokens(\n",
        "        self,\n",
        "        image_tokens: LongTensor,\n",
        "        is_verbose: bool = False\n",
        "    ) -> FloatTensor:\n",
        "        if not self.is_reusable: del self.decoder\n",
        "        if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "        if not self.is_reusable: self.init_detokenizer()\n",
        "        if is_verbose: print(\"detokenizing image\")\n",
        "        images = self.detokenizer.forward(image_tokens).to(torch.uint8)\n",
        "        if not self.is_reusable: del self.detokenizer\n",
        "        return images\n",
        "\n",
        "\n",
        "    def grid_from_images(self, images: FloatTensor) -> Image.Image:\n",
        "        grid_size = int(sqrt(images.shape[0]))\n",
        "        images = images.reshape([grid_size] * 2 + list(images.shape[1:]))\n",
        "        image = images.flatten(1, 2).transpose(0, 1).flatten(1, 2)\n",
        "        image = Image.fromarray(image.to('cpu').numpy())\n",
        "        return image\n",
        "\n",
        "\n",
        "    def generate_images_stream(\n",
        "        self, \n",
        "        text: str, \n",
        "        seed: int,\n",
        "        image_count: int,\n",
        "        log2_mid_count: int,\n",
        "        temperature: float = 1,\n",
        "        top_k: int = 256,\n",
        "        supercondition_factor: int = 16,\n",
        "        is_verbose: bool = False\n",
        "    ) -> Iterator[FloatTensor]:\n",
        "        assert(log2_mid_count in range(5))\n",
        "        if is_verbose: print(\"tokenizing text\")\n",
        "        tokens = self.tokenizer.tokenize(text, is_verbose=is_verbose)\n",
        "        if len(tokens) > self.text_token_count: \n",
        "            tokens = tokens[:self.text_token_count]\n",
        "        if is_verbose: print(\"{} text tokens\".format(len(tokens)), tokens)\n",
        "        text_tokens = numpy.ones((2, 64), dtype=numpy.int32)\n",
        "        text_tokens[0, :2] = [tokens[0], tokens[-1]]\n",
        "        text_tokens[1, :len(tokens)] = tokens\n",
        "\n",
        "        text_tokens = torch.tensor(text_tokens).to(torch.long)\n",
        "        if torch.cuda.is_available(): text_tokens = text_tokens.cuda()\n",
        "\n",
        "        if not self.is_reusable: self.init_encoder()\n",
        "        if is_verbose: print(\"encoding text tokens\")\n",
        "        with torch.cuda.amp.autocast(dtype=self.dtype):\n",
        "            encoder_state = self.encoder.forward(text_tokens)\n",
        "        if not self.is_reusable: del self.encoder\n",
        "        if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "\n",
        "        if not self.is_reusable: self.init_decoder()\n",
        "\n",
        "        with torch.cuda.amp.autocast(dtype=self.dtype):\n",
        "            encoder_state, attention_mask, attention_state, image_tokens = ( \n",
        "                self.decoder.decode_initial(\n",
        "                    seed=seed, \n",
        "                    image_count=image_count, \n",
        "                    text_tokens=text_tokens, \n",
        "                    encoder_state=encoder_state\n",
        "                )\n",
        "            )\n",
        "\n",
        "        row_count = 16\n",
        "        for row_index in range(row_count):\n",
        "            if is_verbose: \n",
        "                print('sampling row {} of {}'.format(row_index + 1, row_count))\n",
        "            with torch.cuda.amp.autocast(dtype=self.dtype):\n",
        "                attention_state, image_tokens = self.decoder.decode_row(\n",
        "                    row_index,\n",
        "                    temperature=temperature,\n",
        "                    top_k=top_k,\n",
        "                    supercondition_factor=supercondition_factor,\n",
        "                    encoder_state=encoder_state,\n",
        "                    attention_mask=attention_mask,\n",
        "                    attention_state=attention_state,\n",
        "                    image_tokens_sequence=image_tokens\n",
        "                )\n",
        "            with torch.cuda.amp.autocast(dtype=torch.float32):\n",
        "                if ((row_index + 1) * (2 ** log2_mid_count)) % row_count == 0:\n",
        "                    tokens = image_tokens[:, 1:]\n",
        "                    images = self.images_from_tokens(tokens, is_verbose)\n",
        "                    yield images\n",
        "\n",
        "\n",
        "    def generate_image_stream(\n",
        "        self, \n",
        "        text: str, \n",
        "        seed: int,\n",
        "        grid_size: int,\n",
        "        log2_mid_count: int,\n",
        "        temperature: float = 1,\n",
        "        top_k: int = 256,\n",
        "        supercondition_factor: int = 16,\n",
        "        is_verbose: bool = False\n",
        "    ) -> Iterator[Image.Image]:\n",
        "        images_stream = self.generate_images_stream(\n",
        "            text=text, \n",
        "            seed=seed,\n",
        "            image_count=grid_size ** 2,\n",
        "            log2_mid_count=log2_mid_count,\n",
        "            temperature=temperature,\n",
        "            top_k=top_k,\n",
        "            supercondition_factor=supercondition_factor,\n",
        "            is_verbose=is_verbose\n",
        "        )\n",
        "        for images in images_stream:\n",
        "            yield self.grid_from_images(images)\n",
        "\n",
        "\n",
        "    def generate_images(\n",
        "        self, \n",
        "        text: str,\n",
        "        seed: int = -1,\n",
        "        image_count: int = 1,\n",
        "        temperature: float = 1,\n",
        "        top_k: int = 1024,\n",
        "        supercondition_factor: int = 16,\n",
        "        is_verbose: bool = False\n",
        "    ) -> FloatTensor:\n",
        "        log2_mid_count = 0\n",
        "        images_stream = self.generate_images_stream(\n",
        "            text=text,\n",
        "            seed=seed,\n",
        "            image_count=image_count,\n",
        "            temperature=temperature,\n",
        "            log2_mid_count=log2_mid_count,\n",
        "            top_k=top_k,\n",
        "            supercondition_factor=supercondition_factor,\n",
        "            is_verbose=is_verbose\n",
        "        )\n",
        "        return next(images_stream)\n",
        "\n",
        "\n",
        "    def generate_image(\n",
        "        self, \n",
        "        text: str,\n",
        "        seed: int = -1,\n",
        "        grid_size: int = 1,\n",
        "        temperature: float = 1,\n",
        "        top_k: int = 1024,\n",
        "        supercondition_factor: int = 16,\n",
        "        is_verbose: bool = False\n",
        "    ) -> Image.Image:\n",
        "        log2_mid_count = 0\n",
        "        image_stream = self.generate_image_stream(\n",
        "            text=text,\n",
        "            seed=seed,\n",
        "            grid_size=grid_size,\n",
        "            log2_mid_count=log2_mid_count,\n",
        "            temperature=temperature,\n",
        "            top_k=top_k,\n",
        "            supercondition_factor=supercondition_factor,\n",
        "            is_verbose=is_verbose\n",
        "        )\n",
        "        return next(image_stream)"
      ],
      "metadata": {
        "id": "LCA4Pryz4zej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "new def funcs"
      ],
      "metadata": {
        "id": "NWRvjNYmdoc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_count = IMAGE_TOKEN_COUNT//16\n",
        "def rumpla(n,dmp='ozv.bin'):\n",
        "  global attention_state\n",
        "  global image_tokens\n",
        "  for row_index in range(n,row_count):\n",
        "      print(str(row_index),end= '-')\n",
        "      with torch.cuda.amp.autocast(dtype=torch.float16):\n",
        "        attention_state, image_tokens = mindd.decoder.decode_row(\n",
        "                        row_index,\n",
        "                        temperature=temperature,\n",
        "                        top_k=top_k,\n",
        "                        supercondition_factor=supercondition_factor,\n",
        "                        encoder_state=encoder_state,\n",
        "                        attention_mask=attention_mask,\n",
        "                        attention_state=attention_state,\n",
        "                        image_tokens_sequence=image_tokens\n",
        "                    )\n",
        "      \n",
        "      if ((row_index + 1) * (2 ** log2_mid_count)) % row_count == 0:\n",
        "          tokens = image_tokens[:, 1:]\n",
        "          \n",
        "  hux=tokens.to('cpu').numpy().astype(np.uint16)\n",
        "  with open(dmp,mode='ba+') as f:\n",
        "    hux.tofile(f)\n",
        "\n",
        "\n",
        "def intpencK():\n",
        "  interpo(dumped_seqs[-1],dumped_seqs[-2])\n",
        "  !rm /content/intp.webm\n",
        "  !/content/ffmpeg-5.0.1-amd64-static/ffmpeg -framerate 18 -i /content/donn/%02d.png -sn -map_metadata -1 -map_chapters -1 -crf 10 -c:v libaom-av1 -aom-params enable-keyframe-filtering=0:enable-tpl-model=1 -lag-in-frames 48 -cpu-used 5 -row-mt 1 -tiles 1x1 -threads 2 -strict experimental -movflags +faststart -flags +cgop -pix_fmt yuv420p10le -c:a libopus -b:a 96k -ac 2 -f webm /content/intp.webm\n",
        "\n",
        "def hcopy(tk,left_sta=8):\n",
        "  len=16-left_sta\n",
        "  for y in range(16):\n",
        "    mae=y*16\n",
        "    tk[mae:mae+len]=tk[mae+left_sta:mae+16]\n",
        "\n",
        "\n",
        "!mkdir donn"
      ],
      "metadata": {
        "id": "-1rC4vBcxtQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PlayGround"
      ],
      "metadata": {
        "id": "N_gjJMl35EVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mindd = MinDalle(is_mega=True, is_reusable=True)\n",
        "mindd.init_decoder()\n",
        "mindd.init_encoder()\n",
        "newstart=True\n",
        "mizu=0\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "wCQAnNtl-Wlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Nice window curtains, cloth-covered side table, single-screen pc setup with a massive painting\" \n",
        "'''\n",
        "\n",
        "Female factory, award winning concept art, trending on artstation, matte\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "seed = 77788\n",
        "image_count= 1\n",
        "log2_mid_count= 0\n",
        "temperature= 0.25\n",
        "top_k= 256\n",
        "supercondition_factor= 8\n",
        "is_verbose= False\n",
        "\n",
        "tokens = mindd.tokenizer.tokenize(text, is_verbose=is_verbose)\n",
        "if len(tokens) > mindd.text_token_count: \n",
        "    tokens = tokens[:mindd.text_token_count]\n",
        "\n",
        "print(\"{} text tokens\".format(len(tokens)), tokens)\n",
        "text_tokens = np.ones((2, 64), dtype=np.int32)\n",
        "text_tokens[0, :2] = [tokens[0], tokens[-1]]\n",
        "text_tokens[1, :len(tokens)] = tokens\n",
        "\n",
        "text_tokens = torch.tensor(text_tokens).to(torch.long).cuda()\n",
        "\n",
        "\n",
        "with torch.cuda.amp.autocast(dtype=torch.float16):\n",
        "  encoder_state_new = mindd.encoder.forward(text_tokens)\n",
        "\n",
        "newprompt=True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpgEjoM95F9M",
        "outputId": "5d3c1bb4-0d5b-4d97-9322-8bde3944e0c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47 text tokens [0, 4688, 3883, 12001, 11, 1166, 3, 4499, 93, 2220, 1268, 11, 1932, 3, 7561, 2238, 11007, 208, 58, 9588, 1545, 54, 13, 867, 111, 99, 786, 111, 5599, 45, 699, 314, 22962, 11, 7817, 1024, 111, 5009, 54, 13, 1485, 37, 551, 3, 2052, 2775, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_state=encoder_state_new"
      ],
      "metadata": {
        "id": "V-VwYJEVOuyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.cuda.amp.autocast(dtype=torch.float16):\n",
        "  encoder_state, attention_mask, attention_state, image_tokens_new = ( \n",
        "      mindd.decoder.decode_initial(\n",
        "          seed=seed, \n",
        "          image_count=image_count, \n",
        "          text_tokens=text_tokens, \n",
        "          encoder_state=encoder_state\n",
        "      )\n",
        "  )\n"
      ],
      "metadata": {
        "id": "JXMHtr0b7MsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_tokens = image_tokens_new"
      ],
      "metadata": {
        "id": "-wIV2ieGEe7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#image_tokens[0][:-1]=torch.tensor(dumped_seqs[-1],dtype=torch.int32)"
      ],
      "metadata": {
        "id": "VgXFSd9S3LmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#image_tokens[0][96:]=torch.tensor([BLANK_TOKEN]*161,dtype=torch.int32)\n",
        "\n",
        "if not newprompt:\n",
        "  ROW_STA=7\n",
        "  hcopy(image_tokens[0])\n",
        "\n",
        "#image_tokens[0][:128]=image_tokens[0][128:-1]"
      ],
      "metadata": {
        "id": "KPhSV7SxyNZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lpsta=0\n",
        "#ROW_STA=0\n",
        "if newprompt:\n",
        "  lpsta=0\n",
        "  ROW_STA=0\n",
        "\n",
        "rumpla(   0   )\n",
        "newprompt=False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBlQpUV7P3Hf",
        "outputId": "74b3a62a-e870-4919-f6bd-daa596b4a145"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0-1-2-3-4-5-6-7-8-9-10-11-12-13-14-15-"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dumped_seqs=np.fromfile('ozv.bin',dtype=np.uint16).astype(np.int32).reshape((-1,256))\n",
        "showp(-1)"
      ],
      "metadata": {
        "id": "OOtmYyLKP44G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#image_tokens[0][:-1]=torch.tensor(dumped_seqs[-1],dtype=torch.int32)"
      ],
      "metadata": {
        "id": "qlpmygzN0gy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Treading=False #@param {type:\"boolean\"}\n",
        "lmz=2 #@param {type:\"integer\"}\n",
        "lamp=16\n",
        "dmp='ozv.bin'\n",
        "dofin=False\n",
        "\n",
        "!mv /content/intp.webm /content/pretrained/{mizu}.webm\n",
        "mizu+=1\n",
        "\n",
        "def wkoo():\n",
        "  dmpsig=dmp+'fin'\n",
        "  if dofin:\n",
        "    if os.path.isfile(dmpsig):\n",
        "      os.remove(dmpsig)\n",
        "  dmpna1=dmp\n",
        "  for k in range(lamp):\n",
        "    rumpla(   lmz  ,dmpna1 )\n",
        "  if dofin:\n",
        "    os.link(dmp,dmpsig)\n",
        "\n",
        "\n",
        "if Treading:\n",
        "  lamp=480\n",
        "  dmp='ozv1.bin'\n",
        "  dofin=True\n",
        "  t1 = Thread(target = wkoo)\n",
        "  a1 = t1.start()\n",
        "else:\n",
        "  wkoo()\n"
      ],
      "metadata": {
        "id": "p-jQlX6_w9f9",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38835faa-926c-43b1-8642-22a26efe9ab5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat '/content/intp.webm': No such file or directory\n",
            "2-3-4-5-6-7-8-9-10-11-12-13-14-15-2-3-4-5-6-7-8-9-10-11-12-13-14-15-2-3-4-5-6-7-8-9-10-11-12-13-14-15-2-3-4-5-6-7-8-9-10-11-12-13-14-15-2-3-4-5-6-7-8-9-10-11-12-13-14-15-2-3-4-5-6-7-8-9-10-11-12-13-14-15-2-3-4-5-6-7-8-9-10-11-12-13-14-15-2-3-4-5-6-7-8-9-10-11-12-13-14-15-2-3-4-5-6-7-8-9-10-11-12-13-14-15-2-3-4-5-6-7-8-9-10-11-12-13-14-15-2-3-4-5-6-7-8-9-10-11-12-13-14-15-2-3-4-5-6-7-8-9-10-11-12-13-14-15-2-3-4-5-6-7-8-9-10-11-12-13-14-15-2-3-4-5-6-7-8-9-10-11-12-13-14-15-2-3-4-5-6-7-8-9-10-11-12-13-14-15-2-3-4-5-6-7-8-9-10-11-12-13-14-15-"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Treading=False #@param {type:\"boolean\"}\n",
        "lmz=2 #@param {type:\"integer\"}\n",
        "lamp=16\n",
        "dmp='ozv.bin'\n",
        "dofin=False\n",
        "def wkoo():\n",
        "  dmpsig=dmp+'fin'\n",
        "  if dofin:\n",
        "    if os.path.isfile(dmpsig):\n",
        "      os.remove(dmpsig)\n",
        "  dmpna=dmp\n",
        "  for k in range(lamp):\n",
        "    rumpla(   lmz  ,dmpna )\n",
        "  if dofin:\n",
        "    os.link(dmp,dmpsig)\n",
        "\n",
        "\n",
        "if Treading:\n",
        "  lamp=480\n",
        "  dmp='ozv1.bin'\n",
        "  dofin=True\n",
        "  t1 = Thread(target = wkoo)\n",
        "  a1 = t1.start()\n",
        "else:\n",
        "  wkoo()\n",
        "\n",
        "dumped_seqs=np.fromfile('ozv.bin',dtype=np.uint16).astype(np.int32).reshape((-1,256))\n",
        "'''\n",
        "tgg = Thread(target = intpencK)\n",
        "agg = tgg.start()\n",
        "'''\n",
        "showp(-1)"
      ],
      "metadata": {
        "id": "tGIrP-aMw_B8",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#image_tokens[0][:128]=image_tokens[0][128:-1]"
      ],
      "metadata": {
        "id": "KKc02lxjJEzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ncnndec"
      ],
      "metadata": {
        "id": "v9oNAJGa9770"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hide=False #@param {type:\"boolean\"}\n",
        "if not os.path.isfile('/tmp/vq.bin'):\n",
        "  !pip install ncnn\n",
        "  !wget -O /tmp/vq.param https://raw.githubusercontent.com/TabuaTambalam/vqqncnn/main/vq.param\n",
        "  !wget -O /tmp/vq.bin https://github.com/TabuaTambalam/vqqncnn/releases/download/0.0/vq.bin\n",
        "\n",
        "if newstart:\n",
        "  import ncnn\n",
        "  net = ncnn.Net()\n",
        "  net.opt.use_vulkan_compute = False\n",
        "  net.load_param(\"/tmp/vq.param\")\n",
        "  net.load_model(\"/tmp/vq.bin\")\n",
        "  newstart = False\n",
        "\n",
        "\n",
        "def npmkCBemb(seq):\n",
        "  with net.create_extractor() as ex:\n",
        "    ex.input(\"in0\", ncnn.Mat(seq).clone())\n",
        "    hrr, out0 = ex.extract(\"2\")\n",
        "  del ex\n",
        "  return np.array(out0)\n",
        "\n",
        "\n",
        "def npemb2img(emb):\n",
        "  with net.create_extractor() as ex:\n",
        "    ex.input(\"2\", ncnn.Mat(emb).clone())\n",
        "    hrr, out0 = ex.extract(\"out0\")\n",
        "  del ex\n",
        "  return Image.fromarray(np.array(out0).astype(np.uint8))\n",
        "\n",
        "def pbla(step,scale):\n",
        "  ret=[]\n",
        "  mga=4-(4/scale)\n",
        "  k=step-1\n",
        "  for i in range(step):\n",
        "    ret.append(k+mga*( ((i**2)/k) - i ))\n",
        "  return ret\n",
        "\n",
        "\n",
        "def interpo(seq1,seq2,step=30,scale=1.11,outfmt='/content/donn/%02d.png'):\n",
        "  stp=step-1\n",
        "  divi=pbla(step,scale)\n",
        "  em1=npmkCBemb(seq1)\n",
        "  em2=npmkCBemb(seq2)\n",
        "  for i in range(step):\n",
        "    npemb2img((em1*i+em2*(stp-i))/divi[i]).save(outfmt%i)\n",
        "\n",
        "def showp(n):\n",
        "  with net.create_extractor() as ex:\n",
        "    ex.input(\"in0\", ncnn.Mat(dumped_seqs[n]).clone())\n",
        "    hrr, out0 = ex.extract(\"out0\")\n",
        "  del ex\n",
        "  uz=Image.fromarray(np.array(out0).astype(np.uint8))\n",
        "  uz.save('/content/sample_data/%d.png'%n)\n",
        "  return uz"
      ],
      "metadata": {
        "id": "qUEp9DS5-D3T",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "⬆  run the cell above once to load ncnn vqgan (once enough)"
      ],
      "metadata": {
        "id": "VIOweW9-YJBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dumped_seqs=np.fromfile('ozv.bin',dtype=np.uint16).astype(np.int32).reshape((-1,256))\n",
        "showp(-1)"
      ],
      "metadata": {
        "id": "-BcQOXlzaibn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "rumpla(   0   )"
      ],
      "metadata": {
        "id": "DIQi87JHzq_F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "338a773d-bec4-46ed-bd9e-523d0c0e6fe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0-1-2-3-4-5-6-7-8-9-10-11-12-13-14-15-"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "showp(-2)"
      ],
      "metadata": {
        "id": "mZ2iNJghGbuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "showp(-3)"
      ],
      "metadata": {
        "id": "Tue7ryBHZQDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "showp(-4)"
      ],
      "metadata": {
        "id": "sfvw6rvmZUml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "showp(-5)"
      ],
      "metadata": {
        "id": "by2RA0ItZZMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aso=-1\n",
        "dumped_seqs=np.fromfile('ozv.bin',dtype=np.uint16).astype(np.int32).reshape((-1,256))"
      ],
      "metadata": {
        "id": "uBQoCrUJ65Iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Treading=True #@param {type:\"boolean\"}\n",
        "def intpenc():\n",
        "  global aso\n",
        "  asoni=aso-1\n",
        "  interpo(dumped_seqs[aso],dumped_seqs[asoni])\n",
        "  !rm /content/intp.webm\n",
        "  !/content/ffmpeg-5.0.1-amd64-static/ffmpeg -framerate 18 -i /content/donn/%02d.png -sn -map_metadata -1 -map_chapters -1 -crf 10 -c:v libaom-av1 -aom-params enable-keyframe-filtering=0:enable-tpl-model=1 -lag-in-frames 48 -cpu-used 5 -row-mt 1 -tiles 1x1 -threads 2 -strict experimental -movflags +faststart -flags +cgop -pix_fmt yuv420p10le -c:a libopus -b:a 96k -ac 2 -f webm /content/intp.webm\n",
        "  aso=asoni\n",
        "\n",
        "if Treading:\n",
        "  t1 = Thread(target = intpenc)\n",
        "  a1 = t1.start()\n",
        "else:\n",
        "  intpenc()"
      ],
      "metadata": {
        "id": "Af2cxlk15Gsj",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ffmpeg"
      ],
      "metadata": {
        "id": "XD5BIG_q6Yro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hydda=False #@param {type:\"boolean\"}\n",
        "import os\n",
        "if not os.path.exists('/content/avif'):\n",
        "  !wget https://johnvansickle.com/ffmpeg/releases/ffmpeg-release-amd64-static.tar.xz\n",
        "  !tar xvf /content/ffmpeg-release-amd64-static.tar.xz\n",
        "  !mkdir /content/tt1\n",
        "  !mkdir /content/tt2\n",
        "  !mkdir /content/tt2/d\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import shutil\n",
        "import subprocess\n",
        "from google.colab import output\n",
        "from google.colab import files\n",
        "from threading import Thread\n",
        "\n",
        "\n",
        "def dl(url):\n",
        "  global bzna\n",
        "  rbzna=url.split('=')[1][:-4]\n",
        "  if bzna=='':\n",
        "    bzna=rbzna\n",
        "  !wget --content-disposition {url}\n",
        "  if url.endswith('.zip'):\n",
        "    !7z x /content/{rbzna}.zip\n",
        "  if rbzna != bzna:\n",
        "    os.link(rbzna+'.mp4',bzna+'.mp4')\n",
        "\n",
        "\n",
        "\n",
        "def runpyproc(na):\n",
        "  subprocess.Popen(['python','/content/'+na+'.py'],close_fds=True)\n",
        "\n",
        "\n",
        "def renya():\n",
        "  sig='/content/avif/'+bzna.replace('-','_')+'-'\n",
        "  avifz=os.listdir('/content/tt2')\n",
        "  for aav in avifz:\n",
        "    os.rename('/content/tt2/'+aav,sig+aav)\n",
        "\n",
        "def loopdl():\n",
        "  shutil.rmtree('/content/avif/.ipynb_checkpoints',ignore_errors=True)\n",
        "  avifz=os.listdir('/content/avif')\n",
        "  avifz.sort()\n",
        "  if len(avifz) > 2:\n",
        "    avifz=avifz[:2]\n",
        "  for aviif in avifz:\n",
        "    dzt='/content/avifbye/'+aviif\n",
        "    srrc='/content/avif/'+aviif\n",
        "    \n",
        "\n",
        "    os.rename(srrc,dzt)\n",
        "    files.download(dzt)\n",
        "\n",
        "def enc(syg):\n",
        "  !rm *.log\n",
        "  !/content/ffmpeg-5.0.1-amd64-static/ffmpeg -i /content/tt1/{syg}.mp4 -pass 1 -passlogfile loga.log -sn -map_metadata -1 -map_chapters -1 -crf 40 -c:v libaom-av1 -aom-params enable-keyframe-filtering=0:enable-tpl-model=1 -lag-in-frames 48 -cpu-used 5 -row-mt 1 -tiles 1x1 -threads 2 -strict experimental -pix_fmt yuv420p10le -f null NUL\n",
        "  !/content/ffmpeg-5.0.1-amd64-static/ffmpeg -i /content/tt1/{syg}.mp4 -pass 2 -passlogfile loga.log -sn -map_metadata -1 -map_chapters -1 -crf 40 -c:v libaom-av1 -aom-params enable-keyframe-filtering=0:enable-tpl-model=1 -lag-in-frames 48 -cpu-used 5 -row-mt 1 -tiles 1x1 -threads 2 -strict experimental -movflags +faststart -flags +cgop -pix_fmt yuv420p10le -c:a libopus -b:a 96k -ac 2 -f webm /content/tt2/{syg}.webm\n",
        "  \n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jnSnagVH6XqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/ffmpeg-5.0.1-amd64-static/ffmpeg -i /content/{bzna}.mp4 -an -c:v copy -f segment -segment_time 8.7 -reset_timestamps 1 /content/tt1/%03d.mp4\n",
        "#   -c:a copy"
      ],
      "metadata": {
        "id": "R7U51Pn06eMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "!wget https://johnvansickle.com/ffmpeg/builds/ffmpeg-git-amd64-static.tar.xz\n",
        "!tar xvf /content/ffmpeg-git-amd64-static.tar.xz\n",
        "!/content/ffmpeg-git-20220622-amd64-static/ffmpeg -i interpo_mini.webm -c copy kat.avif\n",
        "'''"
      ],
      "metadata": {
        "id": "-dbQ_mrslMTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Treading=True #@param {type:\"boolean\"}\n",
        "bznasf='/content/tt2/'+bzna.replace('-','_')+'-'\n",
        "def wk1():\n",
        "  for pp in range(100):\n",
        "    syga='%03d'%pp\n",
        "    if os.path.isfile('/content/tt1/'+syga+'.mp4'):\n",
        "      enc(syga)\n",
        "      os.rename('/content/tt2/'+syga+'.webm',bznasf+syga+'.webm')\n",
        "\n",
        "if Treading:\n",
        "  t1 = Thread(target = wk1)\n",
        "  a1 = t1.start()\n",
        "else:\n",
        "  wk1()"
      ],
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFQxU2PE6pGQ",
        "outputId": "19feee8f-3f79-479e-a9e4-188d2f43190c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '*.log': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ddal"
      ],
      "metadata": {
        "id": "YTP6xojr7KZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "!rm -rf /content/tt1\n",
        "#!rm -rf /content/tt2\n",
        "!mkdir /content/tt1\n",
        "!mkdir /content/tt2\n",
        "!mkdir /content/tt2/d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-E8q2tZB7Lf_",
        "outputId": "51fad73c-906c-44d1-922d-c9474b183538"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/tt2’: File exists\n",
            "mkdir: cannot create directory ‘/content/tt2/d’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# vqenc"
      ],
      "metadata": {
        "id": "URTDw6t4y4B4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/requirements.txt\n",
        "git+https://github.com/patil-suraj/vqgan-jax.git\n",
        "flax"
      ],
      "metadata": {
        "id": "maYlveZgy6q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import jax\n",
        "from flax.training.common_utils import shard\n",
        "from functools import partial\n",
        "\n",
        "from vqgan_jax.modeling_flax_vqgan import VQModel\n",
        "from flax.jax_utils import replicate\n",
        "\n",
        "VQGAN_REPO = \"dalle-mini/vqgan_imagenet_f16_16384\"\n",
        "VQGAN_COMMIT_ID = \"e93a26e7707683d349bf5d5c41c5b0ef69b677a9\"\n",
        "\n",
        "vqgan, vqgan_params = VQModel.from_pretrained(\n",
        "    VQGAN_REPO, revision=VQGAN_COMMIT_ID, _do_init=False\n",
        ")\n",
        "\n",
        "vqgan_params = replicate(vqgan_params)\n",
        "\n",
        "\n",
        "@partial(jax.pmap, axis_name=\"batch\")\n",
        "def p_encode(batch, params):\n",
        "    # Not sure if we should `replicate` params, does not seem to have any effect\n",
        "    _, indices = vqgan.encode(batch, params=params)\n",
        "    return indices"
      ],
      "metadata": {
        "id": "RV_HPCLwy_eP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "im = Image.open(\"/content/still_title_ricca_001.png\")"
      ],
      "metadata": {
        "id": "OC827WhGzBGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "im = np.expand_dims(np.array(im),axis=0).astype(np.float32)\n",
        "im=(im/255)"
      ],
      "metadata": {
        "id": "q_vEKBxRzCS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hux=np.array(    p_encode(shard(im), vqgan_params)    ,dtype=np.uint16)\n",
        "with open('ozm.bin',mode='ba+') as f:\n",
        "  hux.tofile(f)"
      ],
      "metadata": {
        "id": "8-xHkfkNzFEH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}