{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIXUNu94SIWB"
      },
      "outputs": [],
      "source": [
        "!pip install ftfy transformers omegaconf triton==2.0.0.dev20220701 einops accelerate taming-transformers-rom1504\n",
        "!wget https://github.com/TabuaTambalam/DalleWebms/releases/download/0.1/vrs_lib.7z\n",
        "!7z x vrs_lib.7z\n",
        "!wget https://huggingface.co/spaces/shi-labs/Versatile-Diffusion/resolve/main/pretrained/vd-official.pth\n",
        "!7z x vd-official.pth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import PIL\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import numpy.random as npr\n",
        "from contextlib import nullcontext\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as tvtrans\n",
        "from lib.cfg_helper import model_cfg_bank\n",
        "from lib.model_zoo import get_model\n",
        "from lib.model_zoo.ddim_vd import DDIMSampler_VD, DDIMSampler_VD_DualContext\n",
        "from lib.model_zoo.ddim_dualcontext import DDIMSampler_DualContext\n",
        "\n",
        "from lib.experiments.sd_default import color_adjust\n",
        "\n",
        "from accelerate import init_empty_weights\n",
        "\n",
        "cfgm_name = 'vd_noema'\n",
        "sampler = DDIMSampler_VD\n",
        "pth = 'pretrained/vd-official.pth'\n",
        "\n",
        "cfgm = model_cfg_bank()(cfgm_name)\n",
        "\n",
        "with init_empty_weights():\n",
        "  net = get_model()(cfgm)\n",
        "\n",
        "BICUBIC = PIL.Image.BICUBIC\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "model_name = cfgm_name\n",
        "\n",
        "sampler = sampler(net)"
      ],
      "metadata": {
        "id": "vb_QiSdjSMo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metadev=False #@param {type:'boolean'}\n",
        "\n",
        "metadev=torch.device('meta')\n",
        "\n",
        "def get_keys_to_submodule(model):\n",
        "  keys_to_submodule = {}\n",
        "  # iterate all submodules\n",
        "  for submodule_name, submodule in model.named_modules():\n",
        "      # iterate all paramters in each submobule\n",
        "      for param_name, param in submodule.named_parameters():\n",
        "          # param_name is organized as .. ...\n",
        "          splitted_param_name = param_name.split('.')\n",
        "          # we cannot go inside it anymore. This is the actual parameter\n",
        "          is_leaf_param = len(splitted_param_name) == 1\n",
        "          if is_leaf_param:\n",
        "              # we recreate the correct key\n",
        "              key = f\"{submodule_name}.{param_name}\"\n",
        "              # we associate this key with this submodule\n",
        "              keys_to_submodule[key] = submodule\n",
        "              \n",
        "  return keys_to_submodule\n",
        "\n",
        "\n",
        "def load_state_dict_with_low_memory(model, state_dict,modifyfunc=None,fill=True):\n",
        "  if modifyfunc is not None:\n",
        "    state_dict=modifyfunc(state_dict)\n",
        "  print('======hacky load======')\n",
        "  keys_to_submodule = get_keys_to_submodule(model)\n",
        "  mste=model.state_dict()\n",
        "  for key, submodule in keys_to_submodule.items():\n",
        "      if key[0] == '.':\n",
        "        key=key[1:]\n",
        "      if key in state_dict:\n",
        "        val = state_dict[key]\n",
        "      elif fill:\n",
        "        print(key)\n",
        "        continue\n",
        "        #val = torch.ones(mste[key].shape, dtype= torch.float16)\n",
        "      else:\n",
        "        continue\n",
        "\n",
        "      param_name = key.split('.')[-1]\n",
        "      new_val = torch.nn.Parameter(val,requires_grad=False)\n",
        "      setattr(submodule, param_name, new_val)\n",
        "\n",
        "def savcmpl(fna,z):\n",
        "  sv=dict()\n",
        "  sv[0]=True\n",
        "  sv[1]=z[0]\n",
        "  torch.save(sv,fna+'.compiled_prompt')\n",
        "\n",
        "def regularize_image(x):\n",
        "    \n",
        "    if isinstance(x, str):\n",
        "        x = Image.open(x).resize([512, 512], resample=BICUBIC)\n",
        "        x = tvtrans.ToTensor()(x)\n",
        "    elif isinstance(x, PIL.Image.Image):\n",
        "        x = x.resize([512, 512], resample=BICUBIC)\n",
        "        x = tvtrans.ToTensor()(x)\n",
        "    elif isinstance(x, np.ndarray):\n",
        "        x = PIL.Image.fromarray(x).resize([512, 512], resample=BICUBIC)\n",
        "        x = tvtrans.ToTensor()(x)\n",
        "    elif isinstance(x, torch.Tensor):\n",
        "        pass\n",
        "    else:\n",
        "        assert False, 'Unknown image type'\n",
        "\n",
        "    assert (x.shape[1]==512) & (x.shape[2]==512), \\\n",
        "        'Wrong image size'\n",
        "    if use_cuda:\n",
        "        x = x.to('cuda')\n",
        "    return x\n",
        "\n",
        "def find_low_rank(x, demean=True, q=20, niter=10):\n",
        "    if demean:\n",
        "        x_mean = x.mean(-1, keepdim=True)\n",
        "        x_input = x - x_mean\n",
        "    else:\n",
        "        x_input = x\n",
        "\n",
        "    u, s, v = torch.pca_lowrank(x_input, q=q, center=False, niter=niter)\n",
        "    ss = torch.stack([torch.diag(si) for si in s])\n",
        "    x_lowrank = torch.bmm(torch.bmm(u, ss), torch.permute(v, [0, 2, 1]))        \n",
        "\n",
        "    if demean:\n",
        "        x_lowrank += x_mean\n",
        "    return x_lowrank\n",
        "\n",
        "def remove_low_rank(x, demean=True, q=20, niter=10, q_remove=10):\n",
        "    if demean:\n",
        "        x_mean = x.mean(-1, keepdim=True)\n",
        "        x_input = x - x_mean\n",
        "    else:\n",
        "        x_input = x\n",
        "\n",
        "    u, s, v = torch.pca_lowrank(x_input, q=q, center=False, niter=niter)\n",
        "    s[:, 0:q_remove] = 0\n",
        "    ss = torch.stack([torch.diag(si) for si in s])\n",
        "    x_lowrank = torch.bmm(torch.bmm(u, ss), torch.permute(v, [0, 2, 1]))        \n",
        "\n",
        "    if demean:\n",
        "        x_lowrank += x_mean\n",
        "    return x_lowrank\n",
        "\n",
        "\n",
        "def decode(z, xtype, ctype, color_adj='None', color_adj_to=None):\n",
        "    if xtype == 'image':\n",
        "        x = net.autokl_decode(z)\n",
        "\n",
        "        color_adj_flag = (color_adj!='None') and (color_adj is not None)\n",
        "        color_adj_simple = color_adj=='Simple'\n",
        "        color_adj_keep_ratio = 0.5\n",
        "\n",
        "        if color_adj_flag and (ctype=='vision'):\n",
        "            x_adj = []\n",
        "            for xi in x:\n",
        "                color_adj_f = color_adjust(ref_from=(xi+1)/2, ref_to=color_adj_to)\n",
        "                xi_adj = color_adj_f((xi+1)/2, keep=color_adj_keep_ratio, simple=color_adj_simple)\n",
        "                x_adj.append(xi_adj)\n",
        "            x = x_adj\n",
        "        else:\n",
        "            x = torch.clamp((x+1.0)/2.0, min=0.0, max=1.0)\n",
        "            x = [tvtrans.ToPILImage()(xi) for xi in x]\n",
        "        return x\n",
        "\n",
        "    elif xtype == 'text':\n",
        "        prompt_temperature = 1.0\n",
        "        prompt_merge_same_adj_word = True\n",
        "        x = net.optimus_decode(z, temperature=prompt_temperature)\n",
        "        if prompt_merge_same_adj_word:\n",
        "            xnew = []\n",
        "            for xi in x:\n",
        "                xi_split = xi.split()\n",
        "                xinew = []\n",
        "                for idxi, wi in enumerate(xi_split):\n",
        "                    if idxi!=0 and wi==xi_split[idxi-1]:\n",
        "                        continue\n",
        "                    xinew.append(wi)\n",
        "                xnew.append(' '.join(xinew))\n",
        "            x = xnew\n",
        "        return x\n",
        "\n",
        "def application_disensemble(cin, n_samples=1, level=0, color_adj=None,):\n",
        "    scale = 7.5\n",
        "\n",
        "    ddim_steps = 50\n",
        "    ddim_eta = 0.0\n",
        "    \n",
        "\n",
        "    cin = regularize_image(cin)\n",
        "    ctemp = cin*2 - 1\n",
        "    ctemp = ctemp[None].repeat(n_samples, 1, 1, 1)\n",
        "    \n",
        "    c = net.clip_encode_vision(ctemp)\n",
        "    u = None\n",
        "    if scale != 1.0:\n",
        "        dummy = torch.zeros_like(ctemp)\n",
        "        u = net.clip_encode_vision(dummy)\n",
        "\n",
        "    #savcmpl('blankimg',u)\n",
        "    if level == 0:\n",
        "        pass\n",
        "    else:\n",
        "        c_glb = c[:, 0:1]\n",
        "        c_loc = c[:, 1: ]\n",
        "        u_glb = u[:, 0:1]\n",
        "        u_loc = u[:, 1: ]\n",
        "\n",
        "        if level == -1:\n",
        "            c_loc = remove_low_rank(c_loc, demean=True, q=50, q_remove=1)\n",
        "            u_loc = remove_low_rank(u_loc, demean=True, q=50, q_remove=1)\n",
        "        if level == -2:\n",
        "            c_loc = remove_low_rank(c_loc, demean=True, q=50, q_remove=2)\n",
        "            u_loc = remove_low_rank(u_loc, demean=True, q=50, q_remove=2)\n",
        "        if level == 1:\n",
        "            c_loc = find_low_rank(c_loc, demean=True, q=10)\n",
        "            u_loc = find_low_rank(u_loc, demean=True, q=10)\n",
        "        if level == 2:\n",
        "            c_loc = find_low_rank(c_loc, demean=True, q=2)\n",
        "            u_loc = find_low_rank(u_loc, demean=True, q=2)\n",
        "\n",
        "        c = torch.cat([c_glb, c_loc], dim=1)\n",
        "        u = torch.cat([u_glb, u_loc], dim=1)\n",
        "\n",
        "    h, w = [512, 512]\n",
        "    shape = [n_samples, 4, h//8, w//8]\n",
        "    z, _ = sampler.sample(\n",
        "        steps=ddim_steps,\n",
        "        shape=shape,\n",
        "        conditioning=c,\n",
        "        unconditional_guidance_scale=scale,\n",
        "        unconditional_conditioning=u,\n",
        "        xtype='image', ctype='vision',\n",
        "        eta=ddim_eta,\n",
        "        verbose=False,)\n",
        "    x = decode(z, 'image', 'vision', color_adj=color_adj, color_adj_to=cin)\n",
        "    return x\n",
        "\n",
        "def inference(xtype, cin, ctype, scale=7.5, n_samples=1, color_adj=None,):\n",
        "    ddim_steps = 50\n",
        "    ddim_eta = 0.0\n",
        "\n",
        "    if ctype in ['prompt', 'text']:\n",
        "        c = net.clip_encode_text(n_samples * [cin])\n",
        "        u = None\n",
        "        if scale != 1.0:\n",
        "            u = net.clip_encode_text(n_samples * [\"\"])\n",
        "\n",
        "    elif ctype in ['vision', 'image']:\n",
        "        cin = regularize_image(cin)\n",
        "        ctemp = cin*2 - 1\n",
        "        ctemp = ctemp[None].repeat(n_samples, 1, 1, 1)\n",
        "        c = net.clip_encode_vision(ctemp)\n",
        "        u = None\n",
        "        if scale != 1.0:\n",
        "            dummy = torch.zeros_like(ctemp)\n",
        "            u = net.clip_encode_vision(dummy)\n",
        "\n",
        "    if xtype == 'image':\n",
        "        h, w = [512, 512]\n",
        "        shape = [n_samples, 4, h//8, w//8]\n",
        "        z, _ = sampler.sample(\n",
        "            steps=ddim_steps,\n",
        "            shape=shape,\n",
        "            conditioning=c,\n",
        "            unconditional_guidance_scale=scale,\n",
        "            unconditional_conditioning=u,\n",
        "            xtype=xtype, ctype=ctype,\n",
        "            eta=ddim_eta,\n",
        "            verbose=False,)\n",
        "        x = decode(z, xtype, ctype, color_adj=color_adj, color_adj_to=cin)\n",
        "        return x\n",
        "\n",
        "    elif xtype == 'text':\n",
        "        n = 768\n",
        "        shape = [n_samples, n]\n",
        "        z, _ = sampler.sample(\n",
        "            steps=ddim_steps,\n",
        "            shape=shape,\n",
        "            conditioning=c,\n",
        "            unconditional_guidance_scale=scale,\n",
        "            unconditional_conditioning=u,\n",
        "            xtype=xtype, ctype=ctype,\n",
        "            eta=ddim_eta,\n",
        "            verbose=False,)\n",
        "        x = decode(z, xtype, ctype)\n",
        "        return x\n",
        "\n",
        "def not_txtTB(k):\n",
        "  if k.startswith('optimus.'):\n",
        "    return False\n",
        "  if k.startswith('clip.model.vision_model.'):\n",
        "    return False\n",
        "  if not k.startswith('model.diffusion_model.unet_text.'):\n",
        "    return True\n",
        "  if '.in_layers.' in k:\n",
        "    return False\n",
        "  if '.out_layers.' in k:\n",
        "    return False\n",
        "  return True\n",
        "\n",
        "def not_txtALL(k):\n",
        "  if k.startswith('model.diffusion_model.unet_text.'):\n",
        "    return False\n",
        "  return True"
      ],
      "metadata": {
        "cellView": "form",
        "id": "YoPeWncVSOe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vdoff import dik\n",
        "\n",
        "dout=dict()\n",
        "for k in dik:\n",
        "  #   use `not_txtTB(k)` when (xtype = 'image' & ctype = 'prompt')\n",
        "  if not_txtALL(k):\n",
        "    yfo,offset,shape,stride,grad=dik[k]\n",
        "    typ,fna,device,fsiz=yfo\n",
        "    if typ=='F':\n",
        "      typ=np.float32\n",
        "    elif typ=='I':\n",
        "      typ=np.int64\n",
        "    dout[k]=torch.tensor(np.fromfile('archive/data/'+fna,dtype=typ).reshape(shape))\n",
        "\n",
        "\n",
        "load_state_dict_with_low_memory(net,dout)\n",
        "if use_cuda:\n",
        "  net.to('cuda')"
      ],
      "metadata": {
        "id": "WmaeK8OXSW-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#example\n",
        "!wget -O xipooh.jpg https://i.imgur.com/MYMdnVY.jpg"
      ],
      "metadata": {
        "id": "KDDmUDJrTVbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samp=application_disensemble('xipooh.jpg')"
      ],
      "metadata": {
        "id": "Y_FUeRVwSZJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_image = 0 #@param {type:'integer'}\n",
        "samp[display_image]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mE4s3afUSbnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samp=inference(\n",
        "      xtype = 'image',\n",
        "      cin = 'xipooh.jpg',\n",
        "      ctype = 'vision',\n",
        "      color_adj = None)"
      ],
      "metadata": {
        "id": "G4geAlqKSdwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samp=inference(\n",
        "      xtype = 'image',\n",
        "      cin = 'a dream of a village in china, by Caspar David Friedrich, matte painting trending on artstation HQ',\n",
        "      ctype = 'prompt')"
      ],
      "metadata": {
        "id": "MpKhiy_dSfFr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}