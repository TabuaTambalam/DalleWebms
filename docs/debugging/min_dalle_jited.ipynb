{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "min-dalle_jited.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Min-dalle classes"
      ],
      "metadata": {
        "id": "t42iB1_-AUdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MIN_DALLE_REPO = 'https://huggingface.co/Larvik/mmin-dalle/resolve/main/'\n",
        "import os\n",
        "if not os.path.isfile('/content/once.txt'):\n",
        "  !pip install accelerate\n",
        "  !apt-get install -y libvulkan-dev libomp5\n",
        "  !pip install ncnn-vulkan\n",
        "  !mv /usr/local/lib/python3.7/dist-packages/ncnn_vulkan/*.so /usr/local/lib/python3.7/dist-packages/ncnn/\n",
        "  !pip install emoji\n",
        "  !wget -O /tmp/vq.param https://raw.githubusercontent.com/TabuaTambalam/vqqncnn/main/vq.param\n",
        "  !wget -O /tmp/vq.bin https://github.com/TabuaTambalam/vqqncnn/releases/download/0.0/vq.bin\n",
        "  !wget -O /tmp/vq_vert.param https://raw.githubusercontent.com/TabuaTambalam/vqqncnn/main/vq_vert.param\n",
        "  !git clone https://github.com/kuprel/min-dalle.git\n",
        "  !mv 'min-dalle/min_dalle' min_dalle\n",
        "  !rm -rf '/content/min-dalle'"
      ],
      "metadata": {
        "id": "IU7JMfrTAaLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/once.txt\n",
        "\n",
        "2@0"
      ],
      "metadata": {
        "id": "3Hw5Ft2_Ab0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from threading import Thread\n",
        "import sys\n",
        "import numpy as np\n",
        "from torch import LongTensor, FloatTensor, BoolTensor,nn\n",
        "from math import sqrt\n",
        "import torch\n",
        "import torch.backends.cudnn, torch.backends.cuda\n",
        "import json\n",
        "import requests\n",
        "from typing import Iterator, List, Tuple, Dict\n",
        "from min_dalle.text_tokenizer import TextTokenizer\n",
        "\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "torch.set_num_threads(os.cpu_count())\n",
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True\n",
        "\n",
        "IMAGE_TOKEN_COUNT = 256\n",
        "\n",
        "\n",
        "\n",
        "class DalleBartDecoder():\n",
        "  def __init__(self,jitpath,device):\n",
        "    self.token_indices = torch.arange(IMAGE_TOKEN_COUNT, device=device)\n",
        "    self.jit=torch.jit.load(jitpath)\n",
        "    if UseFP16:\n",
        "      self.jit=self.jit.cuda()\n",
        "\n",
        "  def sample_tokens(self, *args, settings) -> Tuple[LongTensor, FloatTensor]:\n",
        "    logits, attention_state = self.jit(*args)\n",
        "    image_count = logits.shape[0] // 2\n",
        "    temperature = settings[[0]]\n",
        "    top_k = settings[[1]].to(torch.long)\n",
        "    supercondition_factor = settings[[2]]\n",
        "    logits = logits[:, -1, : 2 ** 14]\n",
        "    logits: FloatTensor = (\n",
        "        logits[:image_count] * (1 - supercondition_factor) + \n",
        "        logits[image_count:] * supercondition_factor\n",
        "    )\n",
        "    logits_sorted, _ = logits.sort(descending=True)\n",
        "    is_kept = logits >= logits_sorted[:, top_k - 1]\n",
        "    logits -= logits_sorted[:, [0]]\n",
        "    logits /= temperature\n",
        "    logits.exp_()\n",
        "    logits *= is_kept.to(torch.float32)\n",
        "    image_tokens = torch.multinomial(logits, 1)[:, 0]\n",
        "    return image_tokens, attention_state\n",
        "\n",
        "\n",
        "class MinDalle:\n",
        "    def __init__(\n",
        "        self,\n",
        "        models_root: str = 'pretrained',\n",
        "        dtype: torch.dtype = torch.float32,\n",
        "        device: str = None,\n",
        "        is_reusable: bool = True,\n",
        "        is_verbose = True\n",
        "    ):\n",
        "        if torch.cuda.is_available():\n",
        "          if device == None:\n",
        "              device = 'cuda'\n",
        "        else:\n",
        "          device = 'cpu'\n",
        "          dtype=torch.float32\n",
        "\n",
        "        if UseFP16:\n",
        "          dtype=torch.float16\n",
        "\n",
        "        if is_verbose: print('using device', device)\n",
        "        self.device = device\n",
        "        is_mega = True\n",
        "        self.is_mega = True\n",
        "        self.is_reusable = is_reusable\n",
        "        self.dtype = dtype\n",
        "        self.is_verbose = is_verbose\n",
        "        self.text_token_count = 64\n",
        "        self.layer_count = 24 if is_mega else 12\n",
        "        self.attention_head_count = 32 if is_mega else 16\n",
        "        self.embed_count = 2048 if is_mega else 1024\n",
        "        self.glu_embed_count = 4096 if is_mega else 2730\n",
        "        self.text_vocab_count = 50272 if is_mega else 50264\n",
        "        self.image_vocab_count = 16415 if is_mega else 16384\n",
        "\n",
        "        model_name = 'dalle_bart_{}'.format('mega' if is_mega else 'mini')\n",
        "        dalle_path = os.path.join(models_root, model_name)\n",
        "        vqgan_path = os.path.join(models_root, 'vqgan')\n",
        "        if not os.path.exists(dalle_path): os.makedirs(dalle_path)\n",
        "        if not os.path.exists(vqgan_path): os.makedirs(vqgan_path)\n",
        "        self.vocab_path = os.path.join(dalle_path, 'vocab.json')\n",
        "        self.merges_path = os.path.join(dalle_path, 'merges.txt')\n",
        "        self.encoder_params_path = os.path.join(dalle_path, 'encoder.pt')\n",
        "        self.decoder_params_path = os.path.join(dalle_path, 'decoder.pt')\n",
        "        self.detoker_params_path = os.path.join(vqgan_path, 'detoker.pt')\n",
        "\n",
        "        self.init_tokenizer()\n",
        "        if UseFP16:\n",
        "          self.init_decoder()\n",
        "          self.init_encoder()\n",
        "        else:\n",
        "          self.init_encoder()\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "    def download_tokenizer(self):\n",
        "        if self.is_verbose: print('downloading tokenizer params')\n",
        "        suffix = '' if self.is_mega else '_mini'\n",
        "        vocab = requests.get(MIN_DALLE_REPO + 'vocab{}.json'.format(suffix))\n",
        "        merges = requests.get(MIN_DALLE_REPO + 'merges{}.txt'.format(suffix))\n",
        "        with open(self.vocab_path, 'wb') as f: f.write(vocab.content)\n",
        "        with open(self.merges_path, 'wb') as f: f.write(merges.content)\n",
        "\n",
        "\n",
        "    def download_encoder(self):\n",
        "        if self.is_verbose: print('downloading encoder params')\n",
        "        suffix = '_fp16' if UseFP16 else ''\n",
        "        urli=MIN_DALLE_REPO + 'encoder{}.pt'.format(suffix)\n",
        "        !wget -O {self.encoder_params_path} {urli}\n",
        "\n",
        "\n",
        "    def download_decoder(self):\n",
        "        if self.is_verbose: print('downloading decoder params')\n",
        "        suffix = '_fp16' if UseFP16 else ''\n",
        "        urli=MIN_DALLE_REPO + 'decoder{}.pt'.format(suffix)\n",
        "        !wget -O {self.decoder_params_path} {urli}\n",
        "    \n",
        "\n",
        "    def init_tokenizer(self):\n",
        "        is_downloaded = os.path.exists(self.vocab_path)\n",
        "        is_downloaded &= os.path.exists(self.merges_path)\n",
        "        if not is_downloaded: self.download_tokenizer()\n",
        "        if self.is_verbose: print('intializing TextTokenizer')\n",
        "        with open(self.vocab_path, 'r', encoding='utf8') as f:\n",
        "            vocab = json.load(f)\n",
        "        with open(self.merges_path, 'r', encoding='utf8') as f:\n",
        "            merges = f.read().split('\\n')[1:-1]\n",
        "        self.tokenizer = TextTokenizer(vocab, merges)\n",
        "\n",
        "\n",
        "    def init_encoder(self):\n",
        "        is_downloaded = os.path.exists(self.encoder_params_path)\n",
        "        if not is_downloaded: self.download_encoder()\n",
        "        if self.is_verbose: print('initializing DalleBartEncoder')\n",
        "        self.encoder = torch.jit.load(self.encoder_params_path)\n",
        "        if UseFP16:\n",
        "          self.encoder=self.encoder.cuda()\n",
        "        return\n",
        "\n",
        "    def init_decoder(self):\n",
        "        is_downloaded = os.path.exists(self.decoder_params_path)\n",
        "        if not is_downloaded: self.download_decoder()\n",
        "        if self.is_verbose: print('initializing DalleBartDecoder')\n",
        "        self.decoder=DalleBartDecoder(self.decoder_params_path,device=self.device)\n",
        "        return\n",
        "\n",
        "!rm /content/sample_data/izh.txt\n"
      ],
      "metadata": {
        "id": "3qB093ijAdqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "class ExtractorGPU(object):\n",
        "  def __enter__(self):\n",
        "    self.ex = net.create_extractor()\n",
        "    self.ex.set_blob_vkallocator(blob_vkallocator)\n",
        "    self.ex.set_workspace_vkallocator(blob_vkallocator)\n",
        "    self.ex.set_staging_vkallocator(staging_vkallocator)\n",
        "    return self.ex\n",
        "\n",
        "  def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "    blob_vkallocator.clear()\n",
        "    staging_vkallocator.clear()\n",
        "    self.ex.clear()\n",
        "\n",
        "\n",
        "\n",
        "def mkCBemb(seq):\n",
        "  with NCNNex() as ex:\n",
        "    ex.input('in0', ncnn.Mat(seq).clone())\n",
        "    hrr, out0 = ex.extract('2')\n",
        "  del ex\n",
        "  return out0\n",
        "\n",
        "def emb2img(emb):\n",
        "  with NCNNex() as ex:\n",
        "    ex.input('2', emb)\n",
        "    hrr, out0 = ex.extract('out0')\n",
        "  del ex\n",
        "  return Image.fromarray(np.array(out0).astype(np.uint8))\n",
        "\n",
        "def npmkCBemb(seq):\n",
        "  with NCNNex() as ex:\n",
        "    ex.input('in0', ncnn.Mat(seq.astype(np.uint32)).clone())\n",
        "    hrr, out0 = ex.extract('2')\n",
        "  del ex\n",
        "  return np.array(out0)\n",
        "\n",
        "\n",
        "def npemb2img(emb):\n",
        "  with NCNNex() as ex:\n",
        "    ex.input('2', ncnn.Mat(emb).clone())\n",
        "    hrr, out0 = ex.extract('out0')\n",
        "  del ex\n",
        "  return Image.fromarray(np.array(out0).astype(np.uint8))\n",
        "\n",
        "def pbla(step,scale):\n",
        "  ret=[]\n",
        "  mga=4-(4/scale)\n",
        "  k=step-1\n",
        "  for i in range(step):\n",
        "    ret.append(k+mga*( ((i**2)/k) - i ))\n",
        "  return ret\n",
        "\n",
        "\n",
        "def img_float():\n",
        "  with NCNNex() as ex:\n",
        "      ex.input('in0', ncnn.Mat(curfull[:,sta:endo].reshape(16*(endo-sta)).astype(np.uint32)).clone())\n",
        "      hrr, out0 = ex.extract('252')\n",
        "  del ex\n",
        "  return torch.FloatTensor(np.array(out0)).to(device).unsqueeze(0)\n",
        "\n",
        "\n",
        "def npimg_float(seq):\n",
        "  with NCNNex() as ex:\n",
        "      ex.input('in0', ncnn.Mat(seq.astype(np.uint32)).clone())\n",
        "      hrr, out0 = ex.extract('252')\n",
        "  del ex\n",
        "  return np.array(out0)\n",
        "\n",
        "\n",
        "def hstack(sta,n=16,crop=[]):\n",
        "  haf=[]\n",
        "  for k in range(n):\n",
        "    haf.append(dumped_seqs[sta+k].reshape((16,16))[:,:8])\n",
        "  if crop:\n",
        "    sta=crop[0]>>4\n",
        "    endo=(crop[0]+crop[1])>>4\n",
        "    return np.hstack(haf)[:,sta:endo].reshape(16*(endo-sta))\n",
        "  else:\n",
        "    return np.hstack(haf).reshape(128*n)\n",
        "    \n",
        "\n",
        "def fromsteps(liz):\n",
        "  li=len(liz)-1\n",
        "  haf=[]\n",
        "  for n in range(li):\n",
        "    haf.append(np.fromfile('/content/steps/s%d.bin'%n,dtype=np.uint16).astype(np.int32).reshape((-1,256))[liz[n]].reshape((16,16))[:,:8])\n",
        "  haf.append(np.fromfile('/content/steps/s%d.bin'%li,dtype=np.uint16).astype(np.int32).reshape((-1,256))[liz[li]].reshape((16,16)))\n",
        "  return np.hstack(haf).reshape(256 + 128*li)\n",
        "\n",
        "\n",
        "def mk3x3(idx):\n",
        "  rowz=[]\n",
        "  for y in range(3):\n",
        "    colz=[]\n",
        "    for x in range(3):\n",
        "      colz.append(dumped_seqs[idx[y*3+x]].reshape((16,16)))\n",
        "    rowz.append(np.hstack(colz).reshape(768))\n",
        "  return np.concatenate(rowz)\n",
        "\n",
        "def interpo(seq1,seq2,step=30,scale=1.21,outfmt='/content/avif/%02d.png'):\n",
        "  stp=step-1\n",
        "  divi=pbla(step,scale)\n",
        "  em1=npmkCBemb(seq1)\n",
        "  em2=npmkCBemb(seq2)\n",
        "  for i in range(step):\n",
        "    npemb2img((em1*i+em2*(stp-i))/divi[i]).save(outfmt%i)\n",
        "\n",
        "def interpoC():\n",
        "  !mkdir /content/avif\n",
        "  for n in range(candidate_count):\n",
        "    interpo(dumped_seqs[n],dumped_seqs[n+1])\n",
        "    !/tmp/ffmpeg/ffmpeg -framerate 18 -i /content/avif/%02d.png -sn -map_metadata -1 -map_chapters -1 -crf 10 -c:v libaom-av1 -aom-params enable-keyframe-filtering=0:enable-tpl-model=1 -lag-in-frames 48 -cpu-used 5 -row-mt 1 -tiles 1x1 -threads 2 -strict experimental -movflags +faststart -flags +cgop -pix_fmt yuv420p10le -c:a libopus -b:a 96k -ac 2 -f webm /content/avif/intp.webm\n",
        "    os.rename('/content/avif/intp.webm','/content/avif/v%02d.webm'%n)\n",
        "\n",
        "def showp(n, prt=False):\n",
        "  global mlat\n",
        "  global mfn\n",
        "  daaz=dumped_seqs[n]\n",
        "  with NCNNex() as ex:\n",
        "    ex.input('in0', ncnn.Mat(daaz.astype(np.uint32)).clone())\n",
        "    hrr, out0 = ex.extract('out0')\n",
        "  del ex\n",
        "  uz=Image.fromarray(np.array(out0).astype(np.uint8))\n",
        "  uz.save('/content/sample_data/%d.png'%n)\n",
        "  if prt:\n",
        "    display(uz)\n",
        "    mfn='%d'%n\n",
        "    mlat=str(daaz)[1:-1]\n",
        "    return mlat\n",
        "  else:\n",
        "    return uz\n",
        "\n",
        "def showp2(seq):\n",
        "  with NCNNex() as ex:\n",
        "    ex.input('in0', ncnn.Mat(seq.astype(np.uint32)).clone())\n",
        "    hrr, out0 = ex.extract('out0')\n",
        "  del ex\n",
        "  uz=Image.fromarray(np.array(out0).astype(np.uint8))\n",
        "  uz.save('/content/sample_data/000.png')\n",
        "  return uz\n",
        "\n",
        "def hcopy(tk,left_sta=8):\n",
        "  len=16-left_sta\n",
        "  for y in range(16):\n",
        "    mae=1+y*16\n",
        "    tk[mae:mae+len]=tk[mae+left_sta:mae+16]\n",
        "\n",
        "def hcopy_dup(tk,sele,left_sta=8):\n",
        "  src=sele.expand(candidate_count,-1)\n",
        "  len=16-left_sta\n",
        "  for y in range(16):\n",
        "    mae=1+y*16\n",
        "    tk[:,mae:mae+len]=src[:,mae+left_sta:mae+16]\n",
        "  for as0 in range(mindd.layer_count):\n",
        "    for p in range(4):\n",
        "      pkan=p*candidate_count\n",
        "      sle4=attention_state[as0][pkan+candidate_select]\n",
        "      for j in range(candidate_count):\n",
        "        attention_state[as0][pkan+j][:]=sle4[:]\n",
        "\n",
        "def hcopy_dst(src,tk,len=8):\n",
        "  for y in range(16):\n",
        "    mae=y*16\n",
        "    tk[1+mae:1+mae+len]=torch.from_numpy(src[mae:mae+len])\n",
        "\n",
        "def rumpla():\n",
        "  global attention_state\n",
        "  for row_index in range(   ROW_START   ,16):\n",
        "    print('%x:'%row_index, end='')\n",
        "    kt=16 * row_index\n",
        "    for col_index in range(COL_START,16):\n",
        "      i =  kt + col_index       \n",
        "      with torch.cuda.amp.autocast(dtype=mindd.dtype):\n",
        "          image_tokens[:,i + 1], attention_state = mindd.decoder.sample_tokens(\n",
        "              attention_mask,encoder_state,attention_state,image_tokens[:,[i]],token_indices[[i]],settings=settings\n",
        "          )\n",
        "\n",
        "\n",
        "def rumplaL(glist):\n",
        "  global attention_state\n",
        "  dr=16-len(glist)\n",
        "  for row_index in range(   dr   ,16):\n",
        "    print('%x:'%row_index, end='')\n",
        "    kt=16 * row_index\n",
        "    for col_index in range( glist[row_index-dr] ,16):\n",
        "      i =  kt + col_index       \n",
        "      with torch.cuda.amp.autocast(dtype=mindd.dtype):\n",
        "          image_tokens[:,i + 1], attention_state = mindd.decoder.sample_tokens(\n",
        "              attention_mask,encoder_state,attention_state,image_tokens[:,[i]],token_indices[[i]],settings=settings\n",
        "          )\n",
        "\n",
        "def rudallestuff():\n",
        "  if not os.path.isfile('/content/model-ru-latest.pt'):\n",
        "    !git clone https://github.com/sberbank-ai/Real-ESRGAN > /dev/null\n",
        "    !git clone https://github.com/Jack000/guided-diffusion > /dev/null\n",
        "    !pip install rudalle > /dev/null\n",
        "    !pip install -e ./guided-diffusion > /dev/null\n",
        "    !pip install -r Real-ESRGAN/requirements.txt > /dev/null\n",
        "    !wget https://huggingface.co/shonenkov/rudalle-utils/resolve/main/RealESRGAN_x2.pth > /dev/null\n",
        "    !wget https://huggingface.co/shonenkov/rudalle-utils/resolve/main/RealESRGAN_x4.pth > /dev/null\n",
        "    !wget https://dall-3.com/models/guided-diffusion/ru-dalle/model-ru-latest.pt > /dev/null\n",
        "\n",
        "def mksettings(top_k0,temperature0,supercondition_factor0):\n",
        "  return torch.tensor(\n",
        "    [temperature0, top_k0, supercondition_factor0], \n",
        "    dtype=mindd.dtype,\n",
        "    device=mindd.device\n",
        ")\n",
        "  \n",
        "def gen0(dr=0,dc=0):\n",
        "  global ROW_START\n",
        "  global COL_START\n",
        "  global dumped_seqs\n",
        "  ROW_START=dr\n",
        "  COL_START=dc\n",
        "  rumpla()\n",
        "  dumped_seqs=image_tokens[:, 1:].to('cpu').numpy().astype(np.uint16)\n",
        "  with open('/content/ozv.bin',mode='ba+') as f:\n",
        "    dumped_seqs.tofile(f)\n",
        "\n",
        "\n",
        "def gen0L(glist):\n",
        "  global dumped_seqs\n",
        "  rumplaL(glist)\n",
        "  dumped_seqs=image_tokens[:, 1:].to('cpu').numpy().astype(np.uint16)\n",
        "  with open('/content/ozv.bin',mode='ba+') as f:\n",
        "    dumped_seqs.tofile(f)\n",
        "\n",
        "\n",
        "def chkstz(stz):\n",
        "  lstz=len(stz)\n",
        "  if lstz < 2:\n",
        "    return True\n",
        "  if stz[1] == '':\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "\n",
        "def gen1():\n",
        "  torch.set_grad_enabled(False)\n",
        "  dr=ROW_START\n",
        "  dc=COL_START\n",
        "  useL=False\n",
        "  while os.path.isfile('/content/once.txt'):\n",
        "    if useL:\n",
        "      gen0L(glist)\n",
        "    else:\n",
        "      gen0(dr,dc)\n",
        "    try:\n",
        "      with open('/content/once.txt','rt') as f:\n",
        "        stz=f.read().replace(' ','').splitlines()\n",
        "      if stz[0] == '':\n",
        "        dr=dr\n",
        "      elif chkstz(stz):\n",
        "        useL=False\n",
        "        stz=stz[0].split('@')\n",
        "        lstz=len(stz)\n",
        "        if lstz > 0:\n",
        "          dr=int(stz[0])\n",
        "        if lstz > 1:\n",
        "          dc=int(stz[1])\n",
        "        if lstz > 2:\n",
        "          settings[1]=float(stz[2])\n",
        "          if lstz > 3:\n",
        "            settings[0]=float(stz[3])\n",
        "          if lstz > 4:\n",
        "            settings[2]=float(stz[4])\n",
        "      else:\n",
        "        glist=[int(x) for x in stz if x.isdigit()][:16]\n",
        "        useL=True\n",
        "    except:\n",
        "      dr=dr\n",
        "  os.rename('/content/-.txt','/content/once.txt')\n",
        "\n",
        "def localhttp():\n",
        "  global HTML\n",
        "  if not os.path.isfile('/content/sample_data/izh.txt'):\n",
        "    from IPython.core.display import HTML\n",
        "    !nohup python3 -m http.server -d /content/sample_data/ 8233 > /content/sample_data/izh.txt &\n",
        "\n",
        "\n",
        "def prmp(filltoken=False):\n",
        "  global seed\n",
        "  global text_tokens\n",
        "  global image_tokens\n",
        "  global attention_mask\n",
        "  global encoder_state\n",
        "  global attention_state\n",
        "  if is_verbose: print('tokenizing text')\n",
        "  tokens = mindd.tokenizer.tokenize(text, is_verbose=is_verbose)\n",
        "  if len(tokens) > mindd.text_token_count: \n",
        "      tokens = tokens[:mindd.text_token_count]\n",
        "  if is_verbose: print('{} text tokens'.format(len(tokens)), tokens)\n",
        "  text_tokens = np.ones((2, 64), dtype=np.int32)\n",
        "  text_tokens[0, :2] = [tokens[0], tokens[-1]]\n",
        "  text_tokens[1, :len(tokens)] = tokens\n",
        "  dev='cpu'\n",
        "  if UseFP16:\n",
        "    dev=mindd.device\n",
        "  text_tokens = torch.tensor(\n",
        "      text_tokens, \n",
        "      dtype=torch.long, \n",
        "      device=dev\n",
        "  )\n",
        "\n",
        "  if not mindd.is_reusable: mindd.init_encoder()\n",
        "  if is_verbose: print('encoding text tokens')\n",
        "  with torch.cuda.amp.autocast(dtype=mindd.dtype):\n",
        "      encoder_state = mindd.encoder(text_tokens)\n",
        "  if not mindd.is_reusable: del mindd.encoder\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  if not mindd.is_reusable: mindd.init_decoder()\n",
        "\n",
        "  with torch.cuda.amp.autocast(dtype=mindd.dtype):\n",
        "      expanded_indices = [0] * candidate_count + [1] * candidate_count\n",
        "      text_tokens = text_tokens[expanded_indices]\n",
        "      encoder_state = encoder_state[expanded_indices]\n",
        "      attention_mask = text_tokens.not_equal(1)[:, None, None, :]\n",
        "      if filltoken:\n",
        "        attention_state = torch.zeros(\n",
        "            size=(\n",
        "                mindd.layer_count,\n",
        "                candidate_count * 4,\n",
        "                IMAGE_TOKEN_COUNT,\n",
        "                mindd.embed_count\n",
        "            ), \n",
        "            device=mindd.device\n",
        "        )\n",
        "        image_tokens = torch.full(\n",
        "            (candidate_count, IMAGE_TOKEN_COUNT + 1), \n",
        "            mindd.image_vocab_count,\n",
        "            dtype=torch.long,\n",
        "            device=mindd.device\n",
        "        )\n",
        "      \n",
        "      if seed == 0:\n",
        "        seed=random.randint(0, 2**32)\n",
        "        print('rndseed: '+str(seed)) \n",
        "      torch.manual_seed(seed)\n",
        "        \n",
        "\n",
        "newstart=True"
      ],
      "metadata": {
        "id": "tiuMcGQTBgaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PlayGround"
      ],
      "metadata": {
        "id": "2odKotxyCbE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "UseFP16=False #@param {type:'boolean'}\n",
        "PrepareRuDalleStuff=False #@param {type:'boolean'}\n",
        "if newstart:\n",
        "  mindd = MinDalle(is_reusable=True)\n",
        "  import ncnn\n",
        "  import gc\n",
        "  net = ncnn.Net()\n",
        "  net.opt.use_vulkan_compute = False\n",
        "  NCNNex=net.create_extractor\n",
        "  net.load_param(  '/tmp/vq.param'   )  #\t'/content/vq3x3.txt'\n",
        "  net.load_model('/tmp/vq.bin')\n",
        "  newstart=False\n",
        "  !nvidia-smi\n",
        "if PrepareRuDalleStuff:\n",
        "  t1 = Thread(target = rudallestuff)\n",
        "  a1 = t1.start()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "9gORwEf_CbsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text =\"desert oasis merchant market high fantasy book cover painting\" #@param {type:'string'}\n",
        "candidate_count =  2#@param {type:'integer'}\n",
        "seed= 775577  #@param {type:'integer'}\n",
        "log_everything = False #@param {type:'boolean'}\n",
        "\n",
        "'''\n",
        "555\n",
        "'''\n",
        "\n",
        "is_verbose=False\n",
        "\n",
        "\n",
        "prmp(True)\n",
        "\n",
        "\n",
        "userselect=[]\n",
        "userselectN=[0]*128\n",
        "\n",
        "!rm -rf /content/steps\n",
        "!mkdir /content/steps\n",
        "!rm /content/sample_data/*.png\n",
        "\n",
        "if log_everything:\n",
        "  with open('/content/steps/prompt.txt','wt') as f:\n",
        "    f.write(text+'\\ntokens='+str(tokens))\n",
        "  torch.save(attention_mask, '/content/steps/attention_mask.pt')\n",
        "  torch.save(encoder_state, '/content/steps/encoder_state.pt')\n",
        "\n",
        "\n",
        "ROW_START =0\n",
        "COL_START =0\n",
        "step=0\n",
        "newprompt=True"
      ],
      "metadata": {
        "cellView": "form",
        "id": "5hGBXGGmCo5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del mindd.encoder\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "r-EkTmD1CvK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mindd.init_decoder()\n",
        "token_indices = mindd.decoder.token_indices"
      ],
      "metadata": {
        "id": "P_BTi35wCwrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_select=0 #@param {type:'integer'}\n",
        "top_k = 2048 #@param {type:'integer'}\n",
        "temperature = 3  #@param {type:'integer'}\n",
        "supercondition_factor = 64 #@param {type:'integer'}\n",
        "\n",
        "\n",
        "r_ROW_START =35 #@param {type:'integer'}\n",
        "r_COL_START =0 #@param {type:'integer'}\n",
        "\n",
        "settings=mksettings(top_k,temperature,supercondition_factor)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if newprompt:\n",
        "  newprompt=False\n",
        "  gen0()\n",
        "  for n in range(candidate_count):\n",
        "    print('init%d'%n,end='')\n",
        "    display(showp(n))\n",
        "elif candidate_select < candidate_count:\n",
        "  image_tokens[:]=image_tokens[candidate_select].expand(candidate_count,-1)[:]\n",
        "  for as0 in range(mindd.layer_count):\n",
        "    for p in range(4):\n",
        "      pkan=p*candidate_count\n",
        "      sle4=attention_state[as0][pkan+candidate_select]\n",
        "      for j in range(candidate_count):\n",
        "        attention_state[as0][pkan+j][:]=sle4[:]\n",
        "  del sle4\n",
        "  ROW_START=r_ROW_START>>4\n",
        "  COL_START=r_COL_START>>4\n",
        "  print('infinite gen started in thread')\n",
        "  t1 = Thread(target = gen1)\n",
        "  a1 = t1.start()\n",
        "else:\n",
        "  gen0()\n",
        "  for n in range(candidate_count):\n",
        "    print('init%d'%n,end='')\n",
        "    display(showp(n))\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "byJ_fTNaC0T7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}