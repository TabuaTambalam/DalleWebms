{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GRADIOdd2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJUJLWQ92g6R"
      },
      "source": [
        "!pip install einops einops-exts kornia ftfy vector-quantize-pytorch resize-right clip-anytorch rotary-embedding-torch coca-pytorch pytorch-warmup ema-pytorch x-clip accelerate gradio\n",
        "!git clone https://github.com/LAION-AI/dalle2-laion.git\n",
        "!mv 'dalle2-laion/dalle2_laion' dalle2_laion\n",
        "!git clone https://github.com/lucidrains/DALLE2-pytorch.git\n",
        "!mv 'DALLE2-pytorch/dalle2_pytorch' dalle2_pytorch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Any, Tuple, Optional, TypeVar, Generic, List,Iterator,  Dict\n",
        "from dalle2_laion.config import DecoderLoadConfig, SingleDecoderLoadConfig, PriorLoadConfig, ModelLoadConfig\n",
        "from dalle2_pytorch import __version__ as Dalle2Version, Decoder, DiffusionPrior, Unet\n",
        "from dalle2_pytorch.train_configs import TrainDecoderConfig, TrainDiffusionPriorConfig, DecoderConfig, UnetConfig, DiffusionPriorConfig\n",
        "import torch\n",
        "from torch import LongTensor, FloatTensor, BoolTensor,nn\n",
        "from packaging import version\n",
        "\n",
        "from accelerate import init_empty_weights\n",
        "\n",
        "\n",
        "def get_keys_to_submodule(model: nn.Module) -> Dict[str, nn.Module]:\n",
        "    keys_to_submodule = {}\n",
        "    # iterate all submodules\n",
        "    for submodule_name, submodule in model.named_modules():\n",
        "        # iterate all paramters in each submobule\n",
        "        for param_name, param in submodule.named_parameters():\n",
        "            # param_name is organized as <name>.<subname>.<subsubname> ...\n",
        "            splitted_param_name = param_name.split('.')\n",
        "            # we cannot go inside it anymore. This is the actual parameter\n",
        "            is_leaf_param = len(splitted_param_name) == 1\n",
        "            if is_leaf_param:\n",
        "                # we recreate the correct key\n",
        "                key = f\"{submodule_name}.{param_name}\"\n",
        "                # we associate this key with this submodule\n",
        "                keys_to_submodule[key] = submodule\n",
        "                \n",
        "    return keys_to_submodule\n",
        "\n",
        "def load_state_dict_with_low_memory(model: nn.Module, state_dict):\n",
        "    print('======hacky load======')\n",
        "    # free up memory by placing the model in the `meta` device\n",
        "    keys_to_submodule = get_keys_to_submodule(model)\n",
        "    mste=model.state_dict()\n",
        "    for key, submodule in keys_to_submodule.items():\n",
        "        # get the valye from the state_dict\n",
        "        if key in state_dict:\n",
        "          val = state_dict[key]\n",
        "        else:\n",
        "          print(key)\n",
        "          val = torch.ones(mste[key].shape, dtype= torch.float16)\n",
        "        # we need to substitute the parameter inside submodule, \n",
        "        # remember key is composed of <name>.<subname>.<subsubname>\n",
        "        # the actual submodule's parameter is stored inside the \n",
        "        # last subname. If key is `in_proj.weight`, the correct field if `weight`\n",
        "        param_name = key.split('.')[-1]\n",
        "        #param_dtype = getattr(submodule, param_name).dtype\n",
        "        #val = val.to(param_dtype)\n",
        "        # create a new parameter\n",
        "        new_val = torch.nn.Parameter(val)\n",
        "        setattr(submodule, param_name, new_val)\n",
        "\n",
        "\n",
        "\n",
        "def exists(obj: Any) -> bool:\n",
        "    return obj is not None\n",
        "\n",
        "@dataclass\n",
        "class DataRequirements:\n",
        "    image_embedding: bool\n",
        "    text_encoding: bool\n",
        "    image: bool\n",
        "    text: bool\n",
        "    can_generate_embedding: bool\n",
        "    image_size: int\n",
        "\n",
        "    def has_clip(self):\n",
        "        self.can_generate_embedding = True\n",
        "\n",
        "    def is_valid(\n",
        "        self,\n",
        "        has_image_emb: bool = False, has_text_encoding: bool = False,\n",
        "        has_image: bool = False, has_text: bool = False,\n",
        "        image_size: Optional[int] = None\n",
        "    ):\n",
        "        # The image size must be equal to or greater than the required size\n",
        "        # Verify that the text input is valid\n",
        "        errors = []\n",
        "        is_valid = True\n",
        "        if self.text_encoding:\n",
        "            # Then we need to some way to get the text encoding\n",
        "            if not (has_text_encoding or (self.can_generate_embedding and has_text)):\n",
        "                errors.append('Text encoding is required, but no text encoding or text was provided')\n",
        "                is_valid = False\n",
        "        if self.text:\n",
        "            # Then this requires text be passed in explicitly\n",
        "            if not has_text:\n",
        "                errors.append('Text is required, but no text was provided')\n",
        "                is_valid = False\n",
        "\n",
        "        # Verify that the image input is valid\n",
        "        image_size_greater = exists(image_size) and image_size >= self.image_size\n",
        "        if self.image_embedding:\n",
        "            # Then we need to some way to get the image embedding\n",
        "            # In this case, we also need to make sure that the image size is big enough to generate the embedding\n",
        "            if not (has_image_emb or (self.can_generate_embedding and has_image and image_size_greater)):\n",
        "                errors.append('Image embedding is required, but no image embedding or image was provided or the image was too small')\n",
        "                is_valid = False\n",
        "        if self.image:\n",
        "            # Then this requires an image be passed in explicitly\n",
        "            # In this case we also need to make sure the image is big enough to be used\n",
        "            if not (has_image and image_size_greater):\n",
        "                errors.append('Image is required, but no image was provided or the image was too small')\n",
        "                is_valid = False\n",
        "        return is_valid, errors\n",
        "\n",
        "    def __add__(self, other: 'DataRequirements') -> 'DataRequirements':\n",
        "        return DataRequirements(\n",
        "            image_embedding=self.image_embedding or other.image_embedding,  # If either needs an image embedding, the combination needs one\n",
        "            text_embedding=self.text_embedding or other.text_embedding,  # If either needs a text embedding, the combination needs one\n",
        "            image=self.image or other.image,  # If either needs an image, the combination needs it  \n",
        "            text=self.text or other.text,  # If either needs a text, the combination needs it\n",
        "            can_generate_embedding=self.can_generate_embedding and other.can_generate_embedding,  # If either cannot generate an embedding, we know that trying to replace an embedding with raw data will not work\n",
        "            image_size=max(self.image_size, other.image_size)  # We can downsample without loss of information, so we use the larger image size\n",
        "        )\n",
        "\n",
        "ModelType = TypeVar('ModelType', Decoder, DiffusionPrior)\n",
        "\n",
        "@dataclass\n",
        "class ModelInfo(Generic[ModelType]):\n",
        "    model: ModelType\n",
        "    model_version: Optional[version.Version]\n",
        "    requires_clip: bool\n",
        "    data_requirements: DataRequirements\n",
        "\n",
        "class DalleModelManager:\n",
        "    \"\"\"\n",
        "    Used to load priors and decoders and to provide a simple interface to run general scripts against\n",
        "    \"\"\"\n",
        "    def __init__(self, model_load_config: ModelLoadConfig, check_updates: bool = True):\n",
        "        \"\"\"\n",
        "        Downloads the models and loads them into memory.\n",
        "        If check_updates is True, then the models will be re-downloaded if checksums do not match.\n",
        "        \"\"\"\n",
        "        self.check_updates = check_updates\n",
        "        self.model_config = model_load_config\n",
        "        self.current_version = version.parse(Dalle2Version)\n",
        "        self.single_device = isinstance(model_load_config.devices, str)\n",
        "        self.devices = [torch.device(model_load_config.devices)] if self.single_device else [torch.device(d) for d in model_load_config.devices]\n",
        "        self.load_device = torch.device('cpu') if model_load_config.load_on_cpu else self.devices[0]\n",
        "        self.strict_loading = model_load_config.strict_loading\n",
        "\n",
        "        if model_load_config.decoder is not None:\n",
        "            self.decoder_info = self.load_decoder(model_load_config.decoder)\n",
        "        else:\n",
        "            self.decoder_info = None\n",
        "\n",
        "        if model_load_config.prior is not None:\n",
        "            self.prior_info = self.load_prior(model_load_config.prior)\n",
        "        else:\n",
        "            self.prior_info = None\n",
        "\n",
        "        if (exists(self.decoder_info) and self.decoder_info.requires_clip) or (exists(self.prior_info) and self.prior_info.requires_clip):\n",
        "            assert model_load_config.clip is not None, 'Your model requires clip to be loaded. Please provide a clip config.'\n",
        "            self.clip = model_load_config.clip.create()\n",
        "            # Update the data requirements to include the clip model\n",
        "            if exists(self.decoder_info):\n",
        "                self.decoder_info.data_requirements.has_clip()\n",
        "            if exists(self.prior_info):\n",
        "                self.prior_info.data_requirements.has_clip()\n",
        "        else:\n",
        "            if model_load_config.clip is not None:\n",
        "                print(f'WARNING: Your model does not require clip, but you provided a clip config. This will be ignored.')\n",
        "\n",
        "    def _get_decoder_data_requirements(self, decoder_config: DecoderConfig, min_unet_number: int = 1) -> DataRequirements:\n",
        "        \"\"\"\n",
        "        Returns the data requirements for a decoder\n",
        "        \"\"\"\n",
        "        return DataRequirements(\n",
        "            image_embedding=True,\n",
        "            text_encoding=any(unet_config.cond_on_text_encodings for unet_config in decoder_config.unets[min_unet_number - 1:]),\n",
        "            image=min_unet_number > 1,  # If this is an upsampler we need an image\n",
        "            text=False,  # Text is never required for anything\n",
        "            can_generate_embedding=False,  # This might be added later if clip is being used\n",
        "            image_size=decoder_config.image_sizes[min_unet_number - 1]  # The input image size is the input to the first unet we are using\n",
        "        )\n",
        "\n",
        "    def _load_single_decoder(self, load_config: SingleDecoderLoadConfig) -> Tuple[Decoder, DecoderConfig, Optional[version.Version], bool]:\n",
        "        \"\"\"\n",
        "        Loads a single decoder from a model and a config file\n",
        "        \"\"\"\n",
        "        unet_sample_timesteps = load_config.default_sample_timesteps\n",
        "        def apply_default_config(config: DecoderConfig):\n",
        "            if unet_sample_timesteps is not None:\n",
        "                base_sample_timesteps = [None] * len(config.unets)\n",
        "                for unet_number, timesteps in zip(load_config.unet_numbers, unet_sample_timesteps):\n",
        "                    base_sample_timesteps[unet_number - 1] = timesteps\n",
        "                config.sample_timesteps = base_sample_timesteps\n",
        "        \n",
        "        with load_config.load_model_from.as_local_file(check_update=self.check_updates) as model_file:\n",
        "            model_state_dict = torch.load(model_file, map_location=self.load_device)\n",
        "            if 'version' in model_state_dict:\n",
        "                model_version = model_state_dict['version']\n",
        "                if model_version != self.current_version:\n",
        "                    print(f'WARNING: This decoder was trained on version {model_version} but the current version is {self.current_version}. This may result in the model failing to load.')\n",
        "                    print(f'FIX: Switch to this version with `pip install DALLE2-pytorch=={model_version}`. If different models suggest different versions, you may just need to choose one.')\n",
        "            else:\n",
        "                print(f'WARNING: This decoder was trained on an old version of Dalle2. This may result in the model failing to load or it may lead to producing garbage results.')\n",
        "                model_version = None  # No version info in the model\n",
        "            \n",
        "            requires_clip = False\n",
        "            if 'config' in model_state_dict:\n",
        "                # Then we define the decoder config from this object\n",
        "                decoder_config = TrainDecoderConfig(**model_state_dict['config']).decoder\n",
        "                apply_default_config(decoder_config)\n",
        "                if decoder_config.clip is not None:\n",
        "                    # We don't want to load clip with the model\n",
        "                    requires_clip = True\n",
        "                    decoder_config.clip = None\n",
        "                with init_empty_weights():\n",
        "                  decoder = decoder_config.create()\n",
        "                load_state_dict_with_low_memory(decoder, model_state_dict['model'])\n",
        "                #decoder.load_state_dict(model_state_dict['model'], strict=self.strict_loading)  # If the model has a config included, then we know the model_state_dict['model'] is the actual model\n",
        "                decoder.requires_grad_(False).eval()\n",
        "            else:\n",
        "                # In this case, the state_dict is the model itself. This means we also must load the config from an external file\n",
        "                assert load_config.load_config_from is not None\n",
        "                with load_config.load_config_from.as_local_file(check_update=self.check_updates) as config_file:\n",
        "                    decoder_config = TrainDecoderConfig.from_json_path(config_file).decoder\n",
        "                    apply_default_config(decoder_config)\n",
        "                    if decoder_config.clip is not None:\n",
        "                        # We don't want to load clip with the model\n",
        "                        requires_clip = True\n",
        "                        decoder_config.clip = None\n",
        "                with init_empty_weights():\n",
        "                  decoder = decoder_config.create()\n",
        "                load_state_dict_with_low_memory(decoder, model_state_dict)\n",
        "                #decoder.load_state_dict(model_state_dict, strict=self.strict_loading)\n",
        "                decoder.requires_grad_(False).eval()\n",
        "            del model_state_dict\n",
        "            return decoder, decoder_config, model_version, requires_clip\n",
        "\n",
        "    def load_decoder(self, load_config: DecoderLoadConfig) -> 'ModelInfo[Decoder]':\n",
        "        \"\"\"\n",
        "        Loads a decoder from a model and a config file\n",
        "        \"\"\"\n",
        "        if len(load_config.unet_sources) == 1:\n",
        "            # Then we are loading only one model\n",
        "            decoder, decoder_config, decoder_version, requires_clip = self._load_single_decoder(load_config.unet_sources[0])\n",
        "            decoder_data_requirements = self._get_decoder_data_requirements(decoder_config)\n",
        "            decoder.to(torch.float32)\n",
        "            return ModelInfo(decoder, decoder_version, requires_clip, decoder_data_requirements)\n",
        "        else:\n",
        "            true_unets: List[Unet] = [None] * load_config.final_unet_number  # Stores the unets that will replace the ones in the true decoder\n",
        "            true_unet_configs: List[UnetConfig] = [None] * load_config.final_unet_number  # Stores the unet configs that will replace the ones in the true decoder config\n",
        "            true_upsampling_sizes: List[Tuple[int, int]] = [None] * load_config.final_unet_number  # Stores the progression of upsampling sizes for each unet so that we can validate these unets actually work together\n",
        "            true_train_timesteps: List[int] = [None] * load_config.final_unet_number  # Stores the number of timesteps that each unet trained with\n",
        "            true_beta_schedules: List[str] = [None] * load_config.final_unet_number  # Stores the beta scheduler that each unet used\n",
        "            true_uses_learned_variance: List[bool] = [None] * load_config.final_unet_number  # Stores whether each unet uses learned variance\n",
        "            true_sample_timesteps: List[int] = [None] * load_config.final_unet_number  # Stores the number of timesteps that each unet used to sample\n",
        "\n",
        "            requires_clip = False\n",
        "            for source in load_config.unet_sources:\n",
        "                decoder, decoder_config, decoder_version, unets_requires_clip = self._load_single_decoder(source)\n",
        "                if unets_requires_clip:\n",
        "                    requires_clip = True\n",
        "                if source.default_sample_timesteps is not None:\n",
        "                    assert len(source.default_sample_timesteps) == len(source.unet_numbers)\n",
        "                for i, unet_number in enumerate(source.unet_numbers):\n",
        "                    unet_index = unet_number - 1\n",
        "                    # Now we need to insert the unet into the true unets and the unet config into the true config\n",
        "                    if source.default_sample_timesteps is not None:\n",
        "                        true_sample_timesteps[unet_index] = source.default_sample_timesteps[i]\n",
        "                    true_unets[unet_index] = decoder.unets[unet_index]\n",
        "                    true_unet_configs[unet_index] = decoder_config.unets[unet_index]\n",
        "                    true_upsampling_sizes[unet_index] = None if unet_index == 0 else decoder_config.image_sizes[unet_index - 1], decoder_config.image_sizes[unet_index]\n",
        "                    true_train_timesteps[unet_index] = decoder_config.timesteps\n",
        "                    true_beta_schedules[unet_index] = decoder_config.beta_schedule[unet_index]\n",
        "                    true_uses_learned_variance[unet_index] = decoder_config.learned_variance if isinstance(decoder_config.learned_variance, bool) else decoder_config.learned_variance[unet_index]\n",
        "\n",
        "            true_decoder_config_obj = {}\n",
        "            # Insert the true configs into the true decoder config\n",
        "            true_decoder_config_obj['unets'] = true_unet_configs\n",
        "            true_image_sizes = []\n",
        "            for i in range(load_config.final_unet_number):\n",
        "                if i == 0:\n",
        "                    true_image_sizes.append(true_upsampling_sizes[i][1])\n",
        "                else:\n",
        "                    assert true_upsampling_sizes[i - 1][1] == true_upsampling_sizes[i][0], f\"The upsampling sizes for unet {i} are not compatible with unet {i - 1}.\"\n",
        "                    true_image_sizes.append(true_upsampling_sizes[i][1])\n",
        "            true_decoder_config_obj['image_sizes'] = true_image_sizes\n",
        "            # All unets must have been trained with the same number of sampling timesteps in order to be compatible\n",
        "            assert all(true_train_timesteps[0] == t for t in true_train_timesteps), f\"All unets must have been trained with the same number of sampling timesteps in order to be compatible.\"\n",
        "            true_decoder_config_obj['timesteps'] = true_train_timesteps[0]\n",
        "            true_decoder_config_obj['beta_schedule'] = true_beta_schedules\n",
        "            true_decoder_config_obj['learned_variance'] = true_uses_learned_variance\n",
        "            # If any of the sample_timesteps are not None, then we need to insert them into the true decoder config\n",
        "            if any(true_sample_timesteps):\n",
        "                true_decoder_config_obj['sample_timesteps'] = true_sample_timesteps\n",
        "\n",
        "            # Now we can create the decoder and substitute the unets\n",
        "            true_decoder_config = DecoderConfig(**true_decoder_config_obj)\n",
        "            decoder_data_requirements = self._get_decoder_data_requirements(true_decoder_config)\n",
        "            print('trudec shit???')\n",
        "            with init_empty_weights():\n",
        "              decoder = true_decoder_config.create()\n",
        "            decoder.unets = nn.ModuleList(true_unets)\n",
        "            decoder.requires_grad_(False).eval()\n",
        "            decoder=decoder.to(torch.float16).to(self.devices[0])\n",
        "            return ModelInfo(decoder, decoder_version, requires_clip, decoder_data_requirements)\n",
        "            \n",
        "    def _get_prior_data_requirements(self, config: DiffusionPriorConfig) -> DataRequirements:\n",
        "        \"\"\"\n",
        "        Returns the data requirements for a diffusion prior\n",
        "        \"\"\"\n",
        "        return DataRequirements(\n",
        "            image_embedding=False,  # This is kinda the whole point\n",
        "            text_encoding=True,  # This is also kinda the whole point\n",
        "            image=False,  # The prior is never conditioned on the image\n",
        "            text=False,  # Text is never required for anything\n",
        "            can_generate_embedding=False,  # This might be added later if clip is being used\n",
        "            image_size=[-1, -1]  # This is not used\n",
        "        )\n",
        "\n",
        "    def load_prior(self, load_config: PriorLoadConfig) -> 'ModelInfo[DiffusionPrior]':\n",
        "        \"\"\"\n",
        "        Loads a prior from a model and a config file\n",
        "        \"\"\"\n",
        "        sample_timesteps = load_config.default_sample_timesteps\n",
        "        def apply_default_config(config: DiffusionPriorConfig) -> DiffusionPriorConfig:\n",
        "            \"\"\"\n",
        "            Applies the default config to the given config\n",
        "            \"\"\"\n",
        "            if sample_timesteps is not None:\n",
        "                config.sample_timesteps = sample_timesteps\n",
        "\n",
        "        with load_config.load_model_from.as_local_file(check_update=self.check_updates) as model_file:\n",
        "            model_state_dict = torch.load(model_file, map_location=self.load_device)\n",
        "            if 'version' in model_state_dict:\n",
        "                model_version = model_state_dict['version']\n",
        "                if model_version != self.current_version:\n",
        "                    print(f'WARNING: This prior was trained on version {model_version} but the current version is {self.current_version}. This may result in the model failing to load.')\n",
        "                    print(f'FIX: Switch to this version with `pip install DALLE2-pytorch=={model_version}`. If different models suggest different versions, you may just need to choose one.')\n",
        "            else:\n",
        "                print('WARNING: This prior was trained on an old version of Dalle2. This may result in the model failing to load or it may produce garbage results.')\n",
        "                model_version = None\n",
        "\n",
        "            requires_clip = False\n",
        "            if 'config' in model_state_dict:\n",
        "                # Then we define the prior config from this object\n",
        "                prior_config = TrainDiffusionPriorConfig(**model_state_dict['config']).prior\n",
        "                apply_default_config(prior_config)\n",
        "                if prior_config.clip is not None:\n",
        "                    # We don't want to load clip with the model\n",
        "                    prior_config.clip = None\n",
        "                    requires_clip = True\n",
        "                with init_empty_weights():\n",
        "                  prior = prior_config.create()\n",
        "                load_state_dict_with_low_memory(prior, model_state_dict['model'])\n",
        "                #prior.load_state_dict(model_state_dict['model'], strict=self.strict_loading)\n",
        "                prior.requires_grad_(False).eval()\n",
        "            else:\n",
        "                # In this case, the state_dict is the model itself. This means we also must load the config from an external file\n",
        "                assert load_config.load_config_from is not None\n",
        "                with load_config.load_config_from.as_local_file(check_update=self.check_updates) as config_file:\n",
        "                    prior_config = TrainDiffusionPriorConfig.from_json_path(config_file).prior\n",
        "                    apply_default_config(prior_config)\n",
        "                    if prior_config.clip is not None:\n",
        "                        # We don't want to load clip with the model\n",
        "                        prior_config.clip = None\n",
        "                        requires_clip = True\n",
        "                with init_empty_weights():\n",
        "                  prior = prior_config.create()\n",
        "                load_state_dict_with_low_memory(prior, model_state_dict)\n",
        "                #prior.load_state_dict(model_state_dict, strict=self.strict_loading)\n",
        "                prior.requires_grad_(False).eval()\n",
        "            del model_state_dict\n",
        "            data_requirements = self._get_prior_data_requirements(prior_config)\n",
        "            prior.to(torch.float16)\n",
        "            return ModelInfo(prior, model_version, requires_clip, data_requirements)\n"
      ],
      "metadata": {
        "id": "dESBEjk7Dbom"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtlFLbke2Sob"
      },
      "source": [
        "import gradio as gr\n",
        "from pathlib import Path\n",
        "from typing import Dict, List\n",
        "from PIL import Image as PILImage\n",
        "from dalle2_laion import ModelLoadConfig, utils\n",
        "from dalle2_laion.scripts import BasicInference, ImageVariation, BasicInpainting\n",
        "\n",
        "config_path = '/content/dalle2-laion/configs/gradio.example.json'\n",
        "model_config = ModelLoadConfig.from_json_path(config_path)\n",
        "model_manager = DalleModelManager(model_config)\n",
        "\n",
        "output_path = Path('/content/dalle2-laion/output/gradio')\n",
        "output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "cond_scale_sliders = [gr.Slider(minimum=0.5, maximum=5, step=0.05, label=\"Prior Cond Scale\", value=1),]\n",
        "for i in range(model_manager.model_config.decoder.final_unet_number):\n",
        "    cond_scale_sliders.append(gr.Slider(minimum=0.5, maximum=5, step=0.05, label=f\"Decoder {i+1} Cond Scale\", value=1))\n",
        "\n",
        "def dream(text: str, samples_per_prompt: int, prior_cond_scale: float, *decoder_cond_scales: List[float]):\n",
        "    prompts = text.split('\\n')[:8]\n",
        "\n",
        "    script = BasicInference(model_manager, verbose=True)\n",
        "    output = script.run(prompts, prior_sample_count=samples_per_prompt, decoder_batch_size=40, prior_cond_scale=prior_cond_scale, decoder_cond_scale=decoder_cond_scales)\n",
        "    all_outputs = []\n",
        "    for text, embedding_outputs in output.items():\n",
        "        for index, embedding_output in embedding_outputs.items():\n",
        "            all_outputs.extend(embedding_output)\n",
        "    return all_outputs\n",
        "dream_interface = gr.Interface(\n",
        "    dream,\n",
        "    inputs=[\n",
        "        gr.Textbox(placeholder=\"A corgi wearing a top hat...\", lines=8),\n",
        "        gr.Slider(minimum=1, maximum=4, step=1, label=\"Samples per prompt\", value=1),\n",
        "        *cond_scale_sliders\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Gallery()\n",
        "    ],\n",
        "    title=\"Dalle2 Dream\",\n",
        "    description=\"Generate images from text. You can give a maximum of 8 prompts at a time. Any more will be ignored. Generation takes around 5 minutes so be patient.\",\n",
        ")\n",
        "\n",
        "def variation(image: PILImage.Image, text: str, num_generations: int, *decoder_cond_scales: List[float]):\n",
        "    print(\"Variation using text:\", text)\n",
        "    img = utils.center_crop_to_square(image)\n",
        "\n",
        "    script = ImageVariation(model_manager, verbose=True)\n",
        "    output = script.run([img], [text], sample_count=num_generations, cond_scale=decoder_cond_scales)\n",
        "    all_outputs = []\n",
        "    for index, embedding_output in output.items():\n",
        "        all_outputs.extend(embedding_output)\n",
        "    return all_outputs\n",
        "variation_interface = gr.Interface(\n",
        "    variation,\n",
        "    inputs=[\n",
        "        gr.Image(value=\"https://www.thefarmersdog.com/digest/wp-content/uploads/2021/12/corgi-top-1400x871.jpg\", source=\"upload\", interactive=True, type=\"pil\"),\n",
        "        gr.Text(),\n",
        "        gr.Slider(minimum=1, maximum=6, label=\"Number to generate\", value=2, step=1),\n",
        "        *cond_scale_sliders[1:]\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Gallery()\n",
        "    ],\n",
        "    title=\"Dalle2 Variation\",\n",
        "    description=\"Generates images similar to the input image.\\nGeneration takes around 5 minutes so be patient.\",\n",
        ")\n",
        "\n",
        "def inpaint(image: Dict[str, PILImage.Image], text: str, num_generations: int, prior_cond_scale: float, *decoder_cond_scales: List[float]):\n",
        "    print(\"Inpainting using text:\", text)\n",
        "    img, mask = image['image'], image['mask']\n",
        "    # Remove alpha from img\n",
        "    img = img.convert('RGB')\n",
        "    img = utils.center_crop_to_square(img)\n",
        "    mask = utils.center_crop_to_square(mask)\n",
        "\n",
        "    script = BasicInpainting(model_manager, verbose=True)\n",
        "    mask = ~utils.get_mask_from_image(mask)\n",
        "    output = script.run(images=[img], masks=[mask], text=[text], sample_count=num_generations, prior_cond_scale=prior_cond_scale, decoder_cond_scale=decoder_cond_scales)\n",
        "    all_outputs = []\n",
        "    for index, embedding_output in output.items():\n",
        "        all_outputs.extend(embedding_output)\n",
        "    return all_outputs\n",
        "inpaint_interface = gr.Interface(\n",
        "    inpaint,\n",
        "    inputs=[\n",
        "        gr.Image(value=\"https://www.thefarmersdog.com/digest/wp-content/uploads/2021/12/corgi-top-1400x871.jpg\", source=\"upload\", tool=\"sketch\", interactive=True, type=\"pil\"),\n",
        "        gr.Text(),\n",
        "        gr.Slider(minimum=1, maximum=6, label=\"Number to generate\", value=2, step=1),\n",
        "        *cond_scale_sliders\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Gallery()\n",
        "    ],\n",
        "    title=\"Dalle2 Inpainting\",\n",
        "    description=\"Fills in the details of areas you mask out.\\nGeneration takes around 5 minutes so be patient.\",\n",
        ")\n",
        "\n",
        "demo = gr.TabbedInterface(interface_list=[dream_interface, variation_interface, inpaint_interface], tab_names=[\"Dream\", \"Variation\", \"Inpaint\"])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e200MmBU2aLT"
      },
      "source": [
        "demo.launch(share=True, enable_queue=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "pIaK4ywaLwLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zk=model_manager.decoder_info.model.state_dict()\n",
        "for ky in zk:\n",
        "  print(zk[ky].device)\n"
      ],
      "metadata": {
        "id": "-gJvvSnPMGsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zk=model_manager.prior_info.model.state_dict()\n",
        "for ky in zk:\n",
        "  print(zk[ky].device)\n"
      ],
      "metadata": {
        "id": "DTiKwQbNNRLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_manager.devices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seC-ofqRNwPc",
        "outputId": "f8d2750f-96a8-4f35-d7c6-52c10fa34833"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[device(type='cuda', index=0)]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}