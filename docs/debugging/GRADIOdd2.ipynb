{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GRADIOdd2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f1e05ac732d34ef7ad949aa783367897": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_477591be745c4376a53f54bd3c9f82cd",
              "IPY_MODEL_7811411902ef4bc3ada96c5c3c6bc60b",
              "IPY_MODEL_50945d5531704f718b914289abbdc632"
            ],
            "layout": "IPY_MODEL_63e03e329ac3447eb136760d9b7313d8"
          }
        },
        "477591be745c4376a53f54bd3c9f82cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_645087bed6d84fd28bd803d10643b447",
            "placeholder": "​",
            "style": "IPY_MODEL_efb3b8ef14b04a448b1dce6fd4491d05",
            "value": "sampling loop time step: 100%"
          }
        },
        "7811411902ef4bc3ada96c5c3c6bc60b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b69724556e3641d19adeceb84feee085",
            "max": 64,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_287baa8846d84e7ca256413bbc656741",
            "value": 64
          }
        },
        "50945d5531704f718b914289abbdc632": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f29fc5b3c3f945d6959fa35218e37f8f",
            "placeholder": "​",
            "style": "IPY_MODEL_02606148231740b5818eb1c4f36e28e3",
            "value": " 64/64 [00:01&lt;00:00, 36.33it/s]"
          }
        },
        "63e03e329ac3447eb136760d9b7313d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "645087bed6d84fd28bd803d10643b447": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efb3b8ef14b04a448b1dce6fd4491d05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b69724556e3641d19adeceb84feee085": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "287baa8846d84e7ca256413bbc656741": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f29fc5b3c3f945d6959fa35218e37f8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02606148231740b5818eb1c4f36e28e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJUJLWQ92g6R"
      },
      "source": [
        "!pip install DALLE2-pytorch==1.1.0 einops einops-exts kornia ftfy vector-quantize-pytorch resize-right clip-anytorch rotary-embedding-torch coca-pytorch pytorch-warmup ema-pytorch x-clip accelerate gradio\n",
        "!git clone https://github.com/LAION-AI/dalle2-laion.git\n",
        "!mv 'dalle2-laion/dalle2_laion' dalle2_laion\n",
        "#!git clone https://github.com/lucidrains/DALLE2-pytorch.git\n",
        "#!mv 'DALLE2-pytorch/dalle2_pytorch' dalle2_pytorch\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Any, Tuple, Optional, TypeVar, Generic, List,Iterator,  Dict\n",
        "from dalle2_laion.config import DecoderLoadConfig, SingleDecoderLoadConfig, PriorLoadConfig, ModelLoadConfig\n",
        "from dalle2_pytorch import __version__ as Dalle2Version, Decoder, DiffusionPrior, Unet\n",
        "from dalle2_pytorch.train_configs import TrainDecoderConfig, TrainDiffusionPriorConfig, DecoderConfig, UnetConfig, DiffusionPriorConfig\n",
        "import torch\n",
        "from torch import LongTensor, FloatTensor, BoolTensor,nn\n",
        "from packaging import version\n",
        "\n",
        "from accelerate import init_empty_weights\n",
        "\n",
        "\n",
        "def get_keys_to_submodule(model: nn.Module) -> Dict[str, nn.Module]:\n",
        "    keys_to_submodule = {}\n",
        "    # iterate all submodules\n",
        "    for submodule_name, submodule in model.named_modules():\n",
        "        # iterate all paramters in each submobule\n",
        "        for param_name, param in submodule.named_parameters():\n",
        "            # param_name is organized as <name>.<subname>.<subsubname> ...\n",
        "            splitted_param_name = param_name.split('.')\n",
        "            # we cannot go inside it anymore. This is the actual parameter\n",
        "            is_leaf_param = len(splitted_param_name) == 1\n",
        "            if is_leaf_param:\n",
        "                # we recreate the correct key\n",
        "                key = f\"{submodule_name}.{param_name}\"\n",
        "                # we associate this key with this submodule\n",
        "                keys_to_submodule[key] = submodule\n",
        "                \n",
        "    return keys_to_submodule\n",
        "\n",
        "def load_state_dict_with_low_memory(model: nn.Module, state_dict):\n",
        "    print('======hacky load======')\n",
        "    # free up memory by placing the model in the `meta` device\n",
        "    keys_to_submodule = get_keys_to_submodule(model)\n",
        "    mste=model.state_dict()\n",
        "    for key, submodule in keys_to_submodule.items():\n",
        "        # get the valye from the state_dict\n",
        "        if key in state_dict:\n",
        "          val = state_dict[key]\n",
        "        else:\n",
        "          print(key)\n",
        "          val = torch.ones(mste[key].shape, dtype= torch.float32)\n",
        "        # we need to substitute the parameter inside submodule, \n",
        "        # remember key is composed of <name>.<subname>.<subsubname>\n",
        "        # the actual submodule's parameter is stored inside the \n",
        "        # last subname. If key is `in_proj.weight`, the correct field if `weight`\n",
        "        param_name = key.split('.')[-1]\n",
        "        #param_dtype = getattr(submodule, param_name).dtype\n",
        "        #val = val.to(param_dtype)\n",
        "        # create a new parameter\n",
        "        new_val = torch.nn.Parameter(val)\n",
        "        setattr(submodule, param_name, new_val)\n",
        "\n",
        "\n",
        "\n",
        "def exists(obj: Any) -> bool:\n",
        "    return obj is not None\n",
        "\n",
        "@dataclass\n",
        "class DataRequirements:\n",
        "    image_embedding: bool\n",
        "    text_encoding: bool\n",
        "    image: bool\n",
        "    text: bool\n",
        "    can_generate_embedding: bool\n",
        "    image_size: int\n",
        "\n",
        "    def has_clip(self):\n",
        "        self.can_generate_embedding = True\n",
        "\n",
        "    def is_valid(\n",
        "        self,\n",
        "        has_image_emb: bool = False, has_text_encoding: bool = False,\n",
        "        has_image: bool = False, has_text: bool = False,\n",
        "        image_size: Optional[int] = None\n",
        "    ):\n",
        "        # The image size must be equal to or greater than the required size\n",
        "        # Verify that the text input is valid\n",
        "        errors = []\n",
        "        is_valid = True\n",
        "        if self.text_encoding:\n",
        "            # Then we need to some way to get the text encoding\n",
        "            if not (has_text_encoding or (self.can_generate_embedding and has_text)):\n",
        "                errors.append('Text encoding is required, but no text encoding or text was provided')\n",
        "                is_valid = False\n",
        "        if self.text:\n",
        "            # Then this requires text be passed in explicitly\n",
        "            if not has_text:\n",
        "                errors.append('Text is required, but no text was provided')\n",
        "                is_valid = False\n",
        "\n",
        "        # Verify that the image input is valid\n",
        "        image_size_greater = exists(image_size) and image_size >= self.image_size\n",
        "        if self.image_embedding:\n",
        "            # Then we need to some way to get the image embedding\n",
        "            # In this case, we also need to make sure that the image size is big enough to generate the embedding\n",
        "            if not (has_image_emb or (self.can_generate_embedding and has_image and image_size_greater)):\n",
        "                errors.append('Image embedding is required, but no image embedding or image was provided or the image was too small')\n",
        "                is_valid = False\n",
        "        if self.image:\n",
        "            # Then this requires an image be passed in explicitly\n",
        "            # In this case we also need to make sure the image is big enough to be used\n",
        "            if not (has_image and image_size_greater):\n",
        "                errors.append('Image is required, but no image was provided or the image was too small')\n",
        "                is_valid = False\n",
        "        return is_valid, errors\n",
        "\n",
        "    def __add__(self, other: 'DataRequirements') -> 'DataRequirements':\n",
        "        return DataRequirements(\n",
        "            image_embedding=self.image_embedding or other.image_embedding,  # If either needs an image embedding, the combination needs one\n",
        "            text_embedding=self.text_embedding or other.text_embedding,  # If either needs a text embedding, the combination needs one\n",
        "            image=self.image or other.image,  # If either needs an image, the combination needs it  \n",
        "            text=self.text or other.text,  # If either needs a text, the combination needs it\n",
        "            can_generate_embedding=self.can_generate_embedding and other.can_generate_embedding,  # If either cannot generate an embedding, we know that trying to replace an embedding with raw data will not work\n",
        "            image_size=max(self.image_size, other.image_size)  # We can downsample without loss of information, so we use the larger image size\n",
        "        )\n",
        "\n",
        "ModelType = TypeVar('ModelType', Decoder, DiffusionPrior)\n",
        "\n",
        "@dataclass\n",
        "class ModelInfo(Generic[ModelType]):\n",
        "    model: ModelType\n",
        "    model_version: Optional[version.Version]\n",
        "    requires_clip: bool\n",
        "    data_requirements: DataRequirements\n",
        "\n",
        "class DalleModelManager:\n",
        "    \"\"\"\n",
        "    Used to load priors and decoders and to provide a simple interface to run general scripts against\n",
        "    \"\"\"\n",
        "    def __init__(self, model_load_config: ModelLoadConfig, check_updates: bool = True):\n",
        "        \"\"\"\n",
        "        Downloads the models and loads them into memory.\n",
        "        If check_updates is True, then the models will be re-downloaded if checksums do not match.\n",
        "        \"\"\"\n",
        "        self.check_updates = check_updates\n",
        "        self.model_config = model_load_config\n",
        "        self.current_version = version.parse(Dalle2Version)\n",
        "        self.single_device = isinstance(model_load_config.devices, str)\n",
        "        self.devices = [torch.device(model_load_config.devices)] if self.single_device else [torch.device(d) for d in model_load_config.devices]\n",
        "        self.load_device = torch.device('cpu') if model_load_config.load_on_cpu else self.devices[0]\n",
        "        self.strict_loading = model_load_config.strict_loading\n",
        "\n",
        "        if model_load_config.decoder is not None:\n",
        "            self.decoder_info = self.load_decoder(model_load_config.decoder)\n",
        "        else:\n",
        "            self.decoder_info = None\n",
        "\n",
        "        if model_load_config.prior is not None:\n",
        "            self.prior_info = self.load_prior(model_load_config.prior)\n",
        "        else:\n",
        "            self.prior_info = None\n",
        "\n",
        "        if (exists(self.decoder_info) and self.decoder_info.requires_clip) or (exists(self.prior_info) and self.prior_info.requires_clip):\n",
        "            assert model_load_config.clip is not None, 'Your model requires clip to be loaded. Please provide a clip config.'\n",
        "            self.clip = model_load_config.clip.create()\n",
        "            # Update the data requirements to include the clip model\n",
        "            if exists(self.decoder_info):\n",
        "                self.decoder_info.data_requirements.has_clip()\n",
        "            if exists(self.prior_info):\n",
        "                self.prior_info.data_requirements.has_clip()\n",
        "        else:\n",
        "            if model_load_config.clip is not None:\n",
        "                print(f'WARNING: Your model does not require clip, but you provided a clip config. This will be ignored.')\n",
        "\n",
        "    def _get_decoder_data_requirements(self, decoder_config: DecoderConfig, min_unet_number: int = 1) -> DataRequirements:\n",
        "        \"\"\"\n",
        "        Returns the data requirements for a decoder\n",
        "        \"\"\"\n",
        "        return DataRequirements(\n",
        "            image_embedding=True,\n",
        "            text_encoding=any(unet_config.cond_on_text_encodings for unet_config in decoder_config.unets[min_unet_number - 1:]),\n",
        "            image=min_unet_number > 1,  # If this is an upsampler we need an image\n",
        "            text=False,  # Text is never required for anything\n",
        "            can_generate_embedding=False,  # This might be added later if clip is being used\n",
        "            image_size=decoder_config.image_sizes[min_unet_number - 1]  # The input image size is the input to the first unet we are using\n",
        "        )\n",
        "\n",
        "    def _load_single_decoder(self, load_config: SingleDecoderLoadConfig) -> Tuple[Decoder, DecoderConfig, Optional[version.Version], bool]:\n",
        "        \"\"\"\n",
        "        Loads a single decoder from a model and a config file\n",
        "        \"\"\"\n",
        "        unet_sample_timesteps = load_config.default_sample_timesteps\n",
        "        def apply_default_config(config: DecoderConfig):\n",
        "            if unet_sample_timesteps is not None:\n",
        "                base_sample_timesteps = [None] * len(config.unets)\n",
        "                for unet_number, timesteps in zip(load_config.unet_numbers, unet_sample_timesteps):\n",
        "                    base_sample_timesteps[unet_number - 1] = timesteps\n",
        "                config.sample_timesteps = base_sample_timesteps\n",
        "        \n",
        "        with load_config.load_model_from.as_local_file(check_update=self.check_updates) as model_file:\n",
        "            model_state_dict = torch.load(model_file, map_location=self.load_device)\n",
        "            if 'version' in model_state_dict:\n",
        "                model_version = model_state_dict['version']\n",
        "                if model_version != self.current_version:\n",
        "                    print(f'WARNING: This decoder was trained on version {model_version} but the current version is {self.current_version}. This may result in the model failing to load.')\n",
        "                    print(f'FIX: Switch to this version with `pip install DALLE2-pytorch=={model_version}`. If different models suggest different versions, you may just need to choose one.')\n",
        "            else:\n",
        "                print(f'WARNING: This decoder was trained on an old version of Dalle2. This may result in the model failing to load or it may lead to producing garbage results.')\n",
        "                model_version = None  # No version info in the model\n",
        "            \n",
        "            requires_clip = False\n",
        "            if 'config' in model_state_dict:\n",
        "                # Then we define the decoder config from this object\n",
        "                decoder_config = TrainDecoderConfig(**model_state_dict['config']).decoder\n",
        "                apply_default_config(decoder_config)\n",
        "                if decoder_config.clip is not None:\n",
        "                    # We don't want to load clip with the model\n",
        "                    requires_clip = True\n",
        "                    decoder_config.clip = None\n",
        "                with init_empty_weights():\n",
        "                  decoder = decoder_config.create()\n",
        "                load_state_dict_with_low_memory(decoder, model_state_dict['model'])\n",
        "                #decoder.load_state_dict(model_state_dict['model'], strict=self.strict_loading)  # If the model has a config included, then we know the model_state_dict['model'] is the actual model\n",
        "            else:\n",
        "                # In this case, the state_dict is the model itself. This means we also must load the config from an external file\n",
        "                assert load_config.load_config_from is not None\n",
        "                with load_config.load_config_from.as_local_file(check_update=self.check_updates) as config_file:\n",
        "                    decoder_config = TrainDecoderConfig.from_json_path(config_file).decoder\n",
        "                    apply_default_config(decoder_config)\n",
        "                    if decoder_config.clip is not None:\n",
        "                        # We don't want to load clip with the model\n",
        "                        requires_clip = True\n",
        "                        decoder_config.clip = None\n",
        "                with init_empty_weights():\n",
        "                  decoder = decoder_config.create()\n",
        "                load_state_dict_with_low_memory(decoder, model_state_dict)\n",
        "                #decoder.load_state_dict(model_state_dict, strict=self.strict_loading)\n",
        "            del model_state_dict\n",
        "            return decoder.requires_grad_(False).eval(), decoder_config, model_version, requires_clip\n",
        "\n",
        "    def load_decoder(self, load_config: DecoderLoadConfig) -> 'ModelInfo[Decoder]':\n",
        "        \"\"\"\n",
        "        Loads a decoder from a model and a config file\n",
        "        \"\"\"\n",
        "        if len(load_config.unet_sources) == 1:\n",
        "            # Then we are loading only one model\n",
        "            decoder, decoder_config, decoder_version, requires_clip = self._load_single_decoder(load_config.unet_sources[0])\n",
        "            decoder_data_requirements = self._get_decoder_data_requirements(decoder_config)\n",
        "            return ModelInfo(decoder.requires_grad_(False).to(torch.float32), decoder_version, requires_clip, decoder_data_requirements)\n",
        "        else:\n",
        "            true_unets: List[Unet] = [None] * load_config.final_unet_number  # Stores the unets that will replace the ones in the true decoder\n",
        "            true_unet_configs: List[UnetConfig] = [None] * load_config.final_unet_number  # Stores the unet configs that will replace the ones in the true decoder config\n",
        "            true_upsampling_sizes: List[Tuple[int, int]] = [None] * load_config.final_unet_number  # Stores the progression of upsampling sizes for each unet so that we can validate these unets actually work together\n",
        "            true_train_timesteps: List[int] = [None] * load_config.final_unet_number  # Stores the number of timesteps that each unet trained with\n",
        "            true_beta_schedules: List[str] = [None] * load_config.final_unet_number  # Stores the beta scheduler that each unet used\n",
        "            true_uses_learned_variance: List[bool] = [None] * load_config.final_unet_number  # Stores whether each unet uses learned variance\n",
        "            true_sample_timesteps: List[int] = [None] * load_config.final_unet_number  # Stores the number of timesteps that each unet used to sample\n",
        "\n",
        "            requires_clip = False\n",
        "            for source in load_config.unet_sources:\n",
        "                decoder, decoder_config, decoder_version, unets_requires_clip = self._load_single_decoder(source)\n",
        "                if unets_requires_clip:\n",
        "                    requires_clip = True\n",
        "                if source.default_sample_timesteps is not None:\n",
        "                    assert len(source.default_sample_timesteps) == len(source.unet_numbers)\n",
        "                for i, unet_number in enumerate(source.unet_numbers):\n",
        "                    unet_index = unet_number - 1\n",
        "                    # Now we need to insert the unet into the true unets and the unet config into the true config\n",
        "                    if source.default_sample_timesteps is not None:\n",
        "                        true_sample_timesteps[unet_index] = source.default_sample_timesteps[i]\n",
        "                    true_unets[unet_index] = decoder.unets[unet_index]\n",
        "                    true_unet_configs[unet_index] = decoder_config.unets[unet_index]\n",
        "                    true_upsampling_sizes[unet_index] = None if unet_index == 0 else decoder_config.image_sizes[unet_index - 1], decoder_config.image_sizes[unet_index]\n",
        "                    true_train_timesteps[unet_index] = decoder_config.timesteps\n",
        "                    true_beta_schedules[unet_index] = decoder_config.beta_schedule[unet_index]\n",
        "                    true_uses_learned_variance[unet_index] = decoder_config.learned_variance if isinstance(decoder_config.learned_variance, bool) else decoder_config.learned_variance[unet_index]\n",
        "\n",
        "            true_decoder_config_obj = {}\n",
        "            # Insert the true configs into the true decoder config\n",
        "            true_decoder_config_obj['unets'] = true_unet_configs\n",
        "            true_image_sizes = []\n",
        "            for i in range(load_config.final_unet_number):\n",
        "                if i == 0:\n",
        "                    true_image_sizes.append(true_upsampling_sizes[i][1])\n",
        "                else:\n",
        "                    assert true_upsampling_sizes[i - 1][1] == true_upsampling_sizes[i][0], f\"The upsampling sizes for unet {i} are not compatible with unet {i - 1}.\"\n",
        "                    true_image_sizes.append(true_upsampling_sizes[i][1])\n",
        "            true_decoder_config_obj['image_sizes'] = true_image_sizes\n",
        "            # All unets must have been trained with the same number of sampling timesteps in order to be compatible\n",
        "            assert all(true_train_timesteps[0] == t for t in true_train_timesteps), f\"All unets must have been trained with the same number of sampling timesteps in order to be compatible.\"\n",
        "            true_decoder_config_obj['timesteps'] = true_train_timesteps[0]\n",
        "            true_decoder_config_obj['beta_schedule'] = true_beta_schedules\n",
        "            true_decoder_config_obj['learned_variance'] = true_uses_learned_variance\n",
        "            # If any of the sample_timesteps are not None, then we need to insert them into the true decoder config\n",
        "            if any(true_sample_timesteps):\n",
        "                true_decoder_config_obj['sample_timesteps'] = true_sample_timesteps\n",
        "\n",
        "            # Now we can create the decoder and substitute the unets\n",
        "            true_decoder_config = DecoderConfig(**true_decoder_config_obj)\n",
        "            decoder_data_requirements = self._get_decoder_data_requirements(true_decoder_config)\n",
        "            print('trudec shit???')\n",
        "            with init_empty_weights():\n",
        "              decoder = true_decoder_config.create()\n",
        "            decoder.unets = nn.ModuleList(true_unets)\n",
        "            return ModelInfo(decoder.requires_grad_(False).to(torch.float32).to(self.devices[0]).eval(), decoder_version, requires_clip, decoder_data_requirements)\n",
        "            \n",
        "    def _get_prior_data_requirements(self, config: DiffusionPriorConfig) -> DataRequirements:\n",
        "        \"\"\"\n",
        "        Returns the data requirements for a diffusion prior\n",
        "        \"\"\"\n",
        "        return DataRequirements(\n",
        "            image_embedding=False,  # This is kinda the whole point\n",
        "            text_encoding=True,  # This is also kinda the whole point\n",
        "            image=False,  # The prior is never conditioned on the image\n",
        "            text=False,  # Text is never required for anything\n",
        "            can_generate_embedding=False,  # This might be added later if clip is being used\n",
        "            image_size=[-1, -1]  # This is not used\n",
        "        )\n",
        "\n",
        "    def load_prior(self, load_config: PriorLoadConfig) -> 'ModelInfo[DiffusionPrior]':\n",
        "        \"\"\"\n",
        "        Loads a prior from a model and a config file\n",
        "        \"\"\"\n",
        "        sample_timesteps = load_config.default_sample_timesteps\n",
        "        def apply_default_config(config: DiffusionPriorConfig) -> DiffusionPriorConfig:\n",
        "            \"\"\"\n",
        "            Applies the default config to the given config\n",
        "            \"\"\"\n",
        "            if sample_timesteps is not None:\n",
        "                config.sample_timesteps = sample_timesteps\n",
        "\n",
        "        with load_config.load_model_from.as_local_file(check_update=self.check_updates) as model_file:\n",
        "            model_state_dict = torch.load(model_file, map_location=self.load_device)\n",
        "            if 'version' in model_state_dict:\n",
        "                model_version = model_state_dict['version']\n",
        "                if model_version != self.current_version:\n",
        "                    print(f'WARNING: This prior was trained on version {model_version} but the current version is {self.current_version}. This may result in the model failing to load.')\n",
        "                    print(f'FIX: Switch to this version with `pip install DALLE2-pytorch=={model_version}`. If different models suggest different versions, you may just need to choose one.')\n",
        "            else:\n",
        "                print('WARNING: This prior was trained on an old version of Dalle2. This may result in the model failing to load or it may produce garbage results.')\n",
        "                model_version = None\n",
        "\n",
        "            requires_clip = False\n",
        "            if 'config' in model_state_dict:\n",
        "                # Then we define the prior config from this object\n",
        "                prior_config = TrainDiffusionPriorConfig(**model_state_dict['config']).prior\n",
        "                apply_default_config(prior_config)\n",
        "                if prior_config.clip is not None:\n",
        "                    # We don't want to load clip with the model\n",
        "                    prior_config.clip = None\n",
        "                    requires_clip = True\n",
        "                with init_empty_weights():\n",
        "                  prior = prior_config.create()\n",
        "                load_state_dict_with_low_memory(prior, model_state_dict['model'])\n",
        "                #prior.load_state_dict(model_state_dict['model'], strict=self.strict_loading)\n",
        "            else:\n",
        "                # In this case, the state_dict is the model itself. This means we also must load the config from an external file\n",
        "                assert load_config.load_config_from is not None\n",
        "                with load_config.load_config_from.as_local_file(check_update=self.check_updates) as config_file:\n",
        "                    prior_config = TrainDiffusionPriorConfig.from_json_path(config_file).prior\n",
        "                    apply_default_config(prior_config)\n",
        "                    if prior_config.clip is not None:\n",
        "                        # We don't want to load clip with the model\n",
        "                        prior_config.clip = None\n",
        "                        requires_clip = True\n",
        "                with init_empty_weights():\n",
        "                  prior = prior_config.create()\n",
        "                load_state_dict_with_low_memory(prior, model_state_dict)\n",
        "                #prior.load_state_dict(model_state_dict, strict=self.strict_loading)\n",
        "            del model_state_dict\n",
        "            data_requirements = self._get_prior_data_requirements(prior_config)\n",
        "            return ModelInfo(prior.requires_grad_(False).to(torch.float32).to(self.devices[0]).eval(), model_version, requires_clip, data_requirements)\n"
      ],
      "metadata": {
        "id": "dESBEjk7Dbom"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtlFLbke2Sob"
      },
      "source": [
        "import gradio as gr\n",
        "from pathlib import Path\n",
        "from typing import Dict, List\n",
        "from PIL import Image as PILImage\n",
        "from dalle2_laion import ModelLoadConfig, utils\n",
        "from dalle2_laion.scripts import BasicInference, ImageVariation, BasicInpainting\n",
        "\n",
        "config_path = '/content/dalle2-laion/configs/gradio.example.json'\n",
        "model_config = ModelLoadConfig.from_json_path(config_path)\n",
        "model_manager = DalleModelManager(model_config)\n",
        "\n",
        "output_path = Path('/content/dalle2-laion/output/gradio')\n",
        "output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "cond_scale_sliders = [gr.Slider(minimum=0.5, maximum=5, step=0.05, label=\"Prior Cond Scale\", value=1),]\n",
        "for i in range(model_manager.model_config.decoder.final_unet_number):\n",
        "    cond_scale_sliders.append(gr.Slider(minimum=0.5, maximum=5, step=0.05, label=f\"Decoder {i+1} Cond Scale\", value=1))\n",
        "\n",
        "def dream(text: str, samples_per_prompt: int, prior_cond_scale: float, *decoder_cond_scales: List[float]):\n",
        "    prompts = text.split('\\n')[:8]\n",
        "\n",
        "    script = BasicInference(model_manager, verbose=True)\n",
        "    output = script.run(prompts, prior_sample_count=samples_per_prompt, decoder_batch_size=40, prior_cond_scale=prior_cond_scale, decoder_cond_scale=decoder_cond_scales)\n",
        "    all_outputs = []\n",
        "    for text, embedding_outputs in output.items():\n",
        "        for index, embedding_output in embedding_outputs.items():\n",
        "            all_outputs.extend(embedding_output)\n",
        "    return all_outputs\n",
        "dream_interface = gr.Interface(\n",
        "    dream,\n",
        "    inputs=[\n",
        "        gr.Textbox(placeholder=\"A corgi wearing a top hat...\", lines=8),\n",
        "        gr.Slider(minimum=1, maximum=4, step=1, label=\"Samples per prompt\", value=1),\n",
        "        *cond_scale_sliders\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Gallery()\n",
        "    ],\n",
        "    title=\"Dalle2 Dream\",\n",
        "    description=\"Generate images from text. You can give a maximum of 8 prompts at a time. Any more will be ignored. Generation takes around 5 minutes so be patient.\",\n",
        ")\n",
        "\n",
        "def variation(image: PILImage.Image, text: str, num_generations: int, *decoder_cond_scales: List[float]):\n",
        "    print(\"Variation using text:\", text)\n",
        "    img = utils.center_crop_to_square(image)\n",
        "\n",
        "    script = ImageVariation(model_manager, verbose=True)\n",
        "    output = script.run([img], [text], sample_count=num_generations, cond_scale=decoder_cond_scales)\n",
        "    all_outputs = []\n",
        "    for index, embedding_output in output.items():\n",
        "        all_outputs.extend(embedding_output)\n",
        "    return all_outputs\n",
        "variation_interface = gr.Interface(\n",
        "    variation,\n",
        "    inputs=[\n",
        "        gr.Image(value=\"https://www.thefarmersdog.com/digest/wp-content/uploads/2021/12/corgi-top-1400x871.jpg\", source=\"upload\", interactive=True, type=\"pil\"),\n",
        "        gr.Text(),\n",
        "        gr.Slider(minimum=1, maximum=6, label=\"Number to generate\", value=2, step=1),\n",
        "        *cond_scale_sliders[1:]\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Gallery()\n",
        "    ],\n",
        "    title=\"Dalle2 Variation\",\n",
        "    description=\"Generates images similar to the input image.\\nGeneration takes around 5 minutes so be patient.\",\n",
        ")\n",
        "\n",
        "def inpaint(image: Dict[str, PILImage.Image], text: str, num_generations: int, prior_cond_scale: float, *decoder_cond_scales: List[float]):\n",
        "    print(\"Inpainting using text:\", text)\n",
        "    img, mask = image['image'], image['mask']\n",
        "    # Remove alpha from img\n",
        "    img = img.convert('RGB')\n",
        "    img = utils.center_crop_to_square(img)\n",
        "    mask = utils.center_crop_to_square(mask)\n",
        "\n",
        "    script = BasicInpainting(model_manager, verbose=True)\n",
        "    mask = ~utils.get_mask_from_image(mask)\n",
        "    output = script.run(images=[img], masks=[mask], text=[text], sample_count=num_generations, prior_cond_scale=prior_cond_scale, decoder_cond_scale=decoder_cond_scales)\n",
        "    all_outputs = []\n",
        "    for index, embedding_output in output.items():\n",
        "        all_outputs.extend(embedding_output)\n",
        "    return all_outputs\n",
        "inpaint_interface = gr.Interface(\n",
        "    inpaint,\n",
        "    inputs=[\n",
        "        gr.Image(value=\"https://www.thefarmersdog.com/digest/wp-content/uploads/2021/12/corgi-top-1400x871.jpg\", source=\"upload\", tool=\"sketch\", interactive=True, type=\"pil\"),\n",
        "        gr.Text(),\n",
        "        gr.Slider(minimum=1, maximum=6, label=\"Number to generate\", value=2, step=1),\n",
        "        *cond_scale_sliders\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Gallery()\n",
        "    ],\n",
        "    title=\"Dalle2 Inpainting\",\n",
        "    description=\"Fills in the details of areas you mask out.\\nGeneration takes around 5 minutes so be patient.\",\n",
        ")\n",
        "\n",
        "demo = gr.TabbedInterface(interface_list=[dream_interface, variation_interface, inpaint_interface], tab_names=[\"Dream\", \"Variation\", \"Inpaint\"])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e200MmBU2aLT"
      },
      "source": [
        "demo.launch(share=True, enable_queue=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dream(text='a blue cat on fire', samples_per_prompt=1, prior_cond_scale=1)"
      ],
      "metadata": {
        "id": "bSc4DJzhZ7HH",
        "outputId": "aa718f46-9030-4eb8-f882-d564f089a38c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f1e05ac732d34ef7ad949aa783367897",
            "477591be745c4376a53f54bd3c9f82cd",
            "7811411902ef4bc3ada96c5c3c6bc60b",
            "50945d5531704f718b914289abbdc632",
            "63e03e329ac3447eb136760d9b7313d8",
            "645087bed6d84fd28bd803d10643b447",
            "efb3b8ef14b04a448b1dce6fd4491d05",
            "b69724556e3641d19adeceb84feee085",
            "287baa8846d84e7ca256413bbc656741",
            "f29fc5b3c3f945d6959fa35218e37f8f",
            "02606148231740b5818eb1c4f36e28e3"
          ]
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating images for texts: ['a blue cat on fire']\n",
            "Generating prior embeddings...\n",
            "Sampling prior with cond_scale: 1\n",
            "Prior batched inputs into 1 batches. Total number of samples: 1.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sampling loop time step:   0%|          | 0/64 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f1e05ac732d34ef7ad949aa783367897"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished generating prior embeddings.\n",
            "Grouped 1 texts into 1 embeddings.\n",
            "Sampling from decoder...\n",
            "Sampling decoder with cond_scale: ()\n",
            "Decoder batched inputs into 1 batches. Total number of samples: 1.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-a9cd5c42c029>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'a blue cat on fire'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples_per_prompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior_cond_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-9e38ead16d54>\u001b[0m in \u001b[0;36mdream\u001b[0;34m(text, samples_per_prompt, prior_cond_scale, *decoder_cond_scales)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mscript\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBasicInference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior_sample_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msamples_per_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior_cond_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprior_cond_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_cond_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_cond_scales\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mall_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_outputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/dalle2_laion/scripts/BasicInference.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, text, prior_cond_scale, decoder_cond_scale, prior_sample_count, decoder_sample_count, prior_batch_size, decoder_batch_size, prior_num_samples_per_batch)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Grouped {len(text)} texts into {len(image_embeddings)} embeddings.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sampling from decoder...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mimage_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sample_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_embed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_cond_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_sample_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finished sampling from decoder.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# Now we will reconstruct a map from text to a map of img_embedding indices to list of images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/dalle2_laion/scripts/InferenceScript.py\u001b[0m in \u001b[0;36m_sample_decoder\u001b[0;34m(self, images, image_embed, text, text_encoding, inpaint_images, inpaint_image_masks, cond_scale, sample_count, batch_size)\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"inpaint_mask\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minpaint_image_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"image tensor shape: {args['inpaint_image'].shape}. mask shape: {args['inpaint_mask'].shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m                 \u001b[0moutput_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcond_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0moutput_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_embedding_number\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0minput_embedding_number\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput_image_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/dalle2_pytorch/dalle2_pytorch.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(model, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mwas_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwas_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/dalle2_pytorch/dalle2_pytorch.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, image, image_embed, text, text_encodings, batch_size, cond_scale, start_at_unet_number, stop_at_unet_number, distributed, inpaint_image, inpaint_mask, inpaint_resample_times)\u001b[0m\n\u001b[1;32m   2776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2777\u001b[0m         \u001b[0mnum_unets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_unets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2778\u001b[0;31m         \u001b[0mcond_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_unets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2780\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0munet_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_x_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearned_variance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlowres_cond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_timesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munet_cond_scale\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_unets\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvaes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_x_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearned_variance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise_schedulers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowres_conds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_timesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/dalle2_pytorch/dalle2_pytorch.py\u001b[0m in \u001b[0;36mcast_tuple\u001b[0;34m(val, length, validate)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -a"
      ],
      "metadata": {
        "id": "pIaK4ywaLwLk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "61bd27d7-7f5c-4ef9-fde8-770d0a085dbe"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============NVSMI LOG==============\n",
            "\n",
            "Timestamp                                 : Sat Aug  6 05:07:41 2022\n",
            "Driver Version                            : 460.32.03\n",
            "CUDA Version                              : 11.2\n",
            "\n",
            "Attached GPUs                             : 1\n",
            "GPU 00000000:00:04.0\n",
            "    Product Name                          : Tesla T4\n",
            "    Product Brand                         : Tesla\n",
            "    Display Mode                          : Enabled\n",
            "    Display Active                        : Disabled\n",
            "    Persistence Mode                      : Disabled\n",
            "    MIG Mode\n",
            "        Current                           : N/A\n",
            "        Pending                           : N/A\n",
            "    Accounting Mode                       : Disabled\n",
            "    Accounting Mode Buffer Size           : 4000\n",
            "    Driver Model\n",
            "        Current                           : N/A\n",
            "        Pending                           : N/A\n",
            "    Serial Number                         : 1561920024640\n",
            "    GPU UUID                              : GPU-e160ebdd-6388-011c-1052-c9d0ce8bf0c8\n",
            "    Minor Number                          : 0\n",
            "    VBIOS Version                         : 90.04.A7.00.01\n",
            "    MultiGPU Board                        : No\n",
            "    Board ID                              : 0x4\n",
            "    GPU Part Number                       : 900-2G183-6300-T00\n",
            "    Inforom Version\n",
            "        Image Version                     : G183.0200.00.02\n",
            "        OEM Object                        : 1.1\n",
            "        ECC Object                        : 5.0\n",
            "        Power Management Object           : N/A\n",
            "    GPU Operation Mode\n",
            "        Current                           : N/A\n",
            "        Pending                           : N/A\n",
            "    GPU Virtualization Mode\n",
            "        Virtualization Mode               : Pass-Through\n",
            "        Host VGPU Mode                    : N/A\n",
            "    IBMNPU\n",
            "        Relaxed Ordering Mode             : N/A\n",
            "    PCI\n",
            "        Bus                               : 0x00\n",
            "        Device                            : 0x04\n",
            "        Domain                            : 0x0000\n",
            "        Device Id                         : 0x1EB810DE\n",
            "        Bus Id                            : 00000000:00:04.0\n",
            "        Sub System Id                     : 0x12A210DE\n",
            "        GPU Link Info\n",
            "            PCIe Generation\n",
            "                Max                       : 3\n",
            "                Current                   : 3\n",
            "            Link Width\n",
            "                Max                       : 16x\n",
            "                Current                   : 16x\n",
            "        Bridge Chip\n",
            "            Type                          : N/A\n",
            "            Firmware                      : N/A\n",
            "        Replays Since Reset               : 0\n",
            "        Replay Number Rollovers           : 0\n",
            "        Tx Throughput                     : 3000 KB/s\n",
            "        Rx Throughput                     : 27000 KB/s\n",
            "    Fan Speed                             : N/A\n",
            "    Performance State                     : P0\n",
            "    Clocks Throttle Reasons\n",
            "        Idle                              : Not Active\n",
            "        Applications Clocks Setting       : Not Active\n",
            "        SW Power Cap                      : Active\n",
            "        HW Slowdown                       : Not Active\n",
            "            HW Thermal Slowdown           : Not Active\n",
            "            HW Power Brake Slowdown       : Not Active\n",
            "        Sync Boost                        : Not Active\n",
            "        SW Thermal Slowdown               : Not Active\n",
            "        Display Clock Setting             : Not Active\n",
            "    FB Memory Usage\n",
            "        Total                             : 15109 MiB\n",
            "        Used                              : 9142 MiB\n",
            "        Free                              : 5967 MiB\n",
            "    BAR1 Memory Usage\n",
            "        Total                             : 256 MiB\n",
            "        Used                              : 7 MiB\n",
            "        Free                              : 249 MiB\n",
            "    Compute Mode                          : Default\n",
            "    Utilization\n",
            "        Gpu                               : 100 %\n",
            "        Memory                            : 51 %\n",
            "        Encoder                           : 0 %\n",
            "        Decoder                           : 0 %\n",
            "    Encoder Stats\n",
            "        Active Sessions                   : 0\n",
            "        Average FPS                       : 0\n",
            "        Average Latency                   : 0\n",
            "    FBC Stats\n",
            "        Active Sessions                   : 0\n",
            "        Average FPS                       : 0\n",
            "        Average Latency                   : 0\n",
            "    Ecc Mode\n",
            "        Current                           : Enabled\n",
            "        Pending                           : Enabled\n",
            "    ECC Errors\n",
            "        Volatile\n",
            "            SRAM Correctable              : 0\n",
            "            SRAM Uncorrectable            : 0\n",
            "            DRAM Correctable              : 0\n",
            "            DRAM Uncorrectable            : 0\n",
            "        Aggregate\n",
            "            SRAM Correctable              : 0\n",
            "            SRAM Uncorrectable            : 0\n",
            "            DRAM Correctable              : 0\n",
            "            DRAM Uncorrectable            : 0\n",
            "    Retired Pages\n",
            "        Single Bit ECC                    : 0\n",
            "        Double Bit ECC                    : 0\n",
            "        Pending Page Blacklist            : No\n",
            "    Remapped Rows                         : N/A\n",
            "    Temperature\n",
            "        GPU Current Temp                  : 77 C\n",
            "        GPU Shutdown Temp                 : 96 C\n",
            "        GPU Slowdown Temp                 : 93 C\n",
            "        GPU Max Operating Temp            : 85 C\n",
            "        GPU Target Temperature            : N/A\n",
            "        Memory Current Temp               : N/A\n",
            "        Memory Max Operating Temp         : N/A\n",
            "    Power Readings\n",
            "        Power Management                  : Supported\n",
            "        Power Draw                        : 65.98 W\n",
            "        Power Limit                       : 70.00 W\n",
            "        Default Power Limit               : 70.00 W\n",
            "        Enforced Power Limit              : 70.00 W\n",
            "        Min Power Limit                   : 60.00 W\n",
            "        Max Power Limit                   : 70.00 W\n",
            "    Clocks\n",
            "        Graphics                          : 960 MHz\n",
            "        SM                                : 960 MHz\n",
            "        Memory                            : 5000 MHz\n",
            "        Video                             : 885 MHz\n",
            "    Applications Clocks\n",
            "        Graphics                          : 585 MHz\n",
            "        Memory                            : 5001 MHz\n",
            "    Default Applications Clocks\n",
            "        Graphics                          : 585 MHz\n",
            "        Memory                            : 5001 MHz\n",
            "    Max Clocks\n",
            "        Graphics                          : 1590 MHz\n",
            "        SM                                : 1590 MHz\n",
            "        Memory                            : 5001 MHz\n",
            "        Video                             : 1470 MHz\n",
            "    Max Customer Boost Clocks\n",
            "        Graphics                          : 1590 MHz\n",
            "    Clock Policy\n",
            "        Auto Boost                        : N/A\n",
            "        Auto Boost Default                : N/A\n",
            "    Processes                             : None\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#force prior to gpu?\n",
        "model_manager.prior_info.model=model_manager.prior_info.model.to(model_manager.devices[0])"
      ],
      "metadata": {
        "id": "cCyiH3kWftWP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zk=model_manager.decoder_info.model.state_dict()\n",
        "for ky in zk:\n",
        "  print(zk[ky].device)\n"
      ],
      "metadata": {
        "id": "-gJvvSnPMGsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zk=model_manager.prior_info.model.state_dict()\n",
        "for ky in zk:\n",
        "  print(zk[ky].device)"
      ],
      "metadata": {
        "id": "DTiKwQbNNRLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_manager.devices[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seC-ofqRNwPc",
        "outputId": "ba8aa8cf-326f-4b20-c0a0-c494a2cd343a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}