{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "v9EIX_OwR2_9",
        "dFjwxd76R9q2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Common<br>\n",
        "Run this sect first"
      ],
      "metadata": {
        "id": "v9EIX_OwR2_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dlpromptexample():\n",
        "  !wget https://github.com/TabuaTambalam/DalleWebms/releases/download/0.1/pexmp.7z\n",
        "  !7z x pexmp.7z\n",
        "  !wget -O web/psg1.htm https://raw.githubusercontent.com/TabuaTambalam/DalleWebms/main/docs/web/psg1.htm\n",
        "  !wget -O web/svr.py https://raw.githubusercontent.com/TabuaTambalam/DalleWebms/main/docs/web/svr.py\n",
        "  !wget -P web/ https://github.com/TabuaTambalam/tulalia/releases/download/1.1/diffusion_emb_pnnx.pt\n",
        "  !wget -P web/ https://github.com/TabuaTambalam/tulalia/releases/download/1.1/diffusion_mid_pnnx.pt\n",
        "  !wget -P web/ https://github.com/TabuaTambalam/tulalia/releases/download/1.1/diffusion_out_pnnx.pt\n",
        "  !wget https://huggingface.co/Larvik/tfmod/resolve/main/sgmk.7z\n",
        "  !wget https://raw.githubusercontent.com/TabuaTambalam/DalleWebms/main/docs/sd/BERTEmbedder.py\n",
        "  !wget https://github.com/TabuaTambalam/DalleWebms/releases/download/0.1/ldms.7z\n",
        "  !7z x ldms.7z\n",
        "  !mv ldm_xfm ldm\n",
        "  !7z e sgmk.7z\n",
        "  !rm *.7z\n",
        "  !pip install ftfy transformers omegaconf triton==2.0.0.dev20220701 einops accelerate\n",
        "  !wget https://raw.githubusercontent.com/TabuaTambalam/DalleWebms/main/docs/sd/jkt.py\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "from threading import Thread\n",
        "if not os.path.isfile('PromptFuncsExample/MultiPrompt_average.txt'):\n",
        "  t3 = Thread(target = dlpromptexample)\n",
        "  a3 = t3.start()\n",
        "!mv web/svr.py_one web/svr.py\n",
        "!rm /content/sample_data/izh.txt"
      ],
      "metadata": {
        "id": "QWr4gbYDVnNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import PIL\n",
        "import torch\n",
        "import time\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "torch.set_num_threads(os.cpu_count())\n",
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True\n",
        "\n",
        "def procckpt():\n",
        "  os.link(alter_path,'tojit.ckpt')\n",
        "  if not os.path.isfile('ck2jit.py'):\n",
        "    !pip install pytorch-lightning torch-fidelity clip-anytorch kornia\n",
        "    !wget https://raw.githubusercontent.com/TabuaTambalam/DalleWebms/main/docs/sd/ck2jit.py\n",
        "    !wget https://github.com/TabuaTambalam/tulalia/releases/download/1.1/freqs.npy\n",
        "  waitingpip()\n",
        "  !mkdir ckpts\n",
        "  !python ck2jit.py\n",
        "  !rm tojit.ckpt\n",
        "  realname=alter_path.replace('.','/').split('/')[-2]\n",
        "  !mv ckpts {realname}\n",
        "  return realname+'/'\n",
        "\n",
        "def f_dljit(ver='470k',dfsver=''):\n",
        "  convckpt=False\n",
        "  if ver == 'ckpt':\n",
        "    convckpt=True\n",
        "    ver = '840k'\n",
        "  if not os.path.isfile('imgencoder_pnnx.pt'):\n",
        "    !wget https://huggingface.co/Larvik/sd{ver}/resolve/main/alphas_cumprod.npz\n",
        "    !wget https://huggingface.co/Larvik/tfmod/resolve/main/transformer_pnnx.pt\n",
        "    !wget https://huggingface.co/Larvik/sd{ver}/resolve/main/autoencoder_pnnx.pt\n",
        "    !wget https://huggingface.co/Larvik/sd{ver}/resolve/main/imgencoder_pnnx.pt\n",
        "  if convckpt:\n",
        "    return procckpt()\n",
        "  ver+=dfsver\n",
        "  !mkdir {ver}\n",
        "  if not os.path.isfile(ver+'/diffusion_out_pnnx.pt'):\n",
        "    !nohup wget -P {ver}/ https://huggingface.co/Larvik/sd{ver}/raw/main/convd.txt &\n",
        "    !nohup wget -P {ver}/ https://huggingface.co/Larvik/sd{ver}/resolve/main/diffusion_emb_pnnx.pt &\n",
        "    !wget -P {ver}/ https://huggingface.co/Larvik/sd{ver}/resolve/main/diffusion_mid_pnnx.pt\n",
        "    !wget -P {ver}/ https://huggingface.co/Larvik/sd{ver}/resolve/main/diffusion_out_pnnx.pt\n",
        "  return ver+'/'\n",
        "\n",
        "def get_keys_to_submodule(model):\n",
        "  keys_to_submodule = {}\n",
        "  # iterate all submodules\n",
        "  for submodule_name, submodule in model.named_modules():\n",
        "      # iterate all paramters in each submobule\n",
        "      for param_name, param in submodule.named_parameters():\n",
        "          # param_name is organized as .. ...\n",
        "          splitted_param_name = param_name.split('.')\n",
        "          # we cannot go inside it anymore. This is the actual parameter\n",
        "          is_leaf_param = len(splitted_param_name) == 1\n",
        "          if is_leaf_param:\n",
        "              # we recreate the correct key\n",
        "              key = f\"{submodule_name}.{param_name}\"\n",
        "              # we associate this key with this submodule\n",
        "              keys_to_submodule[key] = submodule\n",
        "              \n",
        "  return keys_to_submodule\n",
        "\n",
        "def load_state_dict_with_low_memory(model, state_dict,modifyfunc=None,fill=True):\n",
        "  if modifyfunc is not None:\n",
        "    state_dict=modifyfunc(state_dict)\n",
        "  print('======hacky load======')\n",
        "  keys_to_submodule = get_keys_to_submodule(model)\n",
        "  mste=model.state_dict()\n",
        "  for key, submodule in keys_to_submodule.items():\n",
        "      if key[0] == '.':\n",
        "        key=key[1:]\n",
        "      if key in state_dict:\n",
        "        val = state_dict[key]\n",
        "      elif fill:\n",
        "        print(key)\n",
        "        val = torch.ones(mste[key].shape, dtype= torch.float16)\n",
        "      else:\n",
        "        continue\n",
        "\n",
        "      param_name = key.split('.')[-1]\n",
        "      new_val = torch.nn.Parameter(val,requires_grad=False)\n",
        "      setattr(submodule, param_name, new_val)\n",
        "\n",
        "\n",
        "def load_img(path):\n",
        "  if path.endswith('.npy'):\n",
        "    return torch.tensor(np.load(path))\n",
        "  elif os.path.isfile(path+'.npy'):\n",
        "    return torch.tensor(np.load(path+'.npy'))\n",
        "  \n",
        "  image = Image.open(path).convert(\"RGB\")\n",
        "  w, h = image.size\n",
        "  \n",
        "  w2, h2 = map(lambda x: x - x % 64, (w, h))\n",
        "  print(f\"resize input image to size ({w2}, {h2}) from {path}\")\n",
        "  if w!=w2 or h!=h2:\n",
        "    image = image.resize((w2, h2), resample=PIL.Image.LANCZOS)\n",
        "  image = np.array(image).astype(np.float32) / 255.0\n",
        "  image = image[None].transpose(0, 3, 1, 2)\n",
        "  image = torch.from_numpy(image)\n",
        "  rpt = 2.*image - 1.\n",
        "  H=rpt.size(2)\n",
        "  W=rpt.size(3)\n",
        "  preimg=imgenc(  rpt, torch.randn(torch.Size([1,4,H>>3,W>>3]))  )*0.18215\n",
        "  np.save(path+'.npy',preimg.numpy())\n",
        "  return preimg\n",
        "\n",
        "def make_ldm_unet():\n",
        "  from accelerate import init_empty_weights\n",
        "  from ldm.modules.diffusionmodules.openaimodel import UNetModel\n",
        "  import jkt\n",
        "  mkmodel_state_dict=jkt.mkmodel_state_dict\n",
        "  in_chn=4\n",
        "  sdt_func=None\n",
        "  with init_empty_weights():\n",
        "    ldm_unet = UNetModel(\n",
        "            image_size=32,\n",
        "            in_channels=in_chn,out_channels=4,\n",
        "                model_channels=320,\n",
        "                attention_resolutions=[4,2,1],\n",
        "                num_res_blocks=2,\n",
        "                channel_mult=[1,2,4,4],\n",
        "                num_heads=8,\n",
        "                use_spatial_transformer=True,\n",
        "                context_dim=768,\n",
        "                legacy= False).requires_grad_(False)\n",
        "  load_state_dict_with_low_memory(ldm_unet,mkmodel_state_dict([diffusion_emb,diffusion_mid,diffusion_out]),modifyfunc=sdt_func)\n",
        "  return ldm_unet\n",
        "\n",
        "def calclim():\n",
        "  rt=int((W*H)/0x1000)\n",
        "  if rt > 3:\n",
        "    return 0x800\n",
        "  elif rt > 2:\n",
        "    return 0x1000\n",
        "  else:\n",
        "    return 0x2000\n",
        "\n",
        "def waitingpip():\n",
        "  while not os.path.isfile('jkt.py'):\n",
        "    time.sleep(1)\n",
        "\n",
        "Lat_layer=3\n",
        "def showlat(tok):\n",
        "  ik=maalip[Lat_layer].permute(2,0,1)\n",
        "  wyd=W<<1\n",
        "  if Show_8heads:\n",
        "    kole=[None]*8\n",
        "    for i in range(8):\n",
        "      img=ik[tok][i]\n",
        "      mult=255/img.max()\n",
        "      kole[i]=(img*mult).reshape(H>>2,W>>2).numpy().astype(np.uint8)\n",
        "    im=np.hstack(kole)\n",
        "    wyd=W<<4\n",
        "  else:\n",
        "    img=ik[tok].sum(0).reshape(H>>2,W>>2)\n",
        "    mult=255/img.max()\n",
        "    im=(img*mult).numpy().astype(np.uint8)\n",
        "  im=Image.fromarray(im).resize((wyd, H<<1), resample=PIL.Image.LANCZOS)\n",
        "  print(tok_dict[tok])\n",
        "  display(im)\n",
        "\n",
        "tok_dict=False\n",
        "Show_8heads=False"
      ],
      "metadata": {
        "id": "CUe_MI9Yxsca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# visualize"
      ],
      "metadata": {
        "id": "l7wJXqpI0itJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only `(840k, Orig)` and `(470k, _a300)` are available, by now"
      ],
      "metadata": {
        "id": "-Iy2kv2K5UxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SDver='840k' #@param ['ckpt','470k','840k']\n",
        "Dfm='Orig' #@param ['Orig','_a300']\n",
        "alter_path='Opt: Put ckpt path here, then select [ ckpt ] in [ SDver ] (WIP)' #@param {type:'string'}\n",
        "if Dfm=='Orig':\n",
        "  Dfm=''\n",
        "SDver=f_dljit(SDver,Dfm)\n",
        "diffusion_emb = torch.jit.load(SDver+'diffusion_emb_pnnx.pt').eval()\n",
        "diffusion_mid = torch.jit.load(SDver+'diffusion_mid_pnnx.pt').eval()\n",
        "diffusion_out = torch.jit.load(SDver+'diffusion_out_pnnx.pt').eval()\n",
        "imgenc=torch.jit.load('imgencoder_pnnx.pt').eval()\n",
        "waitingpip()\n",
        "import BERTEmbedder\n",
        "ldm_unet=make_ldm_unet()\n",
        "from einops import rearrange, repeat\n",
        "from torch import nn, einsum"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Y0Xiui3BSOp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inject logging func to qkv"
      ],
      "metadata": {
        "id": "k8eBjz5n5voR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lim=0x2000\n",
        "def qkv_opt(zself,q,k,v):\n",
        "    h = zself.heads\n",
        "    q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q, k, v))\n",
        "    qsiz=q.shape[1]\n",
        "    k=k.transpose(1,2)\n",
        "    \n",
        "    if qsiz > lim:\n",
        "      out = []\n",
        "      kk=0\n",
        "      while kk < qsiz:\n",
        "        kend=kk+lim\n",
        "        if kend > qsiz:\n",
        "          kend=qsiz    \n",
        "        sim = torch.bmm(q[:,kk:kend],k) * zself.scale\n",
        "        attn = sim.softmax(dim=-1)\n",
        "        out.append(torch.bmm(attn, v))\n",
        "        kk+=lim\n",
        "      out=torch.cat(out,dim=1)\n",
        "    else:\n",
        "      sim = torch.bmm(q,k) * zself.scale\n",
        "      attn = sim.softmax(dim=-1)\n",
        "      if zself.attn_info==[1280,768]:\n",
        "        maalip.append(attn)\n",
        "      out = torch.bmm(attn, v)\n",
        "    out = rearrange(out, '(b h) n d -> b n (h d)', h=h)\n",
        "    return out\n",
        "\n",
        "from ldm.modules.attention import CrossAttention_config\n",
        "CrossAttention_config.qkv = qkv_opt"
      ],
      "metadata": {
        "id": "ypP9-R79UIFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Opt: download an example image"
      ],
      "metadata": {
        "id": "iWkZkabd52LR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O lara.jpg https://pbs.twimg.com/media/FhAgJgjWQAEHICc.jpg\n",
        "\n",
        "!wget https://raw.githubusercontent.com/justinpinkney/stable-diffusion/main/im-examples/Official_portrait_of_Barack_Obama.jpg\n",
        "#  A photo of Barack Obama smiling with a big grin\n",
        "\n",
        "!wget https://huggingface.co/datasets/Larvik/gelb/resolve/main/10_0A/001ae3b8.npy"
      ],
      "metadata": {
        "id": "t_FPajiU2ndZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize attn heatmap<br>`Complex_Prompt`: Turn it on when there's stupid gigantic tag salad, or when using TI. "
      ],
      "metadata": {
        "id": "LmTHrC8d570w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "See_Image = 'lara.jpg' #@param {type:'string'}\n",
        "See_concept = 'Lara croft, in rainforest, high definition, unreal engine, 4k' #@param {type:'string'}\n",
        "Complex_Prompt=False #@param {type:'boolean'}\n",
        "x_in=load_img(See_Image)\n",
        "H, W = x_in.shape[-2:]\n",
        "lim=calclim()\n",
        "maalip=[]\n",
        "t=torch.tensor([1e-7],dtype=torch.float)\n",
        "if Complex_Prompt:\n",
        "  cond_k, tok_dict = BERTEmbedder.encode_complex(See_concept)\n",
        "else:\n",
        "  cond_k, tok_dict = BERTEmbedder.encode(See_concept)\n",
        "cond_v=cond_k\n",
        "\n",
        "h, emb, hs = ldm_unet.forward_crossattn(x_in, t, cond_k,cond_v)   \n",
        "h = ldm_unet.forward2(h, emb, cond_k, *hs[6:],cond_v) \n",
        "for i in range(len(tok_dict)):\n",
        "  showlat(i)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ikgA7T8fYtJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advanced Settings"
      ],
      "metadata": {
        "id": "rFs23kPdIWQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Show_8heads=False #@param {type:'boolean'}\n",
        "Lat_layer=3 #@param {type:'integer'}\n",
        "TxtEnc_early_return=0 #@param [0,-1,-2,-3,-4]\n",
        "BERTEmbedder.transformer.cut=int(TxtEnc_early_return)\n",
        "\n",
        "if tok_dict:\n",
        "  for i in range(len(tok_dict)):\n",
        "    showlat(i)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4sbR8C4zIVZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BERTEmbedder.insert('<majipuri>')\n",
        "BERTEmbedder.insert('<pekora>')"
      ],
      "metadata": {
        "id": "rZisGdUc1Fsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# srfixg"
      ],
      "metadata": {
        "id": "dFjwxd76R9q2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37zzjIGHKZdN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "srz=torch.load('srz.pt',map_location=torch.device('cpu'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hyb=srz[...,0]"
      ],
      "metadata": {
        "id": "feePpYYqLC1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyb=hyb[0]+3.2833"
      ],
      "metadata": {
        "id": "MgNKpMHuLQry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img=(hyb*38.2).permute(1,2,0).numpy()"
      ],
      "metadata": {
        "id": "QUyvBjbHLspj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QpbioJuL77s",
        "outputId": "f1902a17-e8cd-4183-b51b-e51f50cff1d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(128, 128, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "cnH3cl7ZMJP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image.fromarray(img.astype(np.uint8), mode=\"RGB\")"
      ],
      "metadata": {
        "id": "JlENKWgaMP84"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}