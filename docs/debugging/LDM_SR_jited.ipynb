{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LDM_SR_jited.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "115Y_TyxYkbk"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SR"
      ],
      "metadata": {
        "id": "115Y_TyxYkbk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4vrU_GL_6f3"
      },
      "outputs": [],
      "source": [
        "!wget https://huggingface.co/Larvik/LDMjit/resolve/main/dm_pnnx.pt\n",
        "!wget https://huggingface.co/Larvik/LDMjit/resolve/main/fsd_pnnx.pt\n",
        "!wget https://huggingface.co/Larvik/LDMjit/resolve/main/alphas_cumprod.npy\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import functools\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "alphas_cumprod = np.load('alphas_cumprod.npy')\n",
        "\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "torch.set_num_threads(os.cpu_count())\n",
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True\n",
        "\n",
        "\n",
        "# ======================\n",
        "# Arguemnt Parser Config\n",
        "# ======================\n",
        "\n",
        "def imread(filename, flags=cv2.IMREAD_COLOR):\n",
        "    if not os.path.isfile(filename):\n",
        "        print(f\"File does not exist: {filename}\")\n",
        "        sys.exit()\n",
        "    data = np.fromfile(filename, np.int8)\n",
        "    img = cv2.imdecode(data, flags)\n",
        "    return img\n",
        "\n",
        "def preprocessing_img(img):\n",
        "    if len(img.shape) < 3:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGRA)\n",
        "    elif img.shape[2] == 3:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2BGRA)\n",
        "    elif img.shape[2] == 1:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGRA)\n",
        "    return img\n",
        "\n",
        "\n",
        "def load_image(image_path):\n",
        "    if os.path.isfile(image_path):\n",
        "        img = imread(image_path, cv2.IMREAD_UNCHANGED)\n",
        "    else:\n",
        "        print(f'{image_path} not found.')\n",
        "    return preprocessing_img(img)\n",
        "\n",
        "\n",
        "\n",
        "def im2col(images, filters, stride=1, pad=0):\n",
        "    if images.ndim == 2:\n",
        "        images = images.reshape(1, 1, *images.shape)\n",
        "    elif images.ndim == 3:\n",
        "        B, I_h, I_w = images.shape\n",
        "        images = images.reshape(B, 1, I_h, I_w)\n",
        "    B, C, I_h, I_w = images.shape\n",
        "\n",
        "    if isinstance(filters, tuple):\n",
        "        if len(filters) == 2:\n",
        "            filters = (1, 1, *filters)\n",
        "        elif len(filters) == 3:\n",
        "            M, F_h, F_w = filters\n",
        "            filters = (M, 1, F_h, F_w)\n",
        "        _, _, F_h, F_w = filters\n",
        "    else:\n",
        "        if filters.ndim == 2:\n",
        "            filters = filters.reshape(1, 1, *filters.shape)\n",
        "        elif filters.ndim == 3:\n",
        "            M, F_h, F_w = filters.shape\n",
        "            filters = filters.reshape(M, 1, F_h, F_w)\n",
        "        _, _, F_h, F_w = filters.shape\n",
        "\n",
        "    if isinstance(stride, tuple):\n",
        "        stride_ud, stride_lr = stride\n",
        "    else:\n",
        "        stride_ud = stride\n",
        "        stride_lr = stride\n",
        "    if isinstance(pad, tuple):\n",
        "        pad_ud, pad_lr = pad\n",
        "    elif isinstance(pad, int):\n",
        "        pad_ud = pad\n",
        "        pad_lr = pad\n",
        "    elif pad == \"same\":\n",
        "        pad_ud = 0.5 * ((I_h - 1) * stride_ud - I_h + F_h)\n",
        "        pad_lr = 0.5 * ((I_w - 1) * stride_lr - I_w + F_w)\n",
        "    pad_zero = (0, 0)\n",
        "\n",
        "    O_h = int((I_h - F_h + 2 * pad_ud) // stride_ud + 1)\n",
        "    O_w = int((I_w - F_w + 2 * pad_lr) // stride_lr + 1)\n",
        "\n",
        "    result_pad = (pad_ud, pad_lr)\n",
        "    pad_ud = int(np.ceil(pad_ud))\n",
        "    pad_lr = int(np.ceil(pad_lr))\n",
        "    pad_ud = (pad_ud, pad_ud)\n",
        "    pad_lr = (pad_lr, pad_lr)\n",
        "    images = np.pad(\n",
        "        images, [pad_zero, pad_zero, pad_ud, pad_lr], \"constant\")\n",
        "\n",
        "    cols = np.empty((B, C, F_h, F_w, O_h, O_w))\n",
        "    for h in range(F_h):\n",
        "        h_lim = h + stride_ud * O_h\n",
        "        for w in range(F_w):\n",
        "            w_lim = w + stride_lr * O_w\n",
        "            cols[:, :, h, w, :, :] = \\\n",
        "                images[:, :, h:h_lim:stride_ud, w:w_lim:stride_lr]\n",
        "\n",
        "    cols = cols.transpose(1, 2, 3, 0, 4, 5).reshape(C * F_h * F_w, B * O_h * O_w)\n",
        "\n",
        "    return cols, (O_h, O_w), result_pad\n",
        "\n",
        "\n",
        "def col2im(cols, I_shape, O_shape, stride=1, pad=0):\n",
        "    def get_f_shape(i, o, s, p):\n",
        "        return int(i + 2 * p - (o - 1) * s)\n",
        "\n",
        "    if len(I_shape) == 2:\n",
        "        B = C = 1\n",
        "        I_h, I_w = I_shape\n",
        "    elif len(I_shape) == 3:\n",
        "        C = 1\n",
        "        B, I_h, I_w = I_shape\n",
        "    else:\n",
        "        B, C, I_h, I_w = I_shape\n",
        "    O_h, O_w = O_shape\n",
        "\n",
        "    if isinstance(stride, tuple):\n",
        "        stride_ud, stride_lr = stride\n",
        "    else:\n",
        "        stride_ud = stride\n",
        "        stride_lr = stride\n",
        "    if isinstance(pad, tuple):\n",
        "        pad_ud, pad_lr = pad\n",
        "    elif isinstance(pad, int):\n",
        "        pad_ud = pad\n",
        "        pad_lr = pad\n",
        "\n",
        "    F_h = get_f_shape(I_h, O_h, stride_ud, pad_ud)\n",
        "    F_w = get_f_shape(I_w, O_w, stride_lr, pad_lr)\n",
        "    pad_ud = int(np.ceil(pad_ud))\n",
        "    pad_lr = int(np.ceil(pad_lr))\n",
        "    cols = cols.reshape(C, F_h, F_w, B, O_h, O_w).transpose(3, 0, 1, 2, 4, 5)\n",
        "    images = np.zeros((B, C, I_h + 2 * pad_ud + stride_ud - 1, I_w + 2 * pad_lr + stride_lr - 1))\n",
        "\n",
        "    for h in range(F_h):\n",
        "        h_lim = h + stride_ud * O_h\n",
        "        for w in range(F_w):\n",
        "            w_lim = w + stride_lr * O_w\n",
        "            images[:, :, h:h_lim:stride_ud, w:w_lim:stride_lr] += cols[:, :, h, w, :, :]\n",
        "\n",
        "    return images[:, :, pad_ud: I_h + pad_ud, pad_lr: I_w + pad_lr]\n",
        "\n",
        "def meshgrid(h, w):\n",
        "    y = np.arange(0, h).reshape(h, 1, 1).repeat(w, axis=1)\n",
        "    x = np.arange(0, w).reshape(1, w, 1).repeat(h, axis=0)\n",
        "    arr = np.concatenate([y, x], axis=-1)\n",
        "\n",
        "    return arr\n",
        "\n",
        "\n",
        "def delta_border(h, w):\n",
        "    \"\"\"\n",
        "    :param h: height\n",
        "    :param w: width\n",
        "    :return: normalized distance to image border,\n",
        "     wtith min distance = 0 at border and max dist = 0.5 at image center\n",
        "    \"\"\"\n",
        "    lower_right_corner = np.array([h - 1, w - 1]).reshape(1, 1, 2)\n",
        "    arr = meshgrid(h, w) / lower_right_corner\n",
        "    dist_left_up = np.min(arr, axis=-1, keepdims=True)\n",
        "    dist_right_down = np.min(1 - arr, axis=-1, keepdims=True)\n",
        "\n",
        "    edge_dist = np.min(np.concatenate([dist_left_up, dist_right_down], axis=-1), axis=-1)\n",
        "\n",
        "    return edge_dist\n",
        "\n",
        "\n",
        "def get_weighting(h, w, Ly, Lx):\n",
        "    clip_min_weight = 0.01\n",
        "    clip_max_weight = 0.5\n",
        "\n",
        "    weighting = delta_border(h, w)\n",
        "    weighting = np.clip(weighting, clip_min_weight, clip_max_weight)\n",
        "    weighting = weighting.reshape(1, h * w, 1).repeat(Ly * Lx, axis=-1)\n",
        "\n",
        "    return weighting\n",
        "\n",
        "\n",
        "def get_fold_unfold(x, kernel_size, stride, uf=1, df=1):\n",
        "    \"\"\"\n",
        "    :param x: img of size (bs, c, h, w)\n",
        "    :return: n img crops of size (n, bs, c, kernel_size[0], kernel_size[1])\n",
        "    \"\"\"\n",
        "    bs, nc, h, w = x.shape\n",
        "\n",
        "    # number of crops in image\n",
        "    Ly = (h - kernel_size[0]) // stride[0] + 1\n",
        "    Lx = (w - kernel_size[1]) // stride[1] + 1\n",
        "\n",
        "    unfold = functools.partial(im2col, filters=kernel_size, stride=stride)\n",
        "    if uf == 1 and df == 1:\n",
        "        fold = functools.partial(\n",
        "            col2im,\n",
        "            stride=stride)\n",
        "\n",
        "        weighting = get_weighting(kernel_size[0], kernel_size[1], Ly, Lx)\n",
        "        weighting = weighting.reshape((1, 1, kernel_size[0], kernel_size[1], Ly * Lx))\n",
        "\n",
        "    elif uf > 1 and df == 1:\n",
        "        fold = functools.partial(\n",
        "            col2im,\n",
        "            stride=(stride[0] * uf, stride[1] * uf))\n",
        "\n",
        "        weighting = get_weighting(kernel_size[0] * uf, kernel_size[1] * uf, Ly, Lx)\n",
        "        weighting = weighting.reshape((1, 1, kernel_size[0] * uf, kernel_size[1] * uf, Ly * Lx))\n",
        "\n",
        "    elif df > 1 and uf == 1:\n",
        "        fold = functools.partial(\n",
        "            col2im,\n",
        "            stride=(stride[0] // df, stride[1] // df))\n",
        "\n",
        "        weighting = get_weighting(kernel_size[0] // df, kernel_size[1] // df, Ly, Lx)\n",
        "        weighting = weighting.reshape((1, 1, kernel_size[0] // df, kernel_size[1] // df, Ly * Lx))\n",
        "\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    return fold, unfold, weighting\n",
        "\n",
        "# ======================\n",
        "# Secondaty Functions\n",
        "# ======================\n",
        "\n",
        "def make_ddim_timesteps(num_ddim_timesteps, num_ddpm_timesteps):\n",
        "    c = num_ddpm_timesteps // num_ddim_timesteps\n",
        "    ddim_timesteps = np.asarray(list(range(0, num_ddpm_timesteps, c)))\n",
        "\n",
        "    # add one to get the final alpha values right (the ones from first scale to data during sampling)\n",
        "    steps_out = ddim_timesteps + 1\n",
        "\n",
        "    return steps_out\n",
        "\n",
        "\n",
        "def make_ddim_sampling_parameters(alphacums, ddim_timesteps, eta):\n",
        "    # select alphas for computing the variance schedule\n",
        "    alphas = alphacums[ddim_timesteps]\n",
        "    alphas_prev = np.asarray([alphacums[0]] + alphacums[ddim_timesteps[:-1]].tolist())\n",
        "\n",
        "    # according the the formula provided in https://arxiv.org/abs/2010.02502\n",
        "    sigmas = eta * np.sqrt((1 - alphas_prev) / (1 - alphas) * (1 - alphas / alphas_prev))\n",
        "\n",
        "    return sigmas, alphas, alphas_prev\n",
        "\n",
        "\n",
        "# ======================\n",
        "# Main functions\n",
        "# ======================\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def normalize_image(image, normalize_type='255'):\n",
        "    \"\"\"\n",
        "    Normalize image\n",
        "    Parameters\n",
        "    ----------\n",
        "    image: numpy array\n",
        "        The image you want to normalize\n",
        "    normalize_type: string\n",
        "        Normalize type should be chosen from the type below.\n",
        "        - '255': simply dividing by 255.0\n",
        "        - '127.5': output range : -1 and 1\n",
        "        - 'ImageNet': normalize by mean and std of ImageNet\n",
        "        - 'None': no normalization\n",
        "    Returns\n",
        "    -------\n",
        "    normalized_image: numpy array\n",
        "    \"\"\"\n",
        "    if normalize_type == 'None':\n",
        "        return image\n",
        "    elif normalize_type == '255':\n",
        "        return image / 255.0\n",
        "    elif normalize_type == '127.5':\n",
        "        return image / 127.5 - 1.0\n",
        "    elif normalize_type == 'ImageNet':\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        image = image / 255.0\n",
        "        for i in range(3):\n",
        "            image[:, :, i] = (image[:, :, i] - mean[i]) / std[i]\n",
        "        return image\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "\n",
        "\n",
        "def preprocess(img):\n",
        "    im_h, im_w, _ = img.shape\n",
        "\n",
        "    up_f = 4\n",
        "    oh, ow = up_f * im_h, up_f * im_w\n",
        "\n",
        "    img = normalize_image(img, normalize_type='255')\n",
        "\n",
        "    c = img * 2 - 1\n",
        "    c = c.transpose(2, 0, 1)  # HWC -> CHW\n",
        "    c = np.expand_dims(c, axis=0)\n",
        "    c = c.astype(np.float32)\n",
        "\n",
        "    c_up = cv2.resize(img, (ow, oh), interpolation=cv2.INTER_LINEAR)\n",
        "    c_up = c_up.transpose(2, 0, 1)  # HWC -> CHW\n",
        "    c_up = np.expand_dims(c_up, axis=0)\n",
        "    c_up = c_up.astype(np.float32)\n",
        "\n",
        "    return c_up, c\n",
        "\n",
        "\n",
        "def postprocess(sample):\n",
        "    sample = np.clip(sample, -1., 1.)\n",
        "    sample = (sample + 1.) / 2. * 255\n",
        "    sample = np.transpose(sample, (1, 2, 0))\n",
        "    sample = sample[:, :, ::-1]  # RGB -> BGR\n",
        "    sample = sample.astype(np.uint8)\n",
        "\n",
        "    return sample\n",
        "\n",
        "\n",
        "def ddim_sampling(\n",
        "        models, cond):\n",
        "    shape = cond.shape\n",
        "    img = np.random.randn(shape[0] * shape[1] * shape[2] * shape[3]).reshape(shape)\n",
        "    img = img.astype(np.float32)\n",
        "\n",
        "    timesteps = ddim_timesteps\n",
        "    time_range = np.flip(timesteps)\n",
        "    total_steps = timesteps.shape[0]\n",
        "\n",
        "    print(f\"Running DDIM Sampling with {total_steps} timesteps\")\n",
        "\n",
        "    try:\n",
        "        from tqdm import tqdm\n",
        "        iterator = tqdm(time_range, desc='DDIM Sampler', total=total_steps)\n",
        "    except ModuleNotFoundError:\n",
        "        def iter_func(a):\n",
        "            for i, x in enumerate(a):\n",
        "                print(\"DDIM Sampler: %s/%s\" % (i + 1, len(a)))\n",
        "                yield x\n",
        "\n",
        "        iterator = iter_func(time_range)\n",
        "\n",
        "    for i, step in enumerate(iterator):\n",
        "        index = total_steps - i - 1\n",
        "        ts = np.full((shape[0],), step, dtype=np.int64)\n",
        "\n",
        "        img, pred_x0 = p_sample_ddim(\n",
        "            models,\n",
        "            img, cond, ts,\n",
        "            index=index,\n",
        "        )\n",
        "        img = img.astype(np.float32)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "# ddim\n",
        "def p_sample_ddim(\n",
        "        models, x, c, t, index,\n",
        "        temperature=1):\n",
        "    e_t = apply_model(models, x, t, c)\n",
        "\n",
        "    alphas = ddim_alphas\n",
        "    alphas_prev = ddim_alphas_prev\n",
        "    sqrt_one_minus_alphas = ddim_sqrt_one_minus_alphas\n",
        "    sigmas = ddim_sigmas\n",
        "\n",
        "    # select parameters corresponding to the currently considered timestep\n",
        "    b, *_ = x.shape\n",
        "    a_t = np.full((b, 1, 1, 1), alphas[index])\n",
        "    a_prev = np.full((b, 1, 1, 1), alphas_prev[index])\n",
        "    sigma_t = np.full((b, 1, 1, 1), sigmas[index])\n",
        "    sqrt_one_minus_at = np.full((b, 1, 1, 1), sqrt_one_minus_alphas[index])\n",
        "\n",
        "    # current prediction for x_0\n",
        "    pred_x0 = (x - sqrt_one_minus_at * e_t) / np.sqrt(a_t)\n",
        "\n",
        "    # direction pointing to x_t\n",
        "    dir_xt = np.sqrt(1. - a_prev - sigma_t ** 2) * e_t\n",
        "\n",
        "    noise = sigma_t * np.random.randn(x.size).reshape(x.shape) * temperature\n",
        "    x_prev = np.sqrt(a_prev) * pred_x0 + dir_xt + noise\n",
        "\n",
        "    return x_prev, pred_x0\n",
        "\n",
        "\n",
        "def decode_first_stage(models, z):\n",
        "    ks = (128, 128)\n",
        "    stride = (64, 64)\n",
        "    uf = 4\n",
        "\n",
        "    bs, nc, h, w = z.shape\n",
        "\n",
        "    fold, unfold, weighting = get_fold_unfold(z, ks, stride, uf=uf)\n",
        "\n",
        "    z, o_shape, _ = unfold(z)  # (bn, nc * prod(**ks), L)\n",
        "\n",
        "    # Reshape to img shape\n",
        "    z = z.reshape((bs, -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n",
        "    z = z.astype(np.float32)\n",
        "\n",
        "    print('first_stage_decode...')\n",
        "\n",
        "    first_stage_decode = models['first_stage_decode']\n",
        "    outputs = []\n",
        "    for i in range(z.shape[-1]):\n",
        "        x = z[:, :, :, :, i]\n",
        "        output = first_stage_decode(torch.tensor(x).cuda())\n",
        "        outputs.append(output[0].cpu().numpy())\n",
        "\n",
        "    o = np.stack(outputs, axis=-1)  # # (bn, nc, ks[0], ks[1], L)\n",
        "    o = o * weighting\n",
        "\n",
        "    # Reverse reshape to img shape\n",
        "    o = o.reshape((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n",
        "    # stitch crops together\n",
        "    decoded = fold(o, I_shape=(1, 3, h * uf, w * uf), O_shape=o_shape)\n",
        "\n",
        "    normalization = fold(weighting, I_shape=(1, 1, h * uf, w * uf), O_shape=o_shape)\n",
        "    decoded = decoded / normalization  # norm is shape (1, 1, h, w)\n",
        "\n",
        "    return decoded\n",
        "\n",
        "\n",
        "# ddpm\n",
        "def apply_model(models, x_noisy, t, cond):\n",
        "    ks = (128, 128)\n",
        "    stride = (64, 64)\n",
        "\n",
        "    bs, nc, h, w = x_noisy.shape\n",
        "\n",
        "    fold, unfold, weighting = get_fold_unfold(x_noisy, ks, stride)\n",
        "\n",
        "    z, o_shape, _ = unfold(x_noisy)  # (bn, nc * prod(**ks), L)\n",
        "    # Reshape to img shape\n",
        "    z = z.reshape((bs, -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n",
        "    z_list = [z[:, :, :, :, i] for i in range(z.shape[-1])]\n",
        "\n",
        "    c, *_ = unfold(cond)\n",
        "    c = c.reshape((bs, -1, ks[0], ks[1], c.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n",
        "    cond_list = [c[:, :, :, :, i] for i in range(c.shape[-1])]\n",
        "\n",
        "    # apply model by loop over crops\n",
        "    diffusion_model = models[\"diffusion_model\"]\n",
        "    outputs = []\n",
        "    for i in range(z.shape[-1]):\n",
        "        x = z_list[i]\n",
        "        cond = cond_list[i]\n",
        "        xc = np.concatenate([x, cond], axis=1)\n",
        "        xc = xc.astype(np.float32)\n",
        "        \n",
        "        output = diffusion_model(torch.tensor(xc).cuda(), torch.tensor(t).cuda())\n",
        "            \n",
        "       \n",
        "        outputs.append(output[0].cpu().numpy())\n",
        "\n",
        "    o = np.stack(outputs, axis=-1)\n",
        "    o = o * weighting\n",
        "\n",
        "    # Reverse reshape to img shape\n",
        "    o = o.reshape((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n",
        "    # stitch crops together\n",
        "    normalization = fold(weighting, I_shape=(1, 1, h, w), O_shape=o_shape)\n",
        "    x_recon = fold(o, I_shape=(1, 3, h, w), O_shape=o_shape) / normalization\n",
        "\n",
        "    return x_recon\n",
        "\n",
        "\n",
        "def predict(models, img):\n",
        "    img = img[:, :, ::-1]  # BGR -> RGB\n",
        "\n",
        "    _, c = preprocess(img)\n",
        "\n",
        "    samples = ddim_sampling(models, c)\n",
        "\n",
        "    x_sample = decode_first_stage(models, samples)\n",
        "\n",
        "    img = postprocess(x_sample[0])\n",
        "\n",
        "    return img\n",
        "\n",
        "models = dict(\n",
        "    first_stage_decode=torch.jit.load('/content/fsd_pnnx.pt').eval().cuda(),\n",
        "    diffusion_model=torch.jit.load('/content/dm_pnnx.pt').eval().cuda(),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "ddim_timesteps\n",
        "\"\"\"\n",
        "ddim_eta = 1.0\n",
        "ddim_num_steps = 100\n",
        "ddpm_num_timesteps = 1000\n",
        "ddim_timesteps = make_ddim_timesteps(ddim_num_steps, ddpm_num_timesteps)\n",
        "\n",
        "\"\"\"\n",
        "ddim sampling parameters\n",
        "\"\"\"\n",
        "\n",
        "ddim_sigmas, ddim_alphas, ddim_alphas_prev = \\\n",
        "    make_ddim_sampling_parameters(\n",
        "        alphacums=alphas_cumprod,\n",
        "        ddim_timesteps=ddim_timesteps,\n",
        "        eta=ddim_eta)\n",
        "\n",
        "ddim_sqrt_one_minus_alphas = np.sqrt(1. - ddim_alphas)\n",
        "\n",
        "inputz=['/content/sample_data/zkrp.jpg']\n",
        "\n",
        " \n",
        "\n",
        "for image_path in inputz:\n",
        "    print(image_path)\n",
        "\n",
        "    # prepare input data\n",
        "    img = load_image(image_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
        "\n",
        "    # inference\n",
        "    print('Start inference...')\n",
        "    \n",
        "    img = predict(models, img)\n",
        "\n",
        "    # plot result\n",
        "    savepath = '/content/sample_data/out.png'\n",
        "    print(f'saved at : {savepath}')\n",
        "    cv2.imwrite(savepath, img)\n",
        "\n",
        "print('Script finished successfully.')"
      ],
      "metadata": {
        "id": "xrSkT1_5AJSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# txt2img"
      ],
      "metadata": {
        "id": "PWCzxzNkYpEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!wget https://huggingface.co/Larvik/tempsd/resolve/main/alphas_cumprod.npz\n",
        "!wget https://huggingface.co/Larvik/temp1/resolve/main/transformer_pnnx.pt\n",
        "!wget https://huggingface.co/Larvik/temp1/resolve/main/diffusion_emb_pnnx.pt\n",
        "!wget https://huggingface.co/Larvik/temp1/resolve/main/diffusion_mid_pnnx.pt\n",
        "!wget https://huggingface.co/Larvik/temp1/resolve/main/diffusion_out_pnnx.pt\n",
        "!wget https://huggingface.co/Larvik/temp1/resolve/main/autoencoder_pnnx.pt\n",
        "!wget https://huggingface.co/Larvik/temp1/resolve/main/imgencoder_pnnx.pt"
      ],
      "metadata": {
        "id": "WxCkSordYqt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jit=False #@param {type:'boolean'}\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "from threading import Thread\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import PIL\n",
        "from transformers import CLIPTokenizer\n",
        "\n",
        "import torch\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "torch.set_num_threads(os.cpu_count())\n",
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True\n",
        "\n",
        "alphas_cumprod = np.load('alphas_cumprod.npz')['a']\n",
        "\n",
        "cudev=torch.device('cuda')\n",
        "\n",
        "\n",
        "def extract_into_tensor(a, t, x_shape):\n",
        "    b, *_ = t.shape\n",
        "    out = a.gather(-1, t)\n",
        "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
        "\n",
        "\n",
        "def stochastic_encode(x0, t, use_original_steps=False, noise=None):\n",
        "    print('came ddim stochastic_encode')\n",
        "    # fast, but does not allow for exact reconstruction\n",
        "    # t serves as an index to gather the correct alphas\n",
        " \n",
        "    sqrt_alphas_cumprod = torch.sqrt(torch.tensor(ddim_alphas))\n",
        "    sqrt_one_minus_alphas_cumprod = torch.tensor(ddim_sqrt_one_minus_alphas)\n",
        "\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(x0)\n",
        "    return (extract_into_tensor(sqrt_alphas_cumprod, t, x0.shape) * x0 +\n",
        "            extract_into_tensor(sqrt_one_minus_alphas_cumprod, t, x0.shape) * noise)\n",
        "\n",
        "# ======================\n",
        "# Secondaty Functions\n",
        "# ======================\n",
        "\n",
        "def make_ddim_timesteps(num_ddim_timesteps, num_ddpm_timesteps):\n",
        "    c = num_ddpm_timesteps // num_ddim_timesteps\n",
        "    ddim_timesteps = np.asarray(list(range(0, num_ddpm_timesteps, c)))\n",
        "\n",
        "    # add one to get the final alpha values right (the ones from first scale to data during sampling)\n",
        "    steps_out = ddim_timesteps + 1\n",
        "\n",
        "    return steps_out\n",
        "\n",
        "\n",
        "def make_ddim_sampling_parameters(alphacums, ddim_timesteps, eta):\n",
        "    # select alphas for computing the variance schedule\n",
        "    alphas = alphacums[ddim_timesteps]\n",
        "    alphas_prev = np.asarray([alphacums[0]] + alphacums[ddim_timesteps[:-1]].tolist())\n",
        "\n",
        "    # according the the formula provided in https://arxiv.org/abs/2010.02502\n",
        "    sigmas = eta * np.sqrt((1 - alphas_prev) / (1 - alphas) * (1 - alphas / alphas_prev))\n",
        "\n",
        "    return sigmas, alphas, alphas_prev\n",
        "\n",
        "\n",
        "# ======================\n",
        "# Main functions\n",
        "# ======================\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# encoder\n",
        "class BERTEmbedder:\n",
        "    \"\"\" Uses a pretrained BERT tokenizer by huggingface. Vocab size: 30522 (?)\"\"\"\n",
        "\n",
        "    def __init__(self, transformer, max_length=77):\n",
        "        self.tokenizer = CLIPTokenizer.from_pretrained('openai/clip-vit-large-patch14')\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.transformer = transformer\n",
        "        \n",
        "\n",
        "    def encode(self, text, nsamp):\n",
        "        batch_encoding = self.tokenizer(text, truncation=True, max_length=self.max_length, return_length=True,\n",
        "                            return_overflowing_tokens=False, padding=\"max_length\", return_tensors=\"pt\")\n",
        "\n",
        "        tokens = batch_encoding[\"input_ids\"]\n",
        "        tokens = torch.tensor(tokens.numpy()).expand(nsamp,-1)\n",
        "        \n",
        "\n",
        "       \n",
        "        z = self.transformer(tokens)\n",
        "        \n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "        return z.cuda()\n",
        "\n",
        "preimg=None\n",
        "\n",
        "def ddim_sampling(\n",
        "        models, \n",
        "        cond,  t_start,\n",
        "        unconditional_guidance_scale=1.0,\n",
        "        unconditional_conditioning=None):\n",
        "    \n",
        "    \n",
        "    if preimg is not None:\n",
        "      t_start= int(strength * ddim_num_steps)\n",
        "      img=stochastic_encode(preimg,  torch.tensor([t_start]*n_samples,device='cpu')).numpy()\n",
        "    else:\n",
        "      img = np.random.randn(shape[0] * shape[1] * shape[2] * shape[3]).reshape(shape).astype(np.float32)\n",
        "\n",
        "    timesteps = ddim_timesteps[:t_start]\n",
        "    time_range = np.flip(timesteps)\n",
        "    total_steps = timesteps.shape[0]\n",
        "\n",
        "    print(f\"Running DDIM Sampling with {total_steps} timesteps\")\n",
        "\n",
        "    try:\n",
        "        from tqdm import tqdm\n",
        "        iterator = tqdm(time_range, desc='DDIM Sampler', total=total_steps)\n",
        "    except ModuleNotFoundError:\n",
        "        def iter_func(a):\n",
        "            for i, x in enumerate(a):\n",
        "                print(\"DDIM Sampler: %s/%s\" % (i + 1, len(a)))\n",
        "                yield x\n",
        "\n",
        "        iterator = iter_func(time_range)\n",
        "\n",
        "    for i, step in enumerate(iterator):\n",
        "        index = total_steps - i - 1\n",
        "        ts = np.full((shape[0],), step, dtype=np.int64)\n",
        "\n",
        "        img, pred_x0 = p_sample_ddim(\n",
        "            models,\n",
        "            img, cond, ts,\n",
        "            index=index,\n",
        "            unconditional_guidance_scale=unconditional_guidance_scale,\n",
        "            unconditional_conditioning=unconditional_conditioning,\n",
        "        )\n",
        "        img = img.astype(np.float32)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "# ddim\n",
        "def p_sample_ddim(\n",
        "        models, x, c, t, index,\n",
        "        temperature=1.,\n",
        "        unconditional_guidance_scale=1.,\n",
        "        unconditional_conditioning=None):\n",
        "    x_in = torch.tensor(np.concatenate([x] * 2) ,device=cudev)\n",
        "    t_in = torch.tensor(np.concatenate([t] * 2) ,device=cudev)\n",
        "    c_in = torch.cat( ( unconditional_conditioning , c) )\n",
        "\n",
        "    x_recon = apply_model(models, x_in, t_in, c_in).cpu().numpy()\n",
        "    e_t_uncond, e_t = np.split(x_recon, 2)\n",
        "\n",
        "    e_t = e_t_uncond + unconditional_guidance_scale * (e_t - e_t_uncond)\n",
        "\n",
        "    alphas = ddim_alphas\n",
        "    alphas_prev = ddim_alphas_prev\n",
        "    sqrt_one_minus_alphas = ddim_sqrt_one_minus_alphas\n",
        "    sigmas = ddim_sigmas\n",
        "\n",
        "    # select parameters corresponding to the currently considered timestep\n",
        "    b, *_ = x.shape\n",
        "    a_t = np.full((b, 1, 1, 1), alphas[index])\n",
        "    a_prev = np.full((b, 1, 1, 1), alphas_prev[index])\n",
        "    sigma_t = np.full((b, 1, 1, 1), sigmas[index])\n",
        "    sqrt_one_minus_at = np.full((b, 1, 1, 1), sqrt_one_minus_alphas[index])\n",
        "\n",
        "    # current prediction for x_0\n",
        "    pred_x0 = (x - sqrt_one_minus_at * e_t) / np.sqrt(a_t)\n",
        "\n",
        "    # direction pointing to x_t\n",
        "    dir_xt = np.sqrt(1. - a_prev - sigma_t ** 2) * e_t\n",
        "\n",
        "    noise = sigma_t * np.random.randn(x.size).reshape(x.shape) * temperature\n",
        "    x_prev = np.sqrt(a_prev) * pred_x0 + dir_xt + noise\n",
        "\n",
        "    return x_prev, pred_x0\n",
        "\n",
        "\n",
        "# ddpm\n",
        "def apply_model(models, x, t, cc):\n",
        "    diffusion_emb = models[\"diffusion_emb\"]\n",
        "    diffusion_mid = models[\"diffusion_mid\"]\n",
        "    diffusion_out = models[\"diffusion_out\"]\n",
        "\n",
        "    with torch.cuda.amp.autocast(dtype=torch.float16):\n",
        "      output = diffusion_emb(x, t, cc)\n",
        "      \n",
        "      h, emb, hs = output\n",
        "      '''\n",
        "      print(h.shape)\n",
        "      print(emb.shape)\n",
        "      \n",
        "      for aaa in hs:\n",
        "        print(aaa.shape)\n",
        "      '''\n",
        "      h = diffusion_mid(h, emb, cc, *hs[6:])\n",
        "\n",
        "      #print(h.shape)\n",
        "\n",
        "      output = diffusion_out(h, emb, cc, *hs[:6])\n",
        "    #print(output.shape)\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "# decoder\n",
        "def decode_first_stage(models, z):\n",
        "    scale_factor = 0.18215\n",
        "    z = z / scale_factor\n",
        "\n",
        "    autoencoder = models['autoencoder']\n",
        "    \n",
        "    \n",
        "    output = autoencoder(z)\n",
        "   \n",
        "    \n",
        "    return output\n",
        "\n",
        "def saver():\n",
        "  global x_samples_ddim\n",
        "  np.save( (outputp%(iita,1))[:-6] + '.npy', samples)\n",
        "  x_samples_ddim = np.clip((x_samples_ddim.numpy() + 1.0) / 2.0, a_min=0.0, a_max=1.0)\n",
        "  k=0\n",
        "  for x_sample in x_samples_ddim:\n",
        "      x_sample = x_sample.transpose(1, 2, 0)  # CHW -> HWC\n",
        "      x_sample = x_sample * 255\n",
        "      img = x_sample.astype(np.uint8)\n",
        "      img = img[:, :, ::-1]  # RGB -> BGR\n",
        "      cv2.imwrite(outputp%(iita,k), img)\n",
        "      k+=1\n",
        "  \n",
        "\n",
        "    \n",
        "def load_img(path):\n",
        "    image = Image.open(path).convert(\"RGB\")\n",
        "    w, h = image.size\n",
        "    print(f\"loaded input image of size ({w}, {h}) from {path}\")\n",
        "    w2, h2 = map(lambda x: x - x % 32, (w, h))\n",
        "    if w!=w2 or h!=h2:\n",
        "      image = image.resize((w2, h2), resample=PIL.Image.LANCZOS)\n",
        "    image = np.array(image).astype(np.float32) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    return 2.*image - 1.\n",
        "\n",
        "def predict(\n",
        "        models, cond_stage_model,\n",
        "        prompt, uc):\n",
        "    global x_samples_ddim\n",
        "    global samples\n",
        "    \n",
        "\n",
        "    c = cond_stage_model.encode(prompt, n_samples)\n",
        "    \n",
        "    \n",
        "   \n",
        "    samples = ddim_sampling(\n",
        "        models, c, ddim_num_steps,\n",
        "        unconditional_guidance_scale=scale,\n",
        "        unconditional_conditioning=uc)\n",
        "\n",
        "    \n",
        "    x_samples_ddim = decode_first_stage(models, torch.tensor(samples, device=cudev) ).cpu()\n",
        "    t3 = Thread(target = saver)\n",
        "    a3 = t3.start()\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "def init_img_type():\n",
        "  global init_img\n",
        "  if init_img.endswith('npy'):\n",
        "    return 0\n",
        "  elif init_img.endswith('jpg') or init_img.endswith('png'):\n",
        "    if os.path.isfile(init_img+'.npy'):\n",
        "      init_img+='.npy'\n",
        "      return 0\n",
        "    else:\n",
        "      return 1\n",
        "  else:\n",
        "    return 99\n",
        "\n",
        "\n",
        "\n",
        "models = dict(\n",
        "    transformer = torch.jit.load('transformer_pnnx.pt').eval(),\n",
        "    diffusion_emb = torch.jit.load('diffusion_emb_pnnx.pt').eval().cuda().half(),\n",
        "    diffusion_mid = torch.jit.load('diffusion_mid_pnnx.pt').eval().cuda().half(),\n",
        "    diffusion_out = torch.jit.load('diffusion_out_pnnx.pt').eval().cuda().half(),\n",
        "    autoencoder = torch.jit.load('autoencoder_pnnx.pt').eval().cuda(),\n",
        ")\n",
        "    \n",
        "imgenc=torch.jit.load('imgencoder_pnnx.pt').eval()\n"
      ],
      "metadata": {
        "id": "fmYgI8PLYudK",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL\n",
        "init_img='xxx' #@param {type:'string'}\n",
        "initymgtyp=init_img_type()\n",
        "if initymgtyp == 0:\n",
        "  preimg=torch.tensor(np.load(init_img), device='cpu')\n",
        "  n_samples=preimg.size(0)\n",
        "  H=preimg.size(2)<<3\n",
        "  W=preimg.size(3)<<3\n",
        "elif initymgtyp == 1:\n",
        "  n_samples=1\n",
        "  rpt=load_img(init_img)\n",
        "  H=rpt.size(2)\n",
        "  W=rpt.size(3)\n",
        "  preimg=imgenc(  rpt, torch.randn(torch.Size([n_samples,4,H>>3,W>>3]))  )*0.18215\n",
        "  np.save(init_img+'.npy',preimg.numpy())\n",
        "else:\n",
        "  preimg=None"
      ],
      "metadata": {
        "id": "I1FtIJmB2Ojd",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "InThread=False #@param {type:'boolean'}\n",
        "\n",
        "prompt = 'a photograph of an astronaut riding a horse' #@param {type:'string'}\n",
        "\n",
        "\n",
        "if preimg is None:\n",
        "  n_samples = 1 #@param {type:'integer'}\n",
        "  H=768 #@param {type:'integer'}\n",
        "  W=704 #@param {type:'integer'}\n",
        "\n",
        "\n",
        "\n",
        "ddim_num_steps = 50  #@param {type:'integer'}\n",
        "ddpm_num_timesteps = 1000\n",
        "\n",
        "outputp='/content/sample_data' #@param {type:'string'}\n",
        "\n",
        "n_iter = 1 #@param {type:'integer'}\n",
        "\n",
        "strength=0.5 #@param {type:'number'}\n",
        "\n",
        "seed=0 #@param {type:'integer'}\n",
        "\n",
        "scale = 7.5 #@param {type:'number'}\n",
        "ddim_eta = 0  #@param {type:'integer'}\n",
        "\n",
        "\n",
        "outputp=outputp+'/'+str(len(os.listdir(outputp)))+'_%dx%d.png'\n",
        "\"\"\"\n",
        "ddim_timesteps\n",
        "\"\"\"\n",
        "\n",
        "ddim_timesteps = make_ddim_timesteps(\n",
        "    ddim_num_steps, ddpm_num_timesteps)\n",
        "\n",
        "\"\"\"\n",
        "ddim sampling parameters\n",
        "\"\"\"\n",
        "\n",
        "ddim_sigmas, ddim_alphas, ddim_alphas_prev = \\\n",
        "    make_ddim_sampling_parameters(\n",
        "        alphacums=alphas_cumprod,\n",
        "        ddim_timesteps=ddim_timesteps,\n",
        "        eta=ddim_eta)\n",
        "\n",
        "ddim_sqrt_one_minus_alphas = np.sqrt(1. - ddim_alphas)\n",
        "\n",
        "shape = [n_samples, 4, H>>3 , W>>3 ]\n",
        "\n",
        "\n",
        "if seed == 0:\n",
        "  np.random.seed(random.randint(0, 2**32))\n",
        "else:\n",
        "  np.random.seed(seed)\n",
        "\n",
        "cond_stage_model = BERTEmbedder(models['transformer'])\n",
        "\n",
        "\n",
        "print(\"prompt: %s\" % prompt)\n",
        "\n",
        "\n",
        "\n",
        "print('Start inference...')\n",
        "uc = None\n",
        "if scale != 1.0:\n",
        "  uc = cond_stage_model.encode([''], n_samples)\n",
        "\n",
        "\n",
        "  \n",
        "def wpa():\n",
        "  global x_samples\n",
        "  global samples\n",
        "  global iita\n",
        "  torch.set_grad_enabled(False)\n",
        "  \n",
        "\n",
        "  all_samples = []\n",
        "  for iita in range(n_iter):\n",
        "      print(\"iteration: %s\" % (iita + 1))\n",
        "\n",
        "      \n",
        "      predict(models, cond_stage_model, prompt, uc)\n",
        "      \n",
        "  print('Script finished successfully.')\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "if InThread:\n",
        "  t1 = Thread(target = wpa)\n",
        "  a1 = t1.start()\n",
        "else:\n",
        "  wpa()\n"
      ],
      "metadata": {
        "id": "V5xaKk_uYwBK",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}