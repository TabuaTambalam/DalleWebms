{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LDM_SR_jited.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_VZUcnVveXny",
        "115Y_TyxYkbk"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Common\n",
        "Always run this, when start/restart the runtime"
      ],
      "metadata": {
        "id": "_VZUcnVveXny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "from scipy import integrate\n",
        "\n",
        "from tqdm.auto import trange, tqdm\n",
        "\n",
        "def append_dims(x, target_dims):\n",
        "    \"\"\"Appends dimensions to the end of a tensor until it has target_dims dimensions.\"\"\"\n",
        "    dims_to_append = target_dims - x.ndim\n",
        "    if dims_to_append < 0:\n",
        "        raise ValueError(f'input has {x.ndim} dims but target_dims is {target_dims}, which is less')\n",
        "    return x[(...,) + (None,) * dims_to_append]\n",
        "\n",
        "\n",
        "def append_zero(x):\n",
        "    return torch.cat([x, x.new_zeros([1])])\n",
        "\n",
        "\n",
        "def get_sigmas_karras(n, sigma_min, sigma_max, rho=7., device='cpu'):\n",
        "    \"\"\"Constructs the noise schedule of Karras et al. (2022).\"\"\"\n",
        "    ramp = torch.linspace(0, 1, n)\n",
        "    min_inv_rho = sigma_min ** (1 / rho)\n",
        "    max_inv_rho = sigma_max ** (1 / rho)\n",
        "    sigmas = (max_inv_rho + ramp * (min_inv_rho - max_inv_rho)) ** rho\n",
        "    return append_zero(sigmas).to(device)\n",
        "\n",
        "\n",
        "def get_sigmas_exponential(n, sigma_min, sigma_max, device='cpu'):\n",
        "    \"\"\"Constructs an exponential noise schedule.\"\"\"\n",
        "    sigmas = torch.linspace(math.log(sigma_max), math.log(sigma_min), n, device=device).exp()\n",
        "    return append_zero(sigmas)\n",
        "\n",
        "\n",
        "def get_sigmas_vp(n, beta_d=19.9, beta_min=0.1, eps_s=1e-3, device='cpu'):\n",
        "    \"\"\"Constructs a continuous VP noise schedule.\"\"\"\n",
        "    t = torch.linspace(1, eps_s, n, device=device)\n",
        "    sigmas = torch.sqrt(torch.exp(beta_d * t ** 2 / 2 + beta_min * t) - 1)\n",
        "    return append_zero(sigmas)\n",
        "\n",
        "\n",
        "def to_d(x, sigma, denoised):\n",
        "    \"\"\"Converts a denoiser output to a Karras ODE derivative.\"\"\"\n",
        "    return (x - denoised) / append_dims(sigma, x.ndim)\n",
        "\n",
        "\n",
        "def get_ancestral_step(sigma_from, sigma_to):\n",
        "    \"\"\"Calculates the noise level (sigma_down) to step down to and the amount\n",
        "    of noise to add (sigma_up) when doing an ancestral sampling step.\"\"\"\n",
        "    sigma_up = (sigma_to ** 2 * (sigma_from ** 2 - sigma_to ** 2) / sigma_from ** 2) ** 0.5\n",
        "    sigma_down = (sigma_to ** 2 - sigma_up ** 2) ** 0.5\n",
        "    return sigma_down, sigma_up\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_euler(model, x, sigmas, extra_args=None, callback=None, disable=None, s_churn=0., s_tmin=0., s_tmax=float('inf'), s_noise=1.):\n",
        "    \"\"\"Implements Algorithm 2 (Euler steps) from Karras et al. (2022).\"\"\"\n",
        "    extra_args = {} if extra_args is None else extra_args\n",
        "    s_in = x.new_ones([x.shape[0]])\n",
        "    for i in trange(len(sigmas) - 1, disable=disable):\n",
        "        gamma = min(s_churn / (len(sigmas) - 1), 2 ** 0.5 - 1) if s_tmin <= sigmas[i] <= s_tmax else 0.\n",
        "        eps = torch.randn_like(x) * s_noise\n",
        "        sigma_hat = sigmas[i] * (gamma + 1)\n",
        "        if gamma > 0:\n",
        "            x = x + eps * (sigma_hat ** 2 - sigmas[i] ** 2) ** 0.5\n",
        "        denoised = model(x, sigma_hat * s_in, **extra_args)\n",
        "        d = to_d(x, sigma_hat, denoised)\n",
        "        if callback is not None:\n",
        "            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigma_hat, 'denoised': denoised})\n",
        "        dt = sigmas[i + 1] - sigma_hat\n",
        "        # Euler method\n",
        "        x = x + d * dt\n",
        "    return x\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None):\n",
        "    \"\"\"Ancestral sampling with Euler method steps.\"\"\"\n",
        "    extra_args = {} if extra_args is None else extra_args\n",
        "    s_in = x.new_ones([x.shape[0]])\n",
        "    for i in trange(len(sigmas) - 1, disable=disable):\n",
        "        denoised = model(x, sigmas[i] * s_in, **extra_args)\n",
        "        sigma_down, sigma_up = get_ancestral_step(sigmas[i], sigmas[i + 1])\n",
        "        if callback is not None:\n",
        "            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n",
        "        d = to_d(x, sigmas[i], denoised)\n",
        "        # Euler method\n",
        "        dt = sigma_down - sigmas[i]\n",
        "        x = x + d * dt\n",
        "        x = x + torch.randn_like(x) * sigma_up\n",
        "    return x\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_heun(model, x, sigmas, extra_args=None, callback=None, disable=None, s_churn=0., s_tmin=0., s_tmax=float('inf'), s_noise=1.):\n",
        "    \"\"\"Implements Algorithm 2 (Heun steps) from Karras et al. (2022).\"\"\"\n",
        "    extra_args = {} if extra_args is None else extra_args\n",
        "    s_in = x.new_ones([x.shape[0]])\n",
        "    for i in trange(len(sigmas) - 1, disable=disable):\n",
        "        gamma = min(s_churn / (len(sigmas) - 1), 2 ** 0.5 - 1) if s_tmin <= sigmas[i] <= s_tmax else 0.\n",
        "        eps = torch.randn_like(x) * s_noise\n",
        "        sigma_hat = sigmas[i] * (gamma + 1)\n",
        "        if gamma > 0:\n",
        "            x = x + eps * (sigma_hat ** 2 - sigmas[i] ** 2) ** 0.5\n",
        "        denoised = model(x, sigma_hat * s_in, **extra_args)\n",
        "        d = to_d(x, sigma_hat, denoised)\n",
        "        if callback is not None:\n",
        "            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigma_hat, 'denoised': denoised})\n",
        "        dt = sigmas[i + 1] - sigma_hat\n",
        "        if sigmas[i + 1] == 0:\n",
        "            # Euler method\n",
        "            x = x + d * dt\n",
        "        else:\n",
        "            # Heun's method\n",
        "            x_2 = x + d * dt\n",
        "            denoised_2 = model(x_2, sigmas[i + 1] * s_in, **extra_args)\n",
        "            d_2 = to_d(x_2, sigmas[i + 1], denoised_2)\n",
        "            d_prime = (d + d_2) / 2\n",
        "            x = x + d_prime * dt\n",
        "    return x\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_dpm_2(model, x, sigmas, extra_args=None, callback=None, disable=None, s_churn=0., s_tmin=0., s_tmax=float('inf'), s_noise=1.):\n",
        "    \"\"\"A sampler inspired by DPM-Solver-2 and Algorithm 2 from Karras et al. (2022).\"\"\"\n",
        "    extra_args = {} if extra_args is None else extra_args\n",
        "    s_in = x.new_ones([x.shape[0]])\n",
        "    for i in trange(len(sigmas) - 1, disable=disable):\n",
        "        gamma = min(s_churn / (len(sigmas) - 1), 2 ** 0.5 - 1) if s_tmin <= sigmas[i] <= s_tmax else 0.\n",
        "        eps = torch.randn_like(x) * s_noise\n",
        "        sigma_hat = sigmas[i] * (gamma + 1)\n",
        "        if gamma > 0:\n",
        "            x = x + eps * (sigma_hat ** 2 - sigmas[i] ** 2) ** 0.5\n",
        "        denoised = model(x, sigma_hat * s_in, **extra_args)\n",
        "        d = to_d(x, sigma_hat, denoised)\n",
        "        if callback is not None:\n",
        "            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigma_hat, 'denoised': denoised})\n",
        "        # Midpoint method, where the midpoint is chosen according to a rho=3 Karras schedule\n",
        "        sigma_mid = ((sigma_hat ** (1 / 3) + sigmas[i + 1] ** (1 / 3)) / 2) ** 3\n",
        "        dt_1 = sigma_mid - sigma_hat\n",
        "        dt_2 = sigmas[i + 1] - sigma_hat\n",
        "        x_2 = x + d * dt_1\n",
        "        denoised_2 = model(x_2, sigma_mid * s_in, **extra_args)\n",
        "        d_2 = to_d(x_2, sigma_mid, denoised_2)\n",
        "        x = x + d_2 * dt_2\n",
        "    return x\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_dpm_2_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None):\n",
        "    \"\"\"Ancestral sampling with DPM-Solver inspired second-order steps.\"\"\"\n",
        "    extra_args = {} if extra_args is None else extra_args\n",
        "    s_in = x.new_ones([x.shape[0]])\n",
        "    for i in trange(len(sigmas) - 1, disable=disable):\n",
        "        denoised = model(x, sigmas[i] * s_in, **extra_args)\n",
        "        sigma_down, sigma_up = get_ancestral_step(sigmas[i], sigmas[i + 1])\n",
        "        if callback is not None:\n",
        "            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n",
        "        d = to_d(x, sigmas[i], denoised)\n",
        "        # Midpoint method, where the midpoint is chosen according to a rho=3 Karras schedule\n",
        "        sigma_mid = ((sigmas[i] ** (1 / 3) + sigma_down ** (1 / 3)) / 2) ** 3\n",
        "        dt_1 = sigma_mid - sigmas[i]\n",
        "        dt_2 = sigma_down - sigmas[i]\n",
        "        x_2 = x + d * dt_1\n",
        "        denoised_2 = model(x_2, sigma_mid * s_in, **extra_args)\n",
        "        d_2 = to_d(x_2, sigma_mid, denoised_2)\n",
        "        x = x + d_2 * dt_2\n",
        "        x = x + torch.randn_like(x) * sigma_up\n",
        "    return x\n",
        "\n",
        "\n",
        "def linear_multistep_coeff(order, t, i, j):\n",
        "    if order - 1 > i:\n",
        "        raise ValueError(f'Order {order} too high for step {i}')\n",
        "    def fn(tau):\n",
        "        prod = 1.\n",
        "        for k in range(order):\n",
        "            if j == k:\n",
        "                continue\n",
        "            prod *= (tau - t[i - k]) / (t[i - j] - t[i - k])\n",
        "        return prod\n",
        "    return integrate.quad(fn, t[i], t[i + 1], epsrel=1e-4)[0]\n",
        "\n",
        "\n",
        "\n",
        "def revpre0(img,sigmas,t):\n",
        "  return img\n",
        "\n",
        "def revpre1(img,sigmas,t):\n",
        "  return (revpreimg+noise * sigmas[t])*(1-zamask)+img*zamask\n",
        "\n",
        "revpre=revpre0\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_lms(model, x, sigmas, extra_args=None, callback=None, disable=None, order=4):\n",
        "    extra_args = {} if extra_args is None else extra_args\n",
        "    s_in = x.new_ones([x.shape[0]])\n",
        "    ds = []\n",
        "    for i in trange(len(sigmas) - 1, disable=disable):\n",
        "        denoised = model( revpre(x,sigmas,i) , sigmas[i] * s_in, **extra_args)\n",
        "        d = to_d(x, sigmas[i], denoised)\n",
        "        ds.append(d)\n",
        "        if len(ds) > order:\n",
        "            ds.pop(0)\n",
        "        if callback is not None:\n",
        "            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n",
        "        cur_order = min(i + 1, order)\n",
        "        coeffs = [linear_multistep_coeff(cur_order, sigmas.cpu(), i, j) for j in range(cur_order)]\n",
        "        x = x + sum(coeff * d for coeff, d in zip(coeffs, reversed(ds)))\n",
        "    return x\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def log_likelihood(model, x, sigma_min, sigma_max, extra_args=None, atol=1e-4, rtol=1e-4):\n",
        "    extra_args = {} if extra_args is None else extra_args\n",
        "    s_in = x.new_ones([x.shape[0]])\n",
        "    v = torch.randint_like(x, 2) * 2 - 1\n",
        "    fevals = 0\n",
        "    def ode_fn(sigma, x):\n",
        "        nonlocal fevals\n",
        "        with torch.enable_grad():\n",
        "            x = x[0].detach().requires_grad_()\n",
        "            denoised = model(x, sigma * s_in, **extra_args)\n",
        "            d = to_d(x, sigma, denoised)\n",
        "            fevals += 1\n",
        "            grad = torch.autograd.grad((d * v).sum(), x)[0]\n",
        "            d_ll = (v * grad).flatten(1).sum(1)\n",
        "        return d.detach(), d_ll\n",
        "    x_min = x, x.new_zeros([x.shape[0]])\n",
        "    t = x.new_tensor([sigma_min, sigma_max])\n",
        "    sol = odeint(ode_fn, x_min, t, atol=atol, rtol=rtol, method='dopri5')\n",
        "    latent, delta_ll = sol[0][-1], sol[1][-1]\n",
        "    ll_prior = torch.distributions.Normal(0, sigma_max).log_prob(latent).flatten(1).sum(1)\n",
        "    return ll_prior + delta_ll, {'fevals': fevals}\n",
        "\n",
        "\n",
        "\n",
        "class DiscreteSchedule(nn.Module):\n",
        "    \"\"\"A mapping between continuous noise levels (sigmas) and a list of discrete noise\n",
        "    levels.\"\"\"\n",
        "\n",
        "    def __init__(self, sigmas, quantize):\n",
        "        super().__init__()\n",
        "        self.register_buffer('sigmas', sigmas)\n",
        "        self.quantize = quantize\n",
        "\n",
        "    def get_sigmas(self, n=None):\n",
        "        if n is None:\n",
        "            return append_zero(self.sigmas.flip(0))\n",
        "        t_max = len(self.sigmas) - 1\n",
        "        t = torch.linspace(t_max, 0, n, device=self.sigmas.device)\n",
        "        return append_zero(self.t_to_sigma(t))\n",
        "\n",
        "    def sigma_to_t(self, sigma, quantize=None):\n",
        "        quantize = self.quantize if quantize is None else quantize\n",
        "        \n",
        "        dists = torch.abs(sigma - self.sigmas[:, None])\n",
        "        if quantize:\n",
        "            return torch.argmin(dists, dim=0).view(sigma.shape)\n",
        "        low_idx, high_idx = torch.sort(torch.topk(dists, dim=0, k=2, largest=False).indices, dim=0)[0]\n",
        "        low, high = self.sigmas[low_idx], self.sigmas[high_idx]\n",
        "        w = (low - sigma) / (low - high)\n",
        "        w = w.clamp(0, 1)\n",
        "        t = (1 - w) * low_idx + w * high_idx\n",
        "        return t.view(sigma.shape)\n",
        "\n",
        "    def t_to_sigma(self, t):\n",
        "        t = t.float()\n",
        "        low_idx, high_idx, w = t.floor().long(), t.ceil().long(), t.frac()\n",
        "        return (1 - w) * self.sigmas[low_idx] + w * self.sigmas[high_idx]\n",
        "\n",
        "\n",
        "class DiscreteEpsDDPMDenoiser(DiscreteSchedule):\n",
        "    \"\"\"A wrapper for discrete schedule DDPM models that output eps (the predicted\n",
        "    noise).\"\"\"\n",
        "\n",
        "    def __init__(self, model, alphas_cumprod, quantize):\n",
        "        super().__init__(((1 - alphas_cumprod) / alphas_cumprod) ** 0.5, quantize)\n",
        "        self.inner_model = model\n",
        "        self.sigma_data = 1.\n",
        "\n",
        "    def get_scalings(self, sigma):\n",
        "        c_out = -sigma\n",
        "        c_in = 1 / (sigma ** 2 + self.sigma_data ** 2) ** 0.5\n",
        "        return c_out, c_in\n",
        "\n",
        "    def get_eps(self, *args, **kwargs):\n",
        "        return self.inner_model(*args, **kwargs)\n",
        "\n",
        "    def loss(self, input, noise, sigma, **kwargs):\n",
        "        c_out, c_in = [append_dims(x, input.ndim) for x in self.get_scalings(sigma)]\n",
        "        noised_input = input + noise * append_dims(sigma, input.ndim)\n",
        "        eps = self.get_eps(noised_input * c_in, self.sigma_to_t(sigma), **kwargs)\n",
        "        return (eps - noise).pow(2).flatten(1).mean(1)\n",
        "\n",
        "    def forward(self, input, sigma, **kwargs):\n",
        "        c_out, c_in = [append_dims(x, input.ndim) for x in self.get_scalings(sigma)]\n",
        "        eps = self.get_eps(input * c_in, self.sigma_to_t(sigma), **kwargs)\n",
        "        return input + eps * c_out\n",
        "\n",
        "\n",
        "\n",
        "def make_ddim_timesteps(num_ddim_timesteps, num_ddpm_timesteps):\n",
        "    c = num_ddpm_timesteps // num_ddim_timesteps\n",
        "    ddim_timesteps = np.asarray(list(range(0, num_ddpm_timesteps, c)))\n",
        "\n",
        "    # add one to get the final alpha values right (the ones from first scale to data during sampling)\n",
        "    steps_out = ddim_timesteps + 1\n",
        "\n",
        "    return steps_out\n",
        "\n",
        "\n",
        "def make_ddim_sampling_parameters(alphacums, ddim_timesteps, eta):\n",
        "    # select alphas for computing the variance schedule\n",
        "    alphas = alphacums[ddim_timesteps]\n",
        "    alphas_prev = np.asarray([alphacums[0]] + alphacums[ddim_timesteps[:-1]].tolist())\n",
        "\n",
        "    # according the the formula provided in https://arxiv.org/abs/2010.02502\n",
        "    sigmas = eta * np.sqrt((1 - alphas_prev) / (1 - alphas) * (1 - alphas / alphas_prev))\n",
        "\n",
        "    return sigmas, alphas, alphas_prev\n",
        "\n",
        "def makerng():\n",
        "  if seed == 0:\n",
        "    rng=random.randint(0, 2**32)\n",
        "    np.random.seed(rng)\n",
        "    torch.manual_seed(rng)\n",
        "    print('random seed=')\n",
        "    print(rng)\n",
        "  else:\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "class CompVisDenoiser(DiscreteEpsDDPMDenoiser):\n",
        "    \"\"\"A wrapper for CompVis diffusion models.\"\"\"\n",
        "\n",
        "    def __init__(self, model, quantize=False, device='cpu'):\n",
        "        super().__init__(model, model.alphas_cumprod, quantize=quantize)\n",
        "\n",
        "    def get_eps(self, *args, **kwargs):\n",
        "        return self.inner_model.apply_model(*args, **kwargs)\n",
        "\n",
        "\n",
        "class CFGDenoiser(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.inner_model = model\n",
        "\n",
        "    def forward(self, x, sigma, uncond, cond, cond_scale):\n",
        "        x_in = torch.cat([x] * 2)\n",
        "        sigma_in = torch.cat([sigma] * 2)\n",
        "        cond_in = torch.cat([uncond, cond])\n",
        "        uncond, cond = self.inner_model(x_in, sigma_in, cond=cond_in).chunk(2)\n",
        "        return uncond + (cond - uncond) * cond_scale\n",
        "\n",
        "\n",
        "class CompVisJIT():\n",
        "  def __init__(self):\n",
        "    self.alphas_cumprod=torch.tensor(alphas_cumprod,device=cudev)\n",
        "    self.apply_model=apply_model\n"
      ],
      "metadata": {
        "id": "YzmPpU9reaMR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/MultiPromptExample1.txt\n",
        "intp  / 30    // intp: interploation between prompts in 2nd line and 3rd line\n",
        "Elon Musk in swimming pool\n",
        "Joe biden ride on Trump"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yh5Ms_sLOk_f",
        "outputId": "e76ab343-8a6f-4383-c6a1-fb4fdeaef1c7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/MultiPromptExample1.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/MultiPromptExample2.txt\n",
        "avg      // avg: mix prompt with weights, weights should not be negative\n",
        "Photoshot of an elephant\n",
        "8.3\n",
        "Photoshot of an xenomorph\n",
        "2.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PfRzcWAPZl4",
        "outputId": "6fc60edf-4bd0-4d8b-a9eb-aeda6d30dfae"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/MultiPromptExample2.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/MultiPromptExample3.txt\n",
        "mad      // mad: calc prompt with weights, weights can be negative\n",
        "Photoshot of an elephant\n",
        "1.2\n",
        "Photoshot of an xenomorph\n",
        "-0.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NieMhgkP2wf",
        "outputId": "0a72a43f-e12c-43d1-cc3f-ce98d404e94b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/MultiPromptExample3.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Super Resolution 4x<br>\n",
        "Select one of these task: Super Resolution, txt2img, (old ldm)infilling"
      ],
      "metadata": {
        "id": "115Y_TyxYkbk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4vrU_GL_6f3"
      },
      "outputs": [],
      "source": [
        "!wget https://huggingface.co/Larvik/LDMjit/resolve/main/dm_pnnx.pt\n",
        "!wget https://huggingface.co/Larvik/LDMjit/resolve/main/fsd_pnnx.pt\n",
        "!wget https://huggingface.co/Larvik/LDMjit/resolve/main/alphas_cumprod.npy\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import functools\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "alphas_cumprod = np.load('alphas_cumprod.npy')\n",
        "\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "torch.set_num_threads(os.cpu_count())\n",
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True\n",
        "\n",
        "\n",
        "# ======================\n",
        "# Arguemnt Parser Config\n",
        "# ======================\n",
        "\n",
        "def imread(filename, flags=cv2.IMREAD_COLOR):\n",
        "    if not os.path.isfile(filename):\n",
        "        print(f\"File does not exist: {filename}\")\n",
        "        sys.exit()\n",
        "    data = np.fromfile(filename, np.int8)\n",
        "    img = cv2.imdecode(data, flags)\n",
        "    return img\n",
        "\n",
        "def preprocessing_img(img):\n",
        "    if len(img.shape) < 3:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGRA)\n",
        "    elif img.shape[2] == 3:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2BGRA)\n",
        "    elif img.shape[2] == 1:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGRA)\n",
        "    return img\n",
        "\n",
        "\n",
        "def load_image(image_path):\n",
        "    if os.path.isfile(image_path):\n",
        "        img = imread(image_path, cv2.IMREAD_UNCHANGED)\n",
        "    else:\n",
        "        print(f'{image_path} not found.')\n",
        "    return preprocessing_img(img)\n",
        "\n",
        "\n",
        "\n",
        "def im2col(images, filters, stride=1, pad=0):\n",
        "    if images.ndim == 2:\n",
        "        images = images.reshape(1, 1, *images.shape)\n",
        "    elif images.ndim == 3:\n",
        "        B, I_h, I_w = images.shape\n",
        "        images = images.reshape(B, 1, I_h, I_w)\n",
        "    B, C, I_h, I_w = images.shape\n",
        "\n",
        "    if isinstance(filters, tuple):\n",
        "        if len(filters) == 2:\n",
        "            filters = (1, 1, *filters)\n",
        "        elif len(filters) == 3:\n",
        "            M, F_h, F_w = filters\n",
        "            filters = (M, 1, F_h, F_w)\n",
        "        _, _, F_h, F_w = filters\n",
        "    else:\n",
        "        if filters.ndim == 2:\n",
        "            filters = filters.reshape(1, 1, *filters.shape)\n",
        "        elif filters.ndim == 3:\n",
        "            M, F_h, F_w = filters.shape\n",
        "            filters = filters.reshape(M, 1, F_h, F_w)\n",
        "        _, _, F_h, F_w = filters.shape\n",
        "\n",
        "    if isinstance(stride, tuple):\n",
        "        stride_ud, stride_lr = stride\n",
        "    else:\n",
        "        stride_ud = stride\n",
        "        stride_lr = stride\n",
        "    if isinstance(pad, tuple):\n",
        "        pad_ud, pad_lr = pad\n",
        "    elif isinstance(pad, int):\n",
        "        pad_ud = pad\n",
        "        pad_lr = pad\n",
        "    elif pad == \"same\":\n",
        "        pad_ud = 0.5 * ((I_h - 1) * stride_ud - I_h + F_h)\n",
        "        pad_lr = 0.5 * ((I_w - 1) * stride_lr - I_w + F_w)\n",
        "    pad_zero = (0, 0)\n",
        "\n",
        "    O_h = int((I_h - F_h + 2 * pad_ud) // stride_ud + 1)\n",
        "    O_w = int((I_w - F_w + 2 * pad_lr) // stride_lr + 1)\n",
        "\n",
        "    result_pad = (pad_ud, pad_lr)\n",
        "    pad_ud = int(np.ceil(pad_ud))\n",
        "    pad_lr = int(np.ceil(pad_lr))\n",
        "    pad_ud = (pad_ud, pad_ud)\n",
        "    pad_lr = (pad_lr, pad_lr)\n",
        "    images = np.pad(\n",
        "        images, [pad_zero, pad_zero, pad_ud, pad_lr], \"constant\")\n",
        "\n",
        "    cols = np.empty((B, C, F_h, F_w, O_h, O_w))\n",
        "    for h in range(F_h):\n",
        "        h_lim = h + stride_ud * O_h\n",
        "        for w in range(F_w):\n",
        "            w_lim = w + stride_lr * O_w\n",
        "            cols[:, :, h, w, :, :] = \\\n",
        "                images[:, :, h:h_lim:stride_ud, w:w_lim:stride_lr]\n",
        "\n",
        "    cols = cols.transpose(1, 2, 3, 0, 4, 5).reshape(C * F_h * F_w, B * O_h * O_w)\n",
        "\n",
        "    return cols, (O_h, O_w), result_pad\n",
        "\n",
        "\n",
        "def col2im(cols, I_shape, O_shape, stride=1, pad=0):\n",
        "    def get_f_shape(i, o, s, p):\n",
        "        return int(i + 2 * p - (o - 1) * s)\n",
        "\n",
        "    if len(I_shape) == 2:\n",
        "        B = C = 1\n",
        "        I_h, I_w = I_shape\n",
        "    elif len(I_shape) == 3:\n",
        "        C = 1\n",
        "        B, I_h, I_w = I_shape\n",
        "    else:\n",
        "        B, C, I_h, I_w = I_shape\n",
        "    O_h, O_w = O_shape\n",
        "\n",
        "    if isinstance(stride, tuple):\n",
        "        stride_ud, stride_lr = stride\n",
        "    else:\n",
        "        stride_ud = stride\n",
        "        stride_lr = stride\n",
        "    if isinstance(pad, tuple):\n",
        "        pad_ud, pad_lr = pad\n",
        "    elif isinstance(pad, int):\n",
        "        pad_ud = pad\n",
        "        pad_lr = pad\n",
        "\n",
        "    F_h = get_f_shape(I_h, O_h, stride_ud, pad_ud)\n",
        "    F_w = get_f_shape(I_w, O_w, stride_lr, pad_lr)\n",
        "    pad_ud = int(np.ceil(pad_ud))\n",
        "    pad_lr = int(np.ceil(pad_lr))\n",
        "    cols = cols.reshape(C, F_h, F_w, B, O_h, O_w).transpose(3, 0, 1, 2, 4, 5)\n",
        "    images = np.zeros((B, C, I_h + 2 * pad_ud + stride_ud - 1, I_w + 2 * pad_lr + stride_lr - 1))\n",
        "\n",
        "    for h in range(F_h):\n",
        "        h_lim = h + stride_ud * O_h\n",
        "        for w in range(F_w):\n",
        "            w_lim = w + stride_lr * O_w\n",
        "            images[:, :, h:h_lim:stride_ud, w:w_lim:stride_lr] += cols[:, :, h, w, :, :]\n",
        "\n",
        "    return images[:, :, pad_ud: I_h + pad_ud, pad_lr: I_w + pad_lr]\n",
        "\n",
        "def meshgrid(h, w):\n",
        "    y = np.arange(0, h).reshape(h, 1, 1).repeat(w, axis=1)\n",
        "    x = np.arange(0, w).reshape(1, w, 1).repeat(h, axis=0)\n",
        "    arr = np.concatenate([y, x], axis=-1)\n",
        "\n",
        "    return arr\n",
        "\n",
        "\n",
        "def delta_border(h, w):\n",
        "    \"\"\"\n",
        "    :param h: height\n",
        "    :param w: width\n",
        "    :return: normalized distance to image border,\n",
        "     wtith min distance = 0 at border and max dist = 0.5 at image center\n",
        "    \"\"\"\n",
        "    lower_right_corner = np.array([h - 1, w - 1]).reshape(1, 1, 2)\n",
        "    arr = meshgrid(h, w) / lower_right_corner\n",
        "    dist_left_up = np.min(arr, axis=-1, keepdims=True)\n",
        "    dist_right_down = np.min(1 - arr, axis=-1, keepdims=True)\n",
        "\n",
        "    edge_dist = np.min(np.concatenate([dist_left_up, dist_right_down], axis=-1), axis=-1)\n",
        "\n",
        "    return edge_dist\n",
        "\n",
        "\n",
        "def get_weighting(h, w, Ly, Lx):\n",
        "    clip_min_weight = 0.01\n",
        "    clip_max_weight = 0.5\n",
        "\n",
        "    weighting = delta_border(h, w)\n",
        "    weighting = np.clip(weighting, clip_min_weight, clip_max_weight)\n",
        "    weighting = weighting.reshape(1, h * w, 1).repeat(Ly * Lx, axis=-1)\n",
        "\n",
        "    return weighting\n",
        "\n",
        "\n",
        "def get_fold_unfold(x, kernel_size, stride, uf=1, df=1):\n",
        "    \"\"\"\n",
        "    :param x: img of size (bs, c, h, w)\n",
        "    :return: n img crops of size (n, bs, c, kernel_size[0], kernel_size[1])\n",
        "    \"\"\"\n",
        "    bs, nc, h, w = x.shape\n",
        "\n",
        "    # number of crops in image\n",
        "    Ly = (h - kernel_size[0]) // stride[0] + 1\n",
        "    Lx = (w - kernel_size[1]) // stride[1] + 1\n",
        "\n",
        "    unfold = functools.partial(im2col, filters=kernel_size, stride=stride)\n",
        "    if uf == 1 and df == 1:\n",
        "        fold = functools.partial(\n",
        "            col2im,\n",
        "            stride=stride)\n",
        "\n",
        "        weighting = get_weighting(kernel_size[0], kernel_size[1], Ly, Lx)\n",
        "        weighting = weighting.reshape((1, 1, kernel_size[0], kernel_size[1], Ly * Lx))\n",
        "\n",
        "    elif uf > 1 and df == 1:\n",
        "        fold = functools.partial(\n",
        "            col2im,\n",
        "            stride=(stride[0] * uf, stride[1] * uf))\n",
        "\n",
        "        weighting = get_weighting(kernel_size[0] * uf, kernel_size[1] * uf, Ly, Lx)\n",
        "        weighting = weighting.reshape((1, 1, kernel_size[0] * uf, kernel_size[1] * uf, Ly * Lx))\n",
        "\n",
        "    elif df > 1 and uf == 1:\n",
        "        fold = functools.partial(\n",
        "            col2im,\n",
        "            stride=(stride[0] // df, stride[1] // df))\n",
        "\n",
        "        weighting = get_weighting(kernel_size[0] // df, kernel_size[1] // df, Ly, Lx)\n",
        "        weighting = weighting.reshape((1, 1, kernel_size[0] // df, kernel_size[1] // df, Ly * Lx))\n",
        "\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    return fold, unfold, weighting\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def normalize_image(image, normalize_type='255'):\n",
        "    \"\"\"\n",
        "    Normalize image\n",
        "    Parameters\n",
        "    ----------\n",
        "    image: numpy array\n",
        "        The image you want to normalize\n",
        "    normalize_type: string\n",
        "        Normalize type should be chosen from the type below.\n",
        "        - '255': simply dividing by 255.0\n",
        "        - '127.5': output range : -1 and 1\n",
        "        - 'ImageNet': normalize by mean and std of ImageNet\n",
        "        - 'None': no normalization\n",
        "    Returns\n",
        "    -------\n",
        "    normalized_image: numpy array\n",
        "    \"\"\"\n",
        "    if normalize_type == 'None':\n",
        "        return image\n",
        "    elif normalize_type == '255':\n",
        "        return image / 255.0\n",
        "    elif normalize_type == '127.5':\n",
        "        return image / 127.5 - 1.0\n",
        "    elif normalize_type == 'ImageNet':\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        image = image / 255.0\n",
        "        for i in range(3):\n",
        "            image[:, :, i] = (image[:, :, i] - mean[i]) / std[i]\n",
        "        return image\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "\n",
        "\n",
        "def preprocess(img):\n",
        "    im_h, im_w, _ = img.shape\n",
        "\n",
        "    up_f = 4\n",
        "    oh, ow = up_f * im_h, up_f * im_w\n",
        "\n",
        "    img = normalize_image(img, normalize_type='255')\n",
        "\n",
        "    c = img * 2 - 1\n",
        "    c = c.transpose(2, 0, 1)  # HWC -> CHW\n",
        "    c = np.expand_dims(c, axis=0)\n",
        "    c = c.astype(np.float32)\n",
        "\n",
        "    c_up = cv2.resize(img, (ow, oh), interpolation=cv2.INTER_LINEAR)\n",
        "    c_up = c_up.transpose(2, 0, 1)  # HWC -> CHW\n",
        "    c_up = np.expand_dims(c_up, axis=0)\n",
        "    c_up = c_up.astype(np.float32)\n",
        "\n",
        "    return c_up, c\n",
        "\n",
        "\n",
        "def postprocess(sample):\n",
        "    sample = np.clip(sample, -1., 1.)\n",
        "    sample = (sample + 1.) / 2. * 255\n",
        "    sample = np.transpose(sample, (1, 2, 0))\n",
        "    sample = sample[:, :, ::-1]  # RGB -> BGR\n",
        "    sample = sample.astype(np.uint8)\n",
        "\n",
        "    return sample\n",
        "\n",
        "\n",
        "def ddim_sampling(\n",
        "        models, cond):\n",
        "    shape = cond.shape\n",
        "    img = np.random.randn(shape[0] * shape[1] * shape[2] * shape[3]).reshape(shape)\n",
        "    img = img.astype(np.float32)\n",
        "\n",
        "    timesteps = ddim_timesteps\n",
        "    time_range = np.flip(timesteps)\n",
        "    total_steps = timesteps.shape[0]\n",
        "\n",
        "    print(f\"Running DDIM Sampling with {total_steps} timesteps\")\n",
        "\n",
        "    try:\n",
        "        from tqdm import tqdm\n",
        "        iterator = tqdm(time_range, desc='DDIM Sampler', total=total_steps)\n",
        "    except ModuleNotFoundError:\n",
        "        def iter_func(a):\n",
        "            for i, x in enumerate(a):\n",
        "                print(\"DDIM Sampler: %s/%s\" % (i + 1, len(a)))\n",
        "                yield x\n",
        "\n",
        "        iterator = iter_func(time_range)\n",
        "\n",
        "    for i, step in enumerate(iterator):\n",
        "        index = total_steps - i - 1\n",
        "        ts = np.full((shape[0],), step, dtype=np.int64)\n",
        "\n",
        "        img, pred_x0 = p_sample_ddim(\n",
        "            models,\n",
        "            img, cond, ts,\n",
        "            index=index,\n",
        "        )\n",
        "        img = img.astype(np.float32)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "# ddim\n",
        "def p_sample_ddim(\n",
        "        models, x, c, t, index,\n",
        "        temperature=1):\n",
        "    e_t = apply_model(models, x, t, c)\n",
        "\n",
        "    alphas = ddim_alphas\n",
        "    alphas_prev = ddim_alphas_prev\n",
        "    sqrt_one_minus_alphas = ddim_sqrt_one_minus_alphas\n",
        "    sigmas = ddim_sigmas\n",
        "\n",
        "    # select parameters corresponding to the currently considered timestep\n",
        "    b, *_ = x.shape\n",
        "    a_t = np.full((b, 1, 1, 1), alphas[index])\n",
        "    a_prev = np.full((b, 1, 1, 1), alphas_prev[index])\n",
        "    sigma_t = np.full((b, 1, 1, 1), sigmas[index])\n",
        "    sqrt_one_minus_at = np.full((b, 1, 1, 1), sqrt_one_minus_alphas[index])\n",
        "\n",
        "    # current prediction for x_0\n",
        "    pred_x0 = (x - sqrt_one_minus_at * e_t) / np.sqrt(a_t)\n",
        "\n",
        "    # direction pointing to x_t\n",
        "    dir_xt = np.sqrt(1. - a_prev - sigma_t ** 2) * e_t\n",
        "\n",
        "    noise = sigma_t * np.random.randn(x.size).reshape(x.shape) * temperature\n",
        "    x_prev = np.sqrt(a_prev) * pred_x0 + dir_xt + noise\n",
        "\n",
        "    return x_prev, pred_x0\n",
        "\n",
        "\n",
        "def decode_first_stage(models, z):\n",
        "    ks = (128, 128)\n",
        "    stride = (64, 64)\n",
        "    uf = 4\n",
        "\n",
        "    bs, nc, h, w = z.shape\n",
        "\n",
        "    fold, unfold, weighting = get_fold_unfold(z, ks, stride, uf=uf)\n",
        "\n",
        "    z, o_shape, _ = unfold(z)  # (bn, nc * prod(**ks), L)\n",
        "\n",
        "    # Reshape to img shape\n",
        "    z = z.reshape((bs, -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n",
        "    z = z.astype(np.float32)\n",
        "\n",
        "    print('first_stage_decode...')\n",
        "\n",
        "    first_stage_decode = models['first_stage_decode']\n",
        "    outputs = []\n",
        "    for i in range(z.shape[-1]):\n",
        "        x = z[:, :, :, :, i]\n",
        "        output = first_stage_decode(torch.tensor(x).cuda())\n",
        "        outputs.append(output[0].cpu().numpy())\n",
        "\n",
        "    o = np.stack(outputs, axis=-1)  # # (bn, nc, ks[0], ks[1], L)\n",
        "    o = o * weighting\n",
        "\n",
        "    # Reverse reshape to img shape\n",
        "    o = o.reshape((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n",
        "    # stitch crops together\n",
        "    decoded = fold(o, I_shape=(1, 3, h * uf, w * uf), O_shape=o_shape)\n",
        "\n",
        "    normalization = fold(weighting, I_shape=(1, 1, h * uf, w * uf), O_shape=o_shape)\n",
        "    decoded = decoded / normalization  # norm is shape (1, 1, h, w)\n",
        "\n",
        "    return decoded\n",
        "\n",
        "\n",
        "# ddpm\n",
        "def apply_model(models, x_noisy, t, cond):\n",
        "    ks = (128, 128)\n",
        "    stride = (64, 64)\n",
        "\n",
        "    bs, nc, h, w = x_noisy.shape\n",
        "\n",
        "    fold, unfold, weighting = get_fold_unfold(x_noisy, ks, stride)\n",
        "\n",
        "    z, o_shape, _ = unfold(x_noisy)  # (bn, nc * prod(**ks), L)\n",
        "    # Reshape to img shape\n",
        "    z = z.reshape((bs, -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n",
        "    z_list = [z[:, :, :, :, i] for i in range(z.shape[-1])]\n",
        "\n",
        "    c, *_ = unfold(cond)\n",
        "    c = c.reshape((bs, -1, ks[0], ks[1], c.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n",
        "    cond_list = [c[:, :, :, :, i] for i in range(c.shape[-1])]\n",
        "\n",
        "    # apply model by loop over crops\n",
        "    diffusion_model = models[\"diffusion_model\"]\n",
        "    outputs = []\n",
        "    for i in range(z.shape[-1]):\n",
        "        x = z_list[i]\n",
        "        cond = cond_list[i]\n",
        "        xc = np.concatenate([x, cond], axis=1)\n",
        "        xc = xc.astype(np.float32)\n",
        "        \n",
        "        output = diffusion_model(torch.tensor(xc).cuda(), torch.tensor(t).cuda())\n",
        "            \n",
        "       \n",
        "        outputs.append(output[0].cpu().numpy())\n",
        "\n",
        "    o = np.stack(outputs, axis=-1)\n",
        "    o = o * weighting\n",
        "\n",
        "    # Reverse reshape to img shape\n",
        "    o = o.reshape((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n",
        "    # stitch crops together\n",
        "    normalization = fold(weighting, I_shape=(1, 1, h, w), O_shape=o_shape)\n",
        "    x_recon = fold(o, I_shape=(1, 3, h, w), O_shape=o_shape) / normalization\n",
        "\n",
        "    return x_recon\n",
        "\n",
        "\n",
        "def predict(models, img):\n",
        "    img = img[:, :, ::-1]  # BGR -> RGB\n",
        "\n",
        "    _, c = preprocess(img)\n",
        "\n",
        "    samples = ddim_sampling(models, c)\n",
        "\n",
        "    x_sample = decode_first_stage(models, samples)\n",
        "\n",
        "    img = postprocess(x_sample[0])\n",
        "\n",
        "    return img\n",
        "\n",
        "models = dict(\n",
        "    first_stage_decode=torch.jit.load('/content/fsd_pnnx.pt').eval().cuda(),\n",
        "    diffusion_model=torch.jit.load('/content/dm_pnnx.pt').eval().cuda(),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "ddim_timesteps\n",
        "\"\"\"\n",
        "ddim_eta = 1.0\n",
        "ddim_num_steps = 100\n",
        "ddpm_num_timesteps = 1000\n",
        "ddim_timesteps = make_ddim_timesteps(ddim_num_steps, ddpm_num_timesteps)\n",
        "\n",
        "\"\"\"\n",
        "ddim sampling parameters\n",
        "\"\"\"\n",
        "\n",
        "ddim_sigmas, ddim_alphas, ddim_alphas_prev = \\\n",
        "    make_ddim_sampling_parameters(\n",
        "        alphacums=alphas_cumprod,\n",
        "        ddim_timesteps=ddim_timesteps,\n",
        "        eta=ddim_eta)\n",
        "\n",
        "ddim_sqrt_one_minus_alphas = np.sqrt(1. - ddim_alphas)\n",
        "\n",
        "inputz=['/content/sample_data/zkrp.jpg']\n",
        "\n",
        " \n",
        "\n",
        "for image_path in inputz:\n",
        "    print(image_path)\n",
        "\n",
        "    # prepare input data\n",
        "    img = load_image(image_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
        "\n",
        "    # inference\n",
        "    print('Start inference...')\n",
        "    \n",
        "    img = predict(models, img)\n",
        "\n",
        "    # plot result\n",
        "    savepath = '/content/sample_data/out.png'\n",
        "    print(f'saved at : {savepath}')\n",
        "    cv2.imwrite(savepath, img)\n",
        "\n",
        "print('Script finished successfully.')"
      ],
      "metadata": {
        "id": "xrSkT1_5AJSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# txt2img"
      ],
      "metadata": {
        "id": "PWCzxzNkYpEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jit=False #@param {type:'boolean'}\n",
        "\n",
        "import os\n",
        "\n",
        "if not os.path.isfile('autoencoder_pnnx.pt'):\n",
        "  !pip install transformers\n",
        "  !wget https://huggingface.co/Larvik/tempsd/resolve/main/alphas_cumprod.npz\n",
        "  !wget https://huggingface.co/Larvik/temp1/resolve/main/transformer_pnnx.pt\n",
        "  !wget https://huggingface.co/Larvik/temp1/resolve/main/diffusion_emb_pnnx.pt\n",
        "  !wget https://huggingface.co/Larvik/temp1/resolve/main/diffusion_mid_pnnx.pt\n",
        "  !wget https://huggingface.co/Larvik/temp1/resolve/main/diffusion_out_pnnx.pt\n",
        "  !wget https://huggingface.co/Larvik/temp1/resolve/main/autoencoder_pnnx.pt\n",
        "  !wget https://huggingface.co/Larvik/temp1/resolve/main/imgencoder_pnnx.pt\n",
        "\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "from threading import Thread\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import PIL\n",
        "from transformers import CLIPTokenizer\n",
        "\n",
        "import torch\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "torch.set_num_threads(os.cpu_count())\n",
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True\n",
        "\n",
        "alphas_cumprod = np.load('alphas_cumprod.npz')['a']\n",
        "\n",
        "cudev=torch.device('cuda')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# encoder\n",
        "class BERTEmbedder:\n",
        "    \"\"\" Uses a pretrained BERT tokenizer by huggingface. Vocab size: 30522 (?)\"\"\"\n",
        "\n",
        "    def __init__(self, transformer, max_length=77):\n",
        "        self.tokenizer = CLIPTokenizer.from_pretrained('openai/clip-vit-large-patch14')\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.transformer = transformer\n",
        "        \n",
        "\n",
        "    def encode(self, text, nsamp):\n",
        "        batch_encoding = self.tokenizer(text, truncation=True, max_length=self.max_length, return_length=True,\n",
        "                            return_overflowing_tokens=False, padding=\"max_length\", return_tensors=\"pt\")\n",
        "\n",
        "        tokens = batch_encoding[\"input_ids\"]\n",
        "        tokens = torch.tensor(tokens.numpy()).expand(nsamp,-1)\n",
        "        \n",
        "\n",
        "       \n",
        "        z = self.transformer(tokens)\n",
        "        \n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "        return z.cuda()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "preimg=None\n",
        "revpreimg=None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ddpm\n",
        "def apply_model(x, t, cond):\n",
        "\n",
        "    h, emb, hs = diffusion_emb(x, t, cond)\n",
        "    \n",
        "    h = diffusion_mid(h, emb, cond, *hs[6:])\n",
        "\n",
        "    output = diffusion_out(h, emb, cond, *hs[:6])\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "# decoder\n",
        "def decode_first_stage(z):\n",
        "\n",
        "    output = autoencoder(z/0.18215)\n",
        "       \n",
        "    return output\n",
        "\n",
        "\n",
        "    \n",
        "def load_img(path):\n",
        "    image = Image.open(path).convert(\"RGB\")\n",
        "    w, h = image.size\n",
        "    print(f\"loaded input image of size ({w}, {h}) from {path}\")\n",
        "    w2, h2 = map(lambda x: x - x % 32, (w, h))\n",
        "    if w!=w2 or h!=h2:\n",
        "      image = image.resize((w2, h2), resample=PIL.Image.LANCZOS)\n",
        "    image = np.array(image).astype(np.float32) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    return 2.*image - 1.\n",
        "\n",
        "\n",
        "def intptxtemb(t1,t2,step):\n",
        "  stp=step-1\n",
        "  c1=cond_stage_model.encode(t1, n_samples)\n",
        "  c2=cond_stage_model.encode(t2, n_samples)\n",
        "  intpos=[]\n",
        "  for i in range(step):\n",
        "    intpos.append((c2*i+c1*(stp-i))/stp)\n",
        "  return intpos\n",
        "\n",
        "def makeCs(prmt):\n",
        "  if prmt.endswith('.txt'):\n",
        "    with open(prmt,'rt') as f:\n",
        "      stz=f.read().splitlines()\n",
        "    cmd=stz[0].replace(' ','').replace('\\t','').split('/')\n",
        "    cmd0=cmd[0]\n",
        "    if cmd0.startswith('intp'):\n",
        "      return intptxtemb(stz[1],stz[2],int(cmd[1]))\n",
        "\n",
        "    prmpl=(len(stz)-1)>>1\n",
        "    stz=stz[1:]\n",
        "    ptxt=[]\n",
        "    pwgt=[]\n",
        "    wgtsum=0\n",
        "    for i in range(prmpl):\n",
        "      ptxt.append(  cond_stage_model.encode(stz[2*i])  )\n",
        "      wgt=float(stz[2*i+1])\n",
        "      wgtsum+=wgt\n",
        "      pwgt.append(  wgt  )\n",
        "    if cmd0.startswith('avg'):\n",
        "      for i in range(prmpl):\n",
        "        pwgt[i]=pwgt[i]/wgtsum\n",
        "    \n",
        "    cout=ptxt[0]*pwgt[0]\n",
        "    for i in range(1,prmpl):\n",
        "      cout+=(ptxt[i]*pwgt[i])\n",
        "    return [cout]\n",
        "  else:\n",
        "    return [cond_stage_model.encode(prmt, n_samples)]\n",
        "\n",
        "\n",
        "\n",
        "fext='_%dx%dv%d.png'\n",
        "def saver():\n",
        "  global x_samples\n",
        "  i=iita\n",
        "  np.save( (outputp+fext%(i,1,ktta))[:-4] + '.npy', samples)\n",
        "  x_samples = np.clip((x_samples.numpy() + 1.0) / 2.0, a_min=0.0, a_max=1.0)\n",
        "  k=0\n",
        "  for x_sample in x_samples:\n",
        "      x_sample = x_sample.transpose(1, 2, 0)  # CHW -> HWC\n",
        "      x_sample = x_sample * 255\n",
        "      img = x_sample.astype(np.uint8)\n",
        "      img = img[:, :, ::-1]  # RGB -> BGR\n",
        "      cv2.imwrite(outputp+fext%(i,k,ktta), img)\n",
        "      k+=1\n",
        "  \n",
        "\n",
        "UseSamplr=sample_lms\n",
        "\n",
        "def predict(prompt, uc):\n",
        "    global x_samples\n",
        "    global samples\n",
        "    global ktta\n",
        "    global noise\n",
        "    \n",
        "\n",
        "    c_list = makeCs(prompt)\n",
        "    \n",
        "    \n",
        "    sigmas = model_wrap.get_sigmas(ddim_num_steps)\n",
        "    noise = torch.randn(shape, dtype=torch.float,device=cudev)\n",
        "    if preimg is not None:\n",
        "      t_enc= int(strength * ddim_num_steps)\n",
        "      img = preimg.cuda() + noise * sigmas[ddim_num_steps - t_enc - 1] \n",
        "      sigma_sched = sigmas[ddim_num_steps - t_enc - 1:]\n",
        "    else:\n",
        "      img = noise*sigmas[0]\n",
        "      sigma_sched=sigmas\n",
        "\n",
        "    ktta=0\n",
        "    for c in c_list:\n",
        "      extra_args = {'cond': c, 'uncond': uc, 'cond_scale': cfg_scale}\n",
        "      with torch.cuda.amp.autocast(dtype=torch.float16):\n",
        "        samples = UseSamplr(model_wrap_cfg, img, sigma_sched, extra_args=extra_args, disable=False)\n",
        "      ktta+=1\n",
        "      \n",
        "      \n",
        "      x_samples = decode_first_stage(  samples ).cpu()\n",
        "      samples=samples.cpu()\n",
        "      t3 = Thread(target = saver)\n",
        "      a3 = t3.start()\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "def init_img_type():\n",
        "  global init_img\n",
        "  if init_img.endswith('npy'):\n",
        "    return 0\n",
        "  elif init_img.endswith('jpg') or init_img.endswith('png'):\n",
        "    if os.path.isfile(init_img+'.npy'):\n",
        "      init_img+='.npy'\n",
        "      return 0\n",
        "    else:\n",
        "      return 1\n",
        "  else:\n",
        "    return 99\n",
        "\n",
        "\n",
        "\n",
        "cond_stage_model = BERTEmbedder(torch.jit.load('transformer_pnnx.pt').eval())\n",
        "diffusion_emb = torch.jit.load('diffusion_emb_pnnx.pt').eval().half().cuda()\n",
        "diffusion_mid = torch.jit.load('diffusion_mid_pnnx.pt').eval().half().cuda()\n",
        "diffusion_out = torch.jit.load('diffusion_out_pnnx.pt').eval().half().cuda()\n",
        "autoencoder = torch.jit.load('autoencoder_pnnx.pt').eval().cuda()\n",
        "imgenc = torch.jit.load('imgencoder_pnnx.pt').eval()\n",
        "\n",
        "\n",
        "model_wrap = CompVisDenoiser(CompVisJIT())\n",
        "model_wrap_cfg = CFGDenoiser(model_wrap)"
      ],
      "metadata": {
        "id": "fmYgI8PLYudK",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional"
      ],
      "metadata": {
        "id": "E4dG4ZxdW4y-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "sample_euler\n",
        "sample_euler_ancestral\n",
        "sample_heun\n",
        "sample_dpm_2\n",
        "sample_dpm_2_ancestral\n",
        "sample_lms\n",
        "'''\n",
        "UseSamplr = sample_heun\n"
      ],
      "metadata": {
        "id": "fQlZQgl7uxkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_img='xxx' #@param {type:'string'}\n",
        "initymgtyp=init_img_type()\n",
        "if initymgtyp == 0:\n",
        "  preimg=torch.tensor(np.load(init_img), device='cpu')\n",
        "  n_samples=preimg.size(0)\n",
        "  H=preimg.size(2)<<3\n",
        "  W=preimg.size(3)<<3\n",
        "elif initymgtyp == 1:\n",
        "  n_samples=1\n",
        "  rpt=load_img(init_img)\n",
        "  H=rpt.size(2)\n",
        "  W=rpt.size(3)\n",
        "  preimg=imgenc(  rpt, torch.randn(torch.Size([n_samples,4,H>>3,W>>3]))  )*0.18215\n",
        "  np.save(init_img+'.npy',preimg.numpy())\n",
        "else:\n",
        "  preimg=None"
      ],
      "metadata": {
        "id": "I1FtIJmB2Ojd",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "infilling"
      ],
      "metadata": {
        "id": "G2ANdLCfXTj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zamask=np.load('bench2_mask.npy')\n",
        "revpreimg=preimg\n",
        "preimg=None\n",
        "revpre=revpre1"
      ],
      "metadata": {
        "id": "Rkrl24HWW31Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional"
      ],
      "metadata": {
        "id": "QunwpFSGXKz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "InThread=False #@param {type:'boolean'}\n",
        "\n",
        "prompt = 'a photograph of an astronaut riding a horse' #@param {type:'string'}\n",
        "\n",
        "n_iter = 1 #@param {type:'integer'}\n",
        "if preimg is None and revpreimg is None:\n",
        "  n_samples = 1 #@param {type:'integer'}\n",
        "  H=704 #@param {type:'integer'}\n",
        "  W=768 #@param {type:'integer'}\n",
        "\n",
        "\n",
        "\n",
        "ddim_num_steps = 50  #@param {type:'integer'}\n",
        "ddpm_num_timesteps = 1000\n",
        "\n",
        "seed=0 #@param {type:'integer'}\n",
        "\n",
        "outputp='/content/sample_data' #@param {type:'string'}\n",
        "\n",
        "\n",
        "\n",
        "strength=0.5 #@param {type:'number'}\n",
        "\n",
        "\n",
        "\n",
        "cfg_scale = 7.5 #@param {type:'number'}\n",
        "ddim_eta = 0  #@param {type:'integer'}\n",
        "\n",
        "\n",
        "outputp=outputp+'/'+str(len(os.listdir(outputp)))\n",
        "\"\"\"\n",
        "ddim_timesteps\n",
        "\"\"\"\n",
        "\n",
        "ddim_timesteps = make_ddim_timesteps(\n",
        "    ddim_num_steps, ddpm_num_timesteps)\n",
        "\n",
        "\"\"\"\n",
        "ddim sampling parameters\n",
        "\"\"\"\n",
        "\n",
        "ddim_sigmas, ddim_alphas, ddim_alphas_prev = \\\n",
        "    make_ddim_sampling_parameters(\n",
        "        alphacums=alphas_cumprod,\n",
        "        ddim_timesteps=ddim_timesteps,\n",
        "        eta=ddim_eta)\n",
        "\n",
        "ddim_sqrt_one_minus_alphas = np.sqrt(1. - ddim_alphas)\n",
        "\n",
        "shape = [n_samples, 4, H>>3 , W>>3 ]\n",
        "\n",
        "\n",
        "makerng()\n",
        "\n",
        "\n",
        "print(\"prompt: %s\" % prompt)\n",
        "\n",
        "\n",
        "\n",
        "print('Start inference...')\n",
        "uc = None\n",
        "if cfg_scale != 1.0:\n",
        "  uc = cond_stage_model.encode([''], n_samples)\n",
        "\n",
        "\n",
        "  \n",
        "def wpa():\n",
        "  global x_samples\n",
        "  global samples\n",
        "  global iita\n",
        "  torch.set_grad_enabled(False)\n",
        "  \n",
        "\n",
        "  all_samples = []\n",
        "  for iita in range(n_iter):\n",
        "      print(\"iteration: %s\" % (iita + 1))\n",
        "\n",
        "      \n",
        "      predict(prompt, uc)\n",
        "      \n",
        "  print('Script finished successfully.')\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "if InThread:\n",
        "  t1 = Thread(target = wpa)\n",
        "  a1 = t1.start()\n",
        "else:\n",
        "  wpa()\n"
      ],
      "metadata": {
        "id": "V5xaKk_uYwBK",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}