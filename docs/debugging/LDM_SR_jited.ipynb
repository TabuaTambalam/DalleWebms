{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LDM_SR_jited.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_VZUcnVveXny",
        "115Y_TyxYkbk",
        "bJsLMq9pplwM",
        "O6t9w7xdHv6D"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Common\n",
        "Always run this, when start/restart the runtime"
      ],
      "metadata": {
        "id": "_VZUcnVveXny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "from scipy import integrate\n",
        "from threading import Thread\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm.auto import trange, tqdm\n",
        "\n",
        "import shutil\n",
        "import subprocess\n",
        "from google.colab import output\n",
        "\n",
        "\n",
        "def runpyproc(na):\n",
        "  subprocess.Popen(['python','/content/'+na+'.py'],close_fds=True)\n",
        "\n",
        "def append_dims(x, target_dims):\n",
        "    \"\"\"Appends dimensions to the end of a tensor until it has target_dims dimensions.\"\"\"\n",
        "    dims_to_append = target_dims - x.ndim\n",
        "    if dims_to_append < 0:\n",
        "        raise ValueError(f'input has {x.ndim} dims but target_dims is {target_dims}, which is less')\n",
        "    return x[(...,) + (None,) * dims_to_append]\n",
        "\n",
        "\n",
        "def append_zero(x):\n",
        "    return torch.cat([x, x.new_zeros([1])])\n",
        "\n",
        "\n",
        "def get_sigmas_karras(n, sigma_min, sigma_max, rho=7., device='cuda'):\n",
        "    \"\"\"Constructs the noise schedule of Karras et al. (2022).\"\"\"\n",
        "    ramp = torch.linspace(0, 1, n,device=device)\n",
        "    min_inv_rho = sigma_min ** (1 / rho)\n",
        "    max_inv_rho = sigma_max ** (1 / rho)\n",
        "    sigmas = (max_inv_rho + ramp * (min_inv_rho - max_inv_rho)) ** rho\n",
        "    return append_zero(sigmas).to(device)\n",
        "\n",
        "\n",
        "def get_sigmas_exponential(n, sigma_min, sigma_max, device='cpu'):\n",
        "    \"\"\"Constructs an exponential noise schedule.\"\"\"\n",
        "    sigmas = torch.linspace(math.log(sigma_max), math.log(sigma_min), n, device=device).exp()\n",
        "    return append_zero(sigmas)\n",
        "\n",
        "\n",
        "def get_sigmas_vp(n, beta_d=19.9, beta_min=0.1, eps_s=1e-3, device='cpu'):\n",
        "    \"\"\"Constructs a continuous VP noise schedule.\"\"\"\n",
        "    t = torch.linspace(1, eps_s, n, device=device)\n",
        "    sigmas = torch.sqrt(torch.exp(beta_d * t ** 2 / 2 + beta_min * t) - 1)\n",
        "    return append_zero(sigmas)\n",
        "\n",
        "\n",
        "def to_d(x, sigma, denoised):\n",
        "    \"\"\"Converts a denoiser output to a Karras ODE derivative.\"\"\"\n",
        "    return (x - denoised) / append_dims(sigma, x.ndim)\n",
        "\n",
        "\n",
        "def get_ancestral_step(sigma_from, sigma_to):\n",
        "    \"\"\"Calculates the noise level (sigma_down) to step down to and the amount\n",
        "    of noise to add (sigma_up) when doing an ancestral sampling step.\"\"\"\n",
        "    sigma_up = (sigma_to ** 2 * (sigma_from ** 2 - sigma_to ** 2) / sigma_from ** 2) ** 0.5\n",
        "    sigma_down = (sigma_to ** 2 - sigma_up ** 2) ** 0.5\n",
        "    return sigma_down, sigma_up\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_euler(model, x, sigmas, extra_args=None, callback=None, disable=None, s_churn=0., s_tmin=0., s_tmax=float('inf'), s_noise=1.):\n",
        "    \"\"\"Implements Algorithm 2 (Euler steps) from Karras et al. (2022).\"\"\"\n",
        "    extra_args = {} if extra_args is None else extra_args\n",
        "    s_in = x.new_ones([x.shape[0]])\n",
        "    for i in trange(len(sigmas) - 1, disable=disable):\n",
        "        gamma = min(s_churn / (len(sigmas) - 1), 2 ** 0.5 - 1) if s_tmin <= sigmas[i] <= s_tmax else 0.\n",
        "        eps = torch.randn_like(x) * s_noise\n",
        "        sigma_hat = sigmas[i] * (gamma + 1)\n",
        "        if gamma > 0:\n",
        "            x = x + eps * (sigma_hat ** 2 - sigmas[i] ** 2) ** 0.5\n",
        "        denoised = model( i, hlog0.revpre(x,sigmas,i), sigma_hat * s_in, **extra_args)\n",
        "        d = to_d(x, sigma_hat, denoised)\n",
        "        if callback is not None:\n",
        "            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigma_hat, 'denoised': denoised})\n",
        "        dt = sigmas[i + 1] - sigma_hat\n",
        "        # Euler method\n",
        "        x = x + d * dt\n",
        "    return x\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None):\n",
        "    \"\"\"Ancestral sampling with Euler method steps.\"\"\"\n",
        "    extra_args = {} if extra_args is None else extra_args\n",
        "    s_in = x.new_ones([x.shape[0]])\n",
        "    for i in trange(len(sigmas) - 1, disable=disable):\n",
        "        denoised = model(  i,  hlog0.revpre(x,sigmas,i), sigmas[i] * s_in, **extra_args)\n",
        "        sigma_down, sigma_up = get_ancestral_step(sigmas[i], sigmas[i + 1])\n",
        "        if callback is not None:\n",
        "            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n",
        "        d = to_d(x, sigmas[i], denoised)\n",
        "        # Euler method\n",
        "        dt = sigma_down - sigmas[i]\n",
        "        x = x + d * dt\n",
        "        x = x + torch.randn_like(x) * sigma_up\n",
        "    return x\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_heun(model, x, sigmas, extra_args=None, callback=None, disable=None, s_churn=0., s_tmin=0., s_tmax=float('inf'), s_noise=1.):\n",
        "    \"\"\"Implements Algorithm 2 (Heun steps) from Karras et al. (2022).\"\"\"\n",
        "    extra_args = {} if extra_args is None else extra_args\n",
        "    s_in = x.new_ones([x.shape[0]])\n",
        "    for i in trange(len(sigmas) - 1, disable=disable):\n",
        "        gamma = min(s_churn / (len(sigmas) - 1), 2 ** 0.5 - 1) if s_tmin <= sigmas[i] <= s_tmax else 0.\n",
        "        eps = torch.randn_like(x) * s_noise\n",
        "        sigma_hat = sigmas[i] * (gamma + 1)\n",
        "        if gamma > 0:\n",
        "            x = x + eps * (sigma_hat ** 2 - sigmas[i] ** 2) ** 0.5\n",
        "        denoised = model(  i,  hlog0.revpre(x,sigmas,i), sigma_hat * s_in, **extra_args)\n",
        "        d = to_d(x, sigma_hat, denoised)\n",
        "        if callback is not None:\n",
        "            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigma_hat, 'denoised': denoised})\n",
        "        dt = sigmas[i + 1] - sigma_hat\n",
        "        if sigmas[i + 1] == 0:\n",
        "            # Euler method\n",
        "            x = x + d * dt\n",
        "        else:\n",
        "            # Heun's method\n",
        "            x_2 = x + d * dt\n",
        "            denoised_2 = model(i, x_2, sigmas[i + 1] * s_in, **extra_args)\n",
        "            d_2 = to_d(x_2, sigmas[i + 1], denoised_2)\n",
        "            d_prime = (d + d_2) / 2\n",
        "            x = x + d_prime * dt\n",
        "    return x\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_dpm_2(model, x, sigmas, extra_args=None, callback=None, disable=None, s_churn=0., s_tmin=0., s_tmax=float('inf'), s_noise=1.):\n",
        "    \"\"\"A sampler inspired by DPM-Solver-2 and Algorithm 2 from Karras et al. (2022).\"\"\"\n",
        "    extra_args = {} if extra_args is None else extra_args\n",
        "    s_in = x.new_ones([x.shape[0]])\n",
        "    for i in trange(len(sigmas) - 1, disable=disable):\n",
        "        gamma = min(s_churn / (len(sigmas) - 1), 2 ** 0.5 - 1) if s_tmin <= sigmas[i] <= s_tmax else 0.\n",
        "        eps = torch.randn_like(x) * s_noise\n",
        "        sigma_hat = sigmas[i] * (gamma + 1)\n",
        "        if gamma > 0:\n",
        "            x = x + eps * (sigma_hat ** 2 - sigmas[i] ** 2) ** 0.5\n",
        "        denoised = model(  i,  hlog0.revpre(x,sigmas,i), sigma_hat * s_in, **extra_args)\n",
        "        d = to_d(x, sigma_hat, denoised)\n",
        "        if callback is not None:\n",
        "            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigma_hat, 'denoised': denoised})\n",
        "        # Midpoint method, where the midpoint is chosen according to a rho=3 Karras schedule\n",
        "        sigma_mid = ((sigma_hat ** (1 / 3) + sigmas[i + 1] ** (1 / 3)) / 2) ** 3\n",
        "        dt_1 = sigma_mid - sigma_hat\n",
        "        dt_2 = sigmas[i + 1] - sigma_hat\n",
        "        x_2 = x + d * dt_1\n",
        "        denoised_2 = model(i,x_2, sigma_mid * s_in, **extra_args)\n",
        "        d_2 = to_d(x_2, sigma_mid, denoised_2)\n",
        "        x = x + d_2 * dt_2\n",
        "    return x\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_dpm_2_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None):\n",
        "    \"\"\"Ancestral sampling with DPM-Solver inspired second-order steps.\"\"\"\n",
        "    extra_args = {} if extra_args is None else extra_args\n",
        "    s_in = x.new_ones([x.shape[0]])\n",
        "    for i in trange(len(sigmas) - 1, disable=disable):\n",
        "        denoised = model(  i,  hlog0.revpre(x,sigmas,i), sigmas[i] * s_in, **extra_args)\n",
        "        sigma_down, sigma_up = get_ancestral_step(sigmas[i], sigmas[i + 1])\n",
        "        if callback is not None:\n",
        "            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n",
        "        d = to_d(x, sigmas[i], denoised)\n",
        "        # Midpoint method, where the midpoint is chosen according to a rho=3 Karras schedule\n",
        "        sigma_mid = ((sigmas[i] ** (1 / 3) + sigma_down ** (1 / 3)) / 2) ** 3\n",
        "        dt_1 = sigma_mid - sigmas[i]\n",
        "        dt_2 = sigma_down - sigmas[i]\n",
        "        x_2 = x + d * dt_1\n",
        "        denoised_2 = model(i, x_2, sigma_mid * s_in, **extra_args)\n",
        "        d_2 = to_d(x_2, sigma_mid, denoised_2)\n",
        "        x = x + d_2 * dt_2\n",
        "        x = x + torch.randn_like(x) * sigma_up\n",
        "    return x\n",
        "\n",
        "\n",
        "def linear_multistep_coeff(order, t, i, j):\n",
        "    if order - 1 > i:\n",
        "        raise ValueError(f'Order {order} too high for step {i}')\n",
        "    def fn(tau):\n",
        "        prod = 1.\n",
        "        for k in range(order):\n",
        "            if j == k:\n",
        "                continue\n",
        "            prod *= (tau - t[i - k]) / (t[i - j] - t[i - k])\n",
        "        return prod\n",
        "    return integrate.quad(fn, t[i], t[i + 1], epsrel=1e-4)[0]\n",
        "\n",
        "class area4:\n",
        "  def __init__(self,tenz):\n",
        "    jlist=[self.cxxxx,self.cWxxx,self.cxExx,self.cWExx,\n",
        "        self.cxxNx,self.cWxNx,self.cxENx,self.cWENx,\n",
        "        self.cxxxS,self.cWxxS,self.cxExS,self.cWExS,\n",
        "        self.cxxNS,self.cWxNS,self.cxENS,self.cWENS]\n",
        "    skey=list(tenz.shape)\n",
        "    self.O_h=skey[2]\n",
        "    self.O_w=skey[3]\n",
        "    self.Wlap,self.Wpad,self.Wall=calcUnCrop4(0)\n",
        "    self.Elap,self.Epad,self.Eall=calcUnCrop4(1)\n",
        "    self.Nlap,self.Npad,self.Nall=calcUnCrop4(2)\n",
        "    self.Slap,self.Spad,self.Sall=calcUnCrop4(3)\n",
        "    skey[2]=self.O_h+self.Npad+self.Spad\n",
        "    skey[3]=self.O_w+self.Wpad+self.Epad\n",
        "    self.skey=skey\n",
        "    njmp=0\n",
        "    if self.Wall > 0:\n",
        "      njmp+=1\n",
        "    if self.Eall > 0:\n",
        "      njmp+=2\n",
        "    if self.Nall > 0:\n",
        "      njmp+=4\n",
        "    if self.Sall > 0:\n",
        "      njmp+=8\n",
        "    self.calc=jlist[njmp]\n",
        "    hlog0.setWENS(self.Wlap,self.Elap,self.Nlap,self.Slap)\n",
        "\n",
        "  def getshapes(self):\n",
        "    return self.calc()\n",
        "\n",
        "  def cxxxx(self):\n",
        "    return []\n",
        "  def simpNS(self):\n",
        "    return [(0,self.Nall, 0,None, 2),(-self.Sall,None, 0,None, 3)]\n",
        "  def simpWE(self):\n",
        "    return [(0,None, 0,self.Wall ,0),(0,None, -self.Eall,None ,1)]\n",
        "  #==\n",
        "  def cWxxx(self):\n",
        "    Npad=self.Npad\n",
        "    return [(Npad,self.O_h+Npad,0,self.Wall,4)]\n",
        "  def cxExx(self):\n",
        "    Npad=self.Npad\n",
        "    return [(Npad,self.O_h+Npad,-self.Eall,None,5)]\n",
        "  def cxxNx(self):\n",
        "    Wpad=self.Wpad\n",
        "    return [(0,self.Nall,self.Wpad,self.O_w+Wpad,6)]\n",
        "  def cxxxS(self):\n",
        "    Wpad=self.Wpad\n",
        "    return [(-self.Sall,None,self.Wpad,self.O_w+Wpad,7)]\n",
        "  #==\n",
        "  def cxENS(self):\n",
        "    Npad=self.Npad\n",
        "    return [(Npad,self.O_h+Npad,-self.Eall,None,1)]+self.simpNS()\n",
        "  def cWxNS(self):\n",
        "    Npad=self.Npad\n",
        "    return [(Npad,self.O_h+Npad,0,self.Wall,0)]+self.simpNS()\n",
        "  def cWExS(self):\n",
        "    Wpad=self.Wpad\n",
        "    return [(-self.Sall,None,self.Wpad,self.O_w+Wpad,3)]+self.simpWE()\n",
        "  def cWENx(self):\n",
        "    Wpad=self.Wpad\n",
        "    return [(0,self.Nall,self.Wpad,self.O_w+Wpad,2)]+self.simpWE()\n",
        "  #==\n",
        "  def cWExx(self):\n",
        "    return self.cWxxx()+self.cxExx()\n",
        "  def cxxNS(self):\n",
        "    return self.cxxNx()+self.cxxxS()\n",
        "  #==\n",
        "  def cWxNx(self):\n",
        "    if self.skey[2] > self.skey[3]: #h>w\n",
        "      return [(self.Npad,None, 0,self.Wall   ,0),(0,self.Nall, 0,None     ,2)]\n",
        "    return   [(0,self.Nall,   self.Wpad,None ,2),(0,None,    0,self.Wall  ,0)]\n",
        "  def cxENx(self):\n",
        "    if self.skey[2] > self.skey[3]:\n",
        "      return [(self.Npad,None, -self.Eall,None, 1),(0,self.Nall, 0,None,    2)]\n",
        "    return   [(0,self.Nall,  0,-self.Epad,   2),(0,None,   -self.Eall,None,1)]\n",
        "  def cWxxS(self):\n",
        "    if self.skey[2] > self.skey[3]:\n",
        "      return [(0,-self.Spad,  0,self.Wall   ,0),(-self.Sall,None, 0,None  ,3)]\n",
        "    return   [(-self.Sall,None, self.Wpad,None ,3),(0,None,     0,self.Wall,0)]\n",
        "  def cxExS(self):\n",
        "    if self.skey[2] > self.skey[3]:\n",
        "      return [(0,-self.Spad,  -self.Eall,None ,1),(-self.Sall,None,  0,None    ,3)]\n",
        "    return   [(-self.Sall,None, 0,-self.Epad,  3),(0,None,      -self.Eall,None,1)]\n",
        "  #==\n",
        "  def cWENS(self):\n",
        "    Wpad=self.Wpad\n",
        "    Npad=self.Npad\n",
        "    if self.skey[2] > self.skey[3]:\n",
        "      if Npad > self.Spad:\n",
        "        return [(-self.Sall,None, Wpad,self.O_w+Wpad,  3),(Npad,None  ,0,self.Wall,0),(Npad,None,-self.Eall,None,1)  ,(0,self.Nall,   0,None  ,2)]\n",
        "      else:\n",
        "        return [(0,self.Nall,   Wpad,self.O_w+Wpad,  2),(0,-self.Spad,0,self.Wall,0),(0,-self.Spad,-self.Eall,None,1)  ,(-self.Sall,None, 0,None  ,3)]\n",
        "    else:\n",
        "      if Wpad > self.Epad:\n",
        "        return [(Npad,self.O_h+Npad, -self.Eall,None ,1),(0,self.Nall, Wpad,None   ,2),(-self.Sall,None, Wpad,None ,3)  ,(0,None, 0,self.Wall  ,0)]\n",
        "      else:\n",
        "        return [(Npad,self.O_h+Npad, 0,self.Wall   ,0),(0,self.Nall, 0,-self.Epad ,2),(-self.Sall,None, 0,-self.Epad ,3) ,(0,None, -self.Eall,None ,1)]\n",
        "\n",
        "def arrmover(arr, itm, n):\n",
        "  if itm is None:\n",
        "    return None\n",
        "  if len(arr) == n:\n",
        "    new_itm=[None]*len(itm)\n",
        "    arr.append(new_itm)\n",
        "    return new_itm\n",
        "  return arr[n]\n",
        "\n",
        "def mulifnotnone(v,r):\n",
        "  if v is None:\n",
        "    return None\n",
        "  return int(0.5+v*r)\n",
        "\n",
        "class hlogger:\n",
        "  def __init__(self):\n",
        "    self.Arevpre = self.Arevpre0\n",
        "    self.revpre = self.revpre0\n",
        "    self.revpre_nocpy = self.revpre0\n",
        "    self.latlog_arr=[]\n",
        "    self.h_bs_arr=[]\n",
        "    self.latlog=None\n",
        "    self.h_bs=None\n",
        "    self.h_bsB=None\n",
        "    self.Wlap=None\n",
        "    self.Elap=None\n",
        "    self.Nlap=None\n",
        "    self.Slap=None\n",
        "    self.Wlap2=None\n",
        "    self.Elap2=None\n",
        "    self.Nlap2=None\n",
        "    self.Slap2=None\n",
        "    self.funclist={'0':self.Arevpre0,'logw0':self.logw0,'logw':self.logw,'loghs':self.loghs}\n",
        "    self.funclistb={'0':self.revpre0,'masking':self.revpreMSK,'1s':self.revpre1s,'log':self.revpre0_log}\n",
        "    self.funclist2=[self.revpreW,self.revpreE,self.revpreN,self.revpreS,\n",
        "            self.revpreW_nocpy,self.revpreE_nocpy,self.revpreN_nocpy,self.revpreS_nocpy]\n",
        "    self.funclist2b=[self.bW,self.bE,self.bN,self.bS]\n",
        "    self.funclist2c=[self.bWsimp,self.bEsimp,self.bNsimp,self.bSsimp]\n",
        "    self.func2Nb_cache=99\n",
        "    self.func2Nc_cache=99\n",
        "\n",
        "\n",
        "  def setWENS(self,Wlap,Elap,Nlap,Slap):\n",
        "    self.Wlap=Wlap\n",
        "    self.Elap=Elap\n",
        "    self.Nlap=Nlap\n",
        "    self.Slap=Slap\n",
        "    self.Wlap2=Wlap<<1\n",
        "    self.Elap2=Elap<<1\n",
        "    self.Nlap2=Nlap<<1\n",
        "    self.Slap2=Slap<<1\n",
        "\n",
        "  def activefuncN2x(self, nx_cache, funclist):\n",
        "    if nx_cache < 99:\n",
        "      ndm=3\n",
        "      if nx_cache < 2:\n",
        "        ndm=2\n",
        "      if self.h_bsB.size(ndm) != noise.size(ndm):\n",
        "        self.revpre = self.revpre0\n",
        "      else:\n",
        "        self.revpre=funclist[nx_cache]\n",
        "\n",
        "\n",
        "\n",
        "  def set_multinm(self,n,cur_h,dst_h,cur_w,dst_w):\n",
        "    self.latlog = arrmover(self.latlog_arr, self.latlog, n)\n",
        "    self.h_bs = arrmover(self.h_bs_arr, self.h_bs, n)\n",
        "    self.activefuncN2x(self.func2Nb_cache, self.funclist2b)\n",
        "    self.activefuncN2x(self.func2Nc_cache, self.funclist2c)\n",
        "    if n == 0:\n",
        "      self.Wlap_orig=self.Wlap\n",
        "      self.Elap_orig=self.Elap\n",
        "      self.Nlap_orig=self.Nlap\n",
        "      self.Slap_orig=self.Slap\n",
        "      self.Wlap2_orig=self.Wlap2\n",
        "      self.Elap2_orig=self.Elap2\n",
        "      self.Nlap2_orig=self.Nlap2\n",
        "      self.Slap2_orig=self.Slap2\n",
        "      \n",
        "    if cur_h != dst_h:\n",
        "      r=cur_h/dst_h\n",
        "      self.Nlap=mulifnotnone(self.Nlap_orig,r)\n",
        "      self.Nlap2=mulifnotnone(self.Nlap2_orig,r)\n",
        "      self.Slap=mulifnotnone(self.Slap_orig,r)\n",
        "      self.Slap2=mulifnotnone(self.Slap2_orig,r)\n",
        "    else:\n",
        "      self.Nlap2=self.Nlap2_orig\n",
        "      self.Slap2=self.Slap2_orig\n",
        "      self.Nlap=self.Nlap_orig\n",
        "      self.Slap=self.Slap_orig\n",
        "\n",
        "    if cur_w != dst_w:\n",
        "      r=cur_w/dst_w\n",
        "      self.Elap=mulifnotnone(self.Elap_orig,r)\n",
        "      self.Elap2=mulifnotnone(self.Elap2_orig,r)\n",
        "      self.Wlap=mulifnotnone(self.Wlap_orig,r)\n",
        "      self.Wlap2=mulifnotnone(self.Wlap2_orig,r)\n",
        "    else:\n",
        "      self.Elap2=self.Elap2_orig\n",
        "      self.Wlap2=self.Wlap2_orig\n",
        "      self.Elap=self.Elap_orig\n",
        "      self.Wlap=self.Wlap_orig\n",
        "\n",
        "\n",
        "  def setfunc(self,key):\n",
        "    self.Arevpre=self.funclist[key]\n",
        "  def setfuncb(self,key,key2='0'):\n",
        "    self.revpre=self.funclistb[key]\n",
        "    self.revpre_nocpy=self.funclistb[key2]\n",
        "  def setfuncN(self,n):\n",
        "    self.Arevpre=self.funclist2[n]\n",
        "\n",
        "  def setfuncNb(self,n,cache=False):\n",
        "    if n > 3:\n",
        "      n-=4\n",
        "    if cache:\n",
        "      self.func2Nb_cache=n\n",
        "    else:\n",
        "      self.revpre=self.funclist2b[n]\n",
        "\n",
        "  def setfuncNc(self,n,cache=False):\n",
        "    if cache:\n",
        "      self.func2Nc_cache=n\n",
        "    else:\n",
        "      self.revpre=self.funclist2c[n]\n",
        "\n",
        "  def setbsB(self,fn,lat):\n",
        "    if fn > 3:\n",
        "      fn-=4\n",
        "    if fn==-10:\n",
        "      self.h_bsB=lat\n",
        "    elif fn==-11:\n",
        "      self.h_bsB=lat[:,:,:,-self.Elap2:-self.Elap].cuda()\n",
        "    elif fn==0:\n",
        "      self.h_bsB=torch.cat([ lat[:,:,:,:-self.Wlap], self.h_bsB[:,:,:,self.Wlap:] ],dim=3)\n",
        "    elif fn==1:\n",
        "      self.h_bsB=torch.cat([ self.h_bsB[:,:,:,:-self.Elap], lat[:,:,:,self.Elap:] ],dim=3)\n",
        "    elif fn==2:\n",
        "      self.h_bsB=torch.cat([ lat[:,:,:-self.Nlap,:], self.h_bsB[:,:,self.Nlap:,:] ],dim=2)\n",
        "    elif fn==3:\n",
        "      self.h_bsB=torch.cat([ self.h_bsB[:,:,:-self.Slap,:], lat[:,:,self.Slap:,:] ],dim=2)\n",
        "\n",
        "\n",
        "  def revpre0(self,img,sigmas,t):\n",
        "    return img\n",
        "\n",
        "  def revpreMSK(self,img,sigmas,t):\n",
        "    return (revpreimg+noise * sigmas[t])*(1-zamask)+img*zamask\n",
        "\n",
        "  def revpre1s(self,img,sigmas,t):\n",
        "    return preimg+(noise*sigmas[t])\n",
        "\n",
        "  def revpre0_log(self,img,sigmas,t):\n",
        "    self.latlog.append( ((img-noise*sigmas[t])*(1+sigmas[t]*0.18215) ).cpu().numpy())\n",
        "    return img\n",
        "  def Arevpre0(self,h,d):\n",
        "    return\n",
        "  def logw0(self,h,d):\n",
        "    self.h_bs[d]=h[:,:,:,-self.Elap:].cpu()\n",
        "    return\n",
        "\n",
        "  def logw(self,h,d):\n",
        "    h[:,:,:,:self.Elap]=self.h_bs[d]\n",
        "    self.logw0(h,d)\n",
        "    return\n",
        "  def loghs(self,h,d):\n",
        "    self.h_bs[d]=h.cpu()\n",
        "    return\n",
        "  def revpreW(self,h,d):\n",
        "    hbz=self.h_bs[d]\n",
        "    h[:,:,:,-self.Wlap:]=hbz[:,:,:,:self.Wlap]\n",
        "    self.h_bs[d]=torch.cat([ h[:,:,:,:-self.Wlap].cpu(), hbz ],dim=3)\n",
        "    return\n",
        "  def revpreE(self,h,d):\n",
        "    hbz=self.h_bs[d]\n",
        "    h[:,:,:,:self.Elap]=hbz[:,:,:,-self.Elap:]\n",
        "    self.h_bs[d]=torch.cat([ hbz, h[:,:,:,self.Elap:].cpu() ],dim=3)\n",
        "    return\n",
        "  def revpreN(self,h,d):\n",
        "    hbz=self.h_bs[d]\n",
        "    h[:,:,-self.Nlap:,:]=hbz[:,:,:self.Nlap,:]\n",
        "    self.h_bs[d]=torch.cat([ h[:,:,:-self.Nlap,:].cpu(), hbz ],dim=2)\n",
        "    return\n",
        "  def revpreS(self,h,d):\n",
        "    hbz=self.h_bs[d]\n",
        "    h[:,:,:self.Slap,:]=hbz[:,:,-self.Slap:,:]\n",
        "    self.h_bs[d]=torch.cat([ hbz,h[:,:,self.Slap:,:].cpu() ],dim=2)\n",
        "    return\n",
        "\n",
        "  def revpreW_nocpy(self,h,d):\n",
        "    h[:,:,:,-self.Wlap:]=self.h_bs[d][:,:,:,:self.Wlap]\n",
        "    return\n",
        "  def revpreE_nocpy(self,h,d):\n",
        "    h[:,:,:,:self.Elap]=self.h_bs[d][:,:,:,-self.Elap:]\n",
        "    return\n",
        "  def revpreN_nocpy(self,h,d):\n",
        "    h[:,:,-self.Nlap:,:]=self.h_bs[d][:,:,:self.Nlap,:]\n",
        "    return\n",
        "  def revpreS_nocpy(self,h,d):\n",
        "    h[:,:,:self.Slap,:]=self.h_bs[d][:,:,-self.Slap:,:]\n",
        "    return\n",
        "\n",
        "\n",
        "  def bW(self,img,sigmas,t):\n",
        "    img[:,:,:,-self.Wlap:]=self.h_bsB[:,:,:,self.Wlap:self.Wlap2]+(noise[:,:,:,-self.Wlap:]*sigmas[t])\n",
        "    return img\n",
        "  def bE(self,img,sigmas,t):\n",
        "    img[:,:,:,:self.Elap]=self.h_bsB[:,:,:,-self.Elap2:-self.Elap]+(noise[:,:,:,:self.Elap]*sigmas[t])\n",
        "    return img\n",
        "  def bN(self,img,sigmas,t):\n",
        "    img[:,:,-self.Nlap:,:]=self.h_bsB[:,:,self.Nlap:self.Nlap2,:]+(noise[:,:,-self.Nlap:,:]*sigmas[t])\n",
        "    return img\n",
        "  def bS(self,img,sigmas,t):\n",
        "    img[:,:,:self.Slap,:]=self.h_bsB[:,:,-self.Slap2:-self.Slap,:]+(noise[:,:,:self.Slap,:]*sigmas[t])\n",
        "    return img\n",
        "\n",
        "  def bWsimp(self,img,sigmas,t):\n",
        "    img[:,:,:,-self.Wlap:]=self.h_bsB+(noise[:,:,:,-self.Wlap:]*sigmas[t])\n",
        "    return img\n",
        "  def bEsimp(self,img,sigmas,t):\n",
        "    img[:,:,:,:self.Elap]=self.h_bsB+(noise[:,:,:,:self.Elap]*sigmas[t])\n",
        "    return img\n",
        "  def bNsimp(self,img,sigmas,t):\n",
        "    img[:,:,-self.Nlap:,:]=self.h_bsB+(noise[:,:,-self.Nlap:,:]*sigmas[t])\n",
        "    return img\n",
        "  def bSsimp(self,img,sigmas,t):\n",
        "    img[:,:,:self.Slap,:]=self.h_bsB+(noise[:,:,:self.Slap,:]*sigmas[t])\n",
        "    return img\n",
        "\n",
        "hlog0=hlogger()\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_lms(model, x, sigmas, extra_args=None, callback=None, disable=None, order=4):\n",
        "    extra_args = {} if extra_args is None else extra_args\n",
        "    s_in = x.new_ones([x.shape[0]])\n",
        "    sigmas_cpu = sigmas.detach().cpu().numpy()\n",
        "    ds = []\n",
        "    for i in trange(len(sigmas) - 1, disable=disable):\n",
        "        denoised = model(  i,  hlog0.revpre(x,sigmas,i) , sigmas[i] * s_in, **extra_args)\n",
        "        d = to_d(x, sigmas[i], denoised)\n",
        "        ds.append(d)\n",
        "        if len(ds) > order:\n",
        "            ds.pop(0)\n",
        "        if callback is not None:\n",
        "            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n",
        "        cur_order = min(i + 1, order)\n",
        "        coeffs = [linear_multistep_coeff(cur_order, sigmas_cpu, i, j) for j in range(cur_order)]\n",
        "        x = x + sum(coeff * d for coeff, d in zip(coeffs, reversed(ds)))\n",
        "    return x\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def log_likelihood(model, x, sigma_min, sigma_max, extra_args=None, atol=1e-4, rtol=1e-4):\n",
        "    extra_args = {} if extra_args is None else extra_args\n",
        "    s_in = x.new_ones([x.shape[0]])\n",
        "    v = torch.randint_like(x, 2) * 2 - 1\n",
        "    fevals = 0\n",
        "    def ode_fn(sigma, x):\n",
        "        nonlocal fevals\n",
        "        with torch.enable_grad():\n",
        "            x = x[0].detach().requires_grad_()\n",
        "            denoised = model(x, sigma * s_in, **extra_args)\n",
        "            d = to_d(x, sigma, denoised)\n",
        "            fevals += 1\n",
        "            grad = torch.autograd.grad((d * v).sum(), x)[0]\n",
        "            d_ll = (v * grad).flatten(1).sum(1)\n",
        "        return d.detach(), d_ll\n",
        "    x_min = x, x.new_zeros([x.shape[0]])\n",
        "    t = x.new_tensor([sigma_min, sigma_max])\n",
        "    sol = odeint(ode_fn, x_min, t, atol=atol, rtol=rtol, method='dopri5')\n",
        "    latent, delta_ll = sol[0][-1], sol[1][-1]\n",
        "    ll_prior = torch.distributions.Normal(0, sigma_max).log_prob(latent).flatten(1).sum(1)\n",
        "    return ll_prior + delta_ll, {'fevals': fevals}\n",
        "\n",
        "\n",
        "\n",
        "class DiscreteSchedule(nn.Module):\n",
        "    \"\"\"A mapping between continuous noise levels (sigmas) and a list of discrete noise\n",
        "    levels.\"\"\"\n",
        "\n",
        "    def __init__(self, sigmas, quantize):\n",
        "        super().__init__()\n",
        "        self.register_buffer('sigmas', sigmas)\n",
        "        self.quantize = quantize\n",
        "\n",
        "    def get_sigmas(self, n=None):\n",
        "        if n is None:\n",
        "            return append_zero(self.sigmas.flip(0))\n",
        "        t_max = len(self.sigmas) - 1\n",
        "        t = torch.linspace(t_max, 0, n, device=self.sigmas.device)\n",
        "        return append_zero(self.t_to_sigma(t))\n",
        "\n",
        "    def sigma_to_t(self, sigma, quantize=None):\n",
        "        quantize = self.quantize if quantize is None else quantize\n",
        "        \n",
        "        dists = torch.abs(sigma - self.sigmas[:, None])\n",
        "        if quantize:\n",
        "            return torch.argmin(dists, dim=0).view(sigma.shape)\n",
        "        low_idx, high_idx = torch.sort(torch.topk(dists, dim=0, k=2, largest=False).indices, dim=0)[0]\n",
        "        low, high = self.sigmas[low_idx], self.sigmas[high_idx]\n",
        "        w = (low - sigma) / (low - high)\n",
        "        w = w.clamp(0, 1)\n",
        "        t = (1 - w) * low_idx + w * high_idx\n",
        "        return t.view(sigma.shape)\n",
        "\n",
        "    def t_to_sigma(self, t):\n",
        "        t = t.float()\n",
        "        low_idx, high_idx, w = t.floor().long(), t.ceil().long(), t.frac()\n",
        "        return (1 - w) * self.sigmas[low_idx] + w * self.sigmas[high_idx]\n",
        "\n",
        "\n",
        "class DiscreteEpsDDPMDenoiser(DiscreteSchedule):\n",
        "    \"\"\"A wrapper for discrete schedule DDPM models that output eps (the predicted\n",
        "    noise).\"\"\"\n",
        "\n",
        "    def __init__(self, model, alphas_cumprod, quantize):\n",
        "        super().__init__(((1 - alphas_cumprod) / alphas_cumprod) ** 0.5, quantize)\n",
        "        self.inner_model = model\n",
        "        self.sigma_data = 1.\n",
        "\n",
        "    def get_scalings(self, sigma):\n",
        "        c_out = -sigma\n",
        "        c_in = 1 / (sigma ** 2 + self.sigma_data ** 2) ** 0.5\n",
        "        return c_out, c_in\n",
        "\n",
        "    def get_eps(self, *args, **kwargs):\n",
        "        return self.inner_model(*args, **kwargs)\n",
        "\n",
        "    def loss(self, input, noise, sigma, **kwargs):\n",
        "        c_out, c_in = [append_dims(x, input.ndim) for x in self.get_scalings(sigma)]\n",
        "        noised_input = input + noise * append_dims(sigma, input.ndim)\n",
        "        eps = self.get_eps(noised_input * c_in, self.sigma_to_t(sigma), **kwargs)\n",
        "        return (eps - noise).pow(2).flatten(1).mean(1)\n",
        "\n",
        "    def forward(self, input, sigma, **kwargs):\n",
        "        c_out, c_in = [append_dims(x, input.ndim) for x in self.get_scalings(sigma)]\n",
        "        eps = self.get_eps(input * c_in, self.sigma_to_t(sigma), **kwargs)\n",
        "        return input + eps * c_out\n",
        "\n",
        "\n",
        "\n",
        "def make_ddim_timesteps(num_ddim_timesteps, num_ddpm_timesteps):\n",
        "    c = num_ddpm_timesteps // num_ddim_timesteps\n",
        "    ddim_timesteps = np.asarray(list(range(0, num_ddpm_timesteps, c)))\n",
        "\n",
        "    # add one to get the final alpha values right (the ones from first scale to data during sampling)\n",
        "    steps_out = ddim_timesteps + 1\n",
        "\n",
        "    return steps_out\n",
        "\n",
        "\n",
        "def make_ddim_sampling_parameters(alphacums, ddim_timesteps, eta):\n",
        "    # select alphas for computing the variance schedule\n",
        "    alphas = alphacums[ddim_timesteps]\n",
        "    alphas_prev = np.asarray([alphacums[0]] + alphacums[ddim_timesteps[:-1]].tolist())\n",
        "\n",
        "    # according the the formula provided in https://arxiv.org/abs/2010.02502\n",
        "    sigmas = eta * np.sqrt((1 - alphas_prev) / (1 - alphas) * (1 - alphas / alphas_prev))\n",
        "\n",
        "    return sigmas, alphas, alphas_prev\n",
        "\n",
        "def makerng():\n",
        "  global seed\n",
        "  if seed == 0:\n",
        "    seed=random.randint(0, 2**32)\n",
        "    print('random seed=')\n",
        "    print(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "\n",
        "def dlpromptexample():\n",
        "  !wget https://github.com/TabuaTambalam/DalleWebms/releases/download/0.1/pexmp.7z\n",
        "  !7z x pexmp.7z\n",
        "  !wget -O web/psg1.htm https://raw.githubusercontent.com/TabuaTambalam/DalleWebms/main/docs/web/psg1.htm\n",
        "  !wget -O web/svr.py https://raw.githubusercontent.com/TabuaTambalam/DalleWebms/main/docs/web/svr.py\n",
        "\n",
        "\n",
        "def mkmodel_state_dict():\n",
        "  try:\n",
        "    import jkt\n",
        "  except:\n",
        "    !wget https://raw.githubusercontent.com/TabuaTambalam/DalleWebms/main/docs/sd/jkt.py\n",
        "    import jkt\n",
        "  \n",
        "  difjit=[diffusion_emb,diffusion_mid,diffusion_out]\n",
        "  model_state_dict = {}\n",
        "  jna1=jkt.nam1\n",
        "  for i in range(3):\n",
        "    sd=difjit[i].state_dict()\n",
        "    jna2=jkt.nam2[i]\n",
        "    for k in sd:\n",
        "      uwa=sd[k]\n",
        "      if 'pnnx' in k:\n",
        "        model_state_dict[jna2[k]]=uwa\n",
        "      else:\n",
        "        model_state_dict[jna1[k]]=uwa\n",
        "  return model_state_dict\n",
        "\n",
        "\n",
        "def procLat(lat):\n",
        "  if lat.dim() == 3:\n",
        "    return [lat.unsqueeze(0)],1\n",
        "  nbat=lat.size(0)\n",
        "  if nbat > 1:\n",
        "    ret=[None]*nbat\n",
        "    for i in range(nbat):\n",
        "      ret[i]=lat[i].unsqueeze(0)\n",
        "    return ret, nbat\n",
        "  return [lat],1\n",
        "\n",
        "\n",
        "SDlatDEC=None\n",
        "def latdec(fna,scale=5.5):\n",
        "  global SDlatDEC\n",
        "  if SDlatDEC is None:\n",
        "    if not os.path.isfile('autoencoder_pnnx.pt'):\n",
        "      !wget https://huggingface.co/Larvik/sd470k/resolve/main/autoencoder_pnnx.pt\n",
        "    SDlatDEC=torch.jit.load('autoencoder_pnnx.pt').cuda()\n",
        "  lat,l =procLat(torch.tensor(np.load(fna)).cuda())\n",
        "  for i in range(l):\n",
        "    lat[i]=SDlatDEC(lat[i]*scale)[0]\n",
        "  return lat\n",
        "\n",
        "def latdec2(fna,scale=5.5):\n",
        "  global SDlatDEC\n",
        "  if SDlatDEC is None:\n",
        "    if not os.path.isfile('autoencoder_pnnx.pt'):\n",
        "      !wget https://huggingface.co/Larvik/sd470k/resolve/main/autoencoder_pnnx.pt\n",
        "    SDlatDEC=torch.jit.load('autoencoder_pnnx.pt').cuda()\n",
        "  lat,l =procLat(torch.tensor(fna).cuda())\n",
        "  for i in range(l):\n",
        "    lat[i]=SDlatDEC(lat[i]*scale)[0]\n",
        "  return lat\n",
        "\n",
        "def localhttp(root='/content/'):\n",
        "  global HTML\n",
        "  if not os.path.isfile('/content/sample_data/izh.txt'):\n",
        "    from IPython.core.display import HTML\n",
        "    !nohup python3 -m http.server -d {root} 8333 > /content/sample_data/izh.txt &\n",
        "\n",
        "\n",
        "def f_sampler():\n",
        "  global UseSamplr\n",
        "  if Sampler == 'euler':\n",
        "    UseSamplr = sample_euler\n",
        "  elif Sampler == 'euler_a':\n",
        "    UseSamplr = sample_euler_ancestral\n",
        "  elif Sampler == 'heun':\n",
        "    UseSamplr = sample_heun\n",
        "  elif Sampler == 'dpm_2':\n",
        "    UseSamplr = sample_dpm_2\n",
        "  elif Sampler == 'dpm_2_a':\n",
        "    UseSamplr = sample_dpm_2_ancestral\n",
        "  elif Sampler == 'lms':\n",
        "    UseSamplr = sample_lms\n",
        "\n",
        "def f_sigmas():\n",
        "  if Karras:\n",
        "    return ddim_eta*get_sigmas_karras(ddim_num_steps,model_wrap.sigmas[0].item(),model_wrap.sigmas[-1].item(),rho=KarrasRho, device=cudev )\n",
        "  else:\n",
        "    return ddim_eta*model_wrap.get_sigmas(ddim_num_steps)\n",
        "\n",
        "def fixver(ver,dfsver):\n",
        "  if ver != '470k':\n",
        "    return ''\n",
        "  return dfsver\n",
        "def f_dljit(ver='470k',dfsver=''):\n",
        "  dfsver=fixver(ver,dfsver)\n",
        "  if not os.path.isfile('imgencoder_pnnx.pt'):\n",
        "    !pip install ftfy transformers omegaconf triton==2.0.0.dev20220701 einops accelerate\n",
        "    !wget https://huggingface.co/Larvik/sd{ver}/resolve/main/alphas_cumprod.npz\n",
        "    !wget https://huggingface.co/Larvik/tfmod/resolve/main/transformer_pnnx.pt\n",
        "    !wget https://huggingface.co/Larvik/sd{ver}/resolve/main/autoencoder_pnnx.pt\n",
        "    !wget https://huggingface.co/Larvik/sd{ver}/resolve/main/imgencoder_pnnx.pt\n",
        "  ver+=dfsver\n",
        "  !mkdir {ver}\n",
        "  if not os.path.isfile(ver+'/diffusion_out_pnnx.pt'):\n",
        "    !wget -P {ver}/ https://huggingface.co/Larvik/sd{ver}/resolve/main/diffusion_emb_pnnx.pt\n",
        "    !wget -P {ver}/ https://huggingface.co/Larvik/sd{ver}/resolve/main/diffusion_mid_pnnx.pt\n",
        "    !wget -P {ver}/ https://huggingface.co/Larvik/sd{ver}/resolve/main/diffusion_out_pnnx.pt\n",
        "  return ver+'/'\n",
        "\n",
        "def install_xformer():\n",
        "  print('xformer')\n",
        "  if not os.path.isfile('xformers/_C.so'):\n",
        "    !wget https://raw.githubusercontent.com/TabuaTambalam/DalleWebms/main/docs/sd/jkt.py\n",
        "    from subprocess import getoutput\n",
        "    pfix='T4'\n",
        "    gputyp=getoutput('nvidia-smi')\n",
        "    if 'P100' in gputyp:\n",
        "      pfix = 'P100'\n",
        "    elif 'V100' in gputyp:\n",
        "      pfix = 'V100'\n",
        "    elif 'A100' in gputyp:\n",
        "      pfix = 'A100'\n",
        "    !pip install https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/{pfix}/xformers-0.0.13.dev0-py3-none-any.whl\n",
        "    !mv /usr/local/lib/python3.7/dist-packages/xformers /content/xformers\n",
        "\n",
        "\n",
        "def get_keys_to_submodule(model):\n",
        "  keys_to_submodule = {}\n",
        "  # iterate all submodules\n",
        "  for submodule_name, submodule in model.named_modules():\n",
        "      # iterate all paramters in each submobule\n",
        "      for param_name, param in submodule.named_parameters():\n",
        "          # param_name is organized as <name>.<subname>.<subsubname> ...\n",
        "          splitted_param_name = param_name.split('.')\n",
        "          # we cannot go inside it anymore. This is the actual parameter\n",
        "          is_leaf_param = len(splitted_param_name) == 1\n",
        "          if is_leaf_param:\n",
        "              # we recreate the correct key\n",
        "              key = f\"{submodule_name}.{param_name}\"\n",
        "              # we associate this key with this submodule\n",
        "              keys_to_submodule[key] = submodule\n",
        "              \n",
        "  return keys_to_submodule\n",
        "\n",
        "inpaintwgt='UserEmb/inpaintwgt.pt'\n",
        "def wgt_to_inp(state_dict):\n",
        "  if not os.path.isfile(inpaintwgt):\n",
        "    !wget -O {inpaintwgt} https://huggingface.co/Larvik/tfmod/resolve/main/inpaintwgt.pt\n",
        "  state_dict['input_blocks.0.0.weight']=torch.cat((state_dict['input_blocks.0.0.weight'],torch.load(inpaintwgt)),dim=1)\n",
        "  return state_dict\n",
        "kvwgtbak=dict()\n",
        "wgtkeybysz={\n",
        "    320:[],\n",
        "    640:[],\n",
        "    768:[],\n",
        "    1280:[]\n",
        "}\n",
        "atnnames=[\n",
        "    'attn1.to_k',\n",
        "    'attn1.to_v',\n",
        "    'attn2.to_k',\n",
        "    'attn2.to_v'\n",
        "]\n",
        "\n",
        "KVmerge_ratio=1.0\n",
        "\n",
        "def mergeWnB(subkey,dk,dback):\n",
        "  a3=kvwgtbak[subkey+'.weight'].float().T\n",
        "  k=KVmerge_ratio\n",
        "  dback[subkey+'.weight']=  ( a3+ k*(( dk['linear1.weight'].T @ dk['linear2.weight'].T )@a3) ).half().T\n",
        "  dback[subkey+'.bias']=( dk['linear1.bias'].T @ (dk['linear2.weight'].T @ a3) +dk['linear2.bias'].T@a3  ).half().T\n",
        "\n",
        "\n",
        "def mkPreKV_dict(pt):\n",
        "  dback=dict()\n",
        "  fk=torch.load(pt,map_location=cudev)\n",
        "  for shpkey in wgtkeybysz:\n",
        "    toK, toV=fk[shpkey]\n",
        "    bag=wgtkeybysz[shpkey]\n",
        "    for subkey in bag:\n",
        "      if subkey.endswith('.to_k'):\n",
        "        mergeWnB(subkey,toK,dback)\n",
        "      else:\n",
        "        mergeWnB(subkey,toV,dback)\n",
        "  return dback\n",
        "\n",
        "\n",
        "def add_kvbias2(state_dict,k):\n",
        "  for j in range(4):\n",
        "    k0=k+atnnames[j]\n",
        "    k1=k0+'.weight'\n",
        "    wgt=state_dict[k1]\n",
        "    kvwgtbak[k1]=wgt\n",
        "    state_dict[k0+'.bias']=torch.zeros(wgt.size(0))\n",
        "    wgtkeybysz[wgt.size(1)].append(k0)\n",
        "\n",
        "def add_kvbias(state_dict):\n",
        "  yp=list(range(1,9))\n",
        "  yp.remove(3)\n",
        "  yp.remove(6)\n",
        "  for i in yp:\n",
        "    add_kvbias2(state_dict,'input_blocks.'+str(i)+'.1.transformer_blocks.0.')\n",
        "  \n",
        "  add_kvbias2(state_dict,'middle_block.1.transformer_blocks.0.')\n",
        "\n",
        "  for i in range(3,12):\n",
        "    add_kvbias2(state_dict,'output_blocks.'+str(i)+'.1.transformer_blocks.0.')\n",
        "\n",
        "  return state_dict\n",
        "      \n",
        "\n",
        "\n",
        "def load_state_dict_with_low_memory(model, state_dict,modifyfunc=None,fill=True):\n",
        "  if modifyfunc is not None:\n",
        "    state_dict=modifyfunc(state_dict)\n",
        "  print('======hacky load======')\n",
        "  keys_to_submodule = get_keys_to_submodule(model)\n",
        "  mste=model.state_dict()\n",
        "  for key, submodule in keys_to_submodule.items():\n",
        "      # get the valye from the state_dict\n",
        "      if key in state_dict:\n",
        "        val = state_dict[key]\n",
        "      elif fill:\n",
        "        print(key)\n",
        "        val = torch.ones(mste[key].shape, dtype= torch.float16)\n",
        "      else:\n",
        "        continue\n",
        "\n",
        "      param_name = key.split('.')[-1]\n",
        "      new_val = torch.nn.Parameter(val,requires_grad=False)\n",
        "      setattr(submodule, param_name, new_val)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ldmbase='ldm'\n",
        "def init_ldm(mode,sd_modify=-1):\n",
        "  print('orig ldm')\n",
        "  if not os.path.exists('ldm_opt'):\n",
        "    !wget https://github.com/TabuaTambalam/DalleWebms/releases/download/0.1/ldms.7z\n",
        "    !7z x ldms.7z\n",
        "  if os.path.exists(ldmbase):\n",
        "    os.unlink(ldmbase)\n",
        "  if mode==1:\n",
        "    os.symlink('ldm_opt',ldmbase)\n",
        "  elif mode==2:\n",
        "    os.symlink('ldm_xfm',ldmbase)\n",
        "  from ldm.modules.diffusionmodules.openaimodel import UNetModel\n",
        "\n",
        "  in_chn=4\n",
        "  if INP:\n",
        "    in_chn=8\n",
        "  sdt_func=None\n",
        "  if sd_modify == 1:\n",
        "    sdt_func=add_kvbias\n",
        "    from ldm.modules.attention import CrossAttention_config\n",
        "    CrossAttention_config.kvbias=True\n",
        "\n",
        "\n",
        "  with init_empty_weights():\n",
        "    ldm_unet = UNetModel(\n",
        "        image_size=32,\n",
        "        in_channels=in_chn,out_channels=4,\n",
        "            model_channels=320,\n",
        "            attention_resolutions=[4,2,1],\n",
        "            num_res_blocks=2,\n",
        "            channel_mult=[1,2,4,4],\n",
        "            num_heads=8,\n",
        "            use_spatial_transformer=True,\n",
        "            context_dim=768,\n",
        "            legacy= False).requires_grad_(False)\n",
        "  load_state_dict_with_low_memory(ldm_unet,mkmodel_state_dict(),modifyfunc=sdt_func)\n",
        "  ldm_unet=ldm_unet.eval().to(cudev)\n",
        "  return ldm_unet\n",
        "\n",
        "def clamp64(n):\n",
        "  ret=n>>3\n",
        "  lez=ret&7\n",
        "  ret-=lez\n",
        "  if lez >3:\n",
        "    ret+=8\n",
        "  return ret\n",
        "\n",
        "def mk_shape():\n",
        "  shape = [n_samples, 4, clamp64(H) , clamp64(W) ]\n",
        "  nl=len(seed_size)\n",
        "  if nl> 0:\n",
        "    dst =[seed_size[-1]]+shape[2:]\n",
        "    shape[2]=clamp64(seed_size[0])\n",
        "    shape[3]=clamp64(seed_size[1])\n",
        "\n",
        "    if nl > 3:\n",
        "      ksd=seed_size[2:-1]\n",
        "      nl_2=(nl-3)//3\n",
        "      for n in range(nl_2):\n",
        "        ksd[3*n+1]=clamp64(ksd[3*n+1])\n",
        "        ksd[3*n+2]=clamp64(ksd[3*n+2])\n",
        "      shape=shape+ksd+dst\n",
        "    else:\n",
        "      shape=shape+dst\n",
        "\n",
        "  return shape\n",
        "\n",
        "class Insertor:\n",
        "  def __init__(self, string):\n",
        "    self.rpla=string+'}'\n",
        "    self.rpla_cut=len(string)+1\n",
        "    varias=mkInsertor_pstz(string)\n",
        "    ll=len(varias)\n",
        "    self.cplxlv=-1\n",
        "\n",
        "    idkole=[None]*ll\n",
        "    for n in range(ll):\n",
        "      dikv=set()\n",
        "      vaa=varias[n]\n",
        "      for u in vaa:\n",
        "        vyd=u.id\n",
        "        if vyd != 0:\n",
        "          dikv.add(vyd)\n",
        "      idkole[n]=dikv\n",
        "\n",
        "    self.ids=idkole\n",
        "    self.varias=varias\n",
        "    self.ll=ll\n",
        "    \n",
        "  def cplxLevel(self,n):\n",
        "    if n < 0:\n",
        "      cplx=0\n",
        "      if self.cplxlv>=0:\n",
        "        return self.cplxlv\n",
        "      for i in range(self.ll):\n",
        "        cplx+=self.cplxLevel(i)\n",
        "      self.cplxlv=cplx\n",
        "      return cplx\n",
        "    if self.ids[n]:\n",
        "      return len(self.ids[n])*0x1000\n",
        "    return 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# encoder\n",
        "class BERTEmbedder:\n",
        "    def __init__(self, transformer, max_length=77):\n",
        "        self.tokenizer = CLIPTokenizer.from_pretrained('openai/clip-vit-large-patch14')\n",
        "        self.max_length = max_length\n",
        "        self.dedup=dict()\n",
        "\n",
        "        self.transformer = transformer\n",
        "        self.embedding = torch.nn.Embedding.from_pretrained(self.transformer.state_dict()['text_model_embeddings_token_embedding.weight'])\n",
        "        self.encode = self.encode0\n",
        "\n",
        "        emptytok=self.tok('')\n",
        "        self.tok_bos, self.tok_eos = int(emptytok[0]), int(emptytok[1])\n",
        "        emptyemb=self.amb(emptytok)\n",
        "        self.emb_bos, self.emb_eos = emptyemb[0].unsqueeze(0) ,emptyemb[1]\n",
        "        \n",
        "\n",
        "    def insert(self,inz):\n",
        "      self.dedup[inz]=torch.tensor(np.fromfile('UserEmb/'+inz[1:-1]+'.bin',dtype=np.float32).reshape(-1,768))\n",
        "\n",
        "    def insert_prompt_vars(self,inz):\n",
        "      inz='{'+inz\n",
        "      self.dedup[inz]=Insertor(inz)\n",
        "      \n",
        "    def get_empty(self):\n",
        "      return torch.cat([self.emb_bos,self.emb_eos.expand(self.max_length-1,-1) ])\n",
        "      \n",
        "\n",
        "    \n",
        "\n",
        "    def tok(self, text, pad=False):\n",
        "      padstr='do_not_pad'\n",
        "      if pad:\n",
        "        padstr='max_length'\n",
        "      batch_encoding = self.tokenizer(text, truncation=True, max_length=self.max_length, return_length=True,\n",
        "                            return_overflowing_tokens=False, padding=padstr, return_tensors='pt')\n",
        "\n",
        "      return batch_encoding['input_ids'][0]\n",
        "\n",
        "    def amb(self, tokens):\n",
        "        return self.embedding(tokens)\n",
        "        \n",
        "    def mk_emb_wgt(self,unit_arr,dtal=-1):\n",
        "      if dtal < 0:\n",
        "        dtal=len(unit_arr)\n",
        "      \n",
        "      emb=[None]*(dtal+2)\n",
        "      wgt=[None]*(dtal+2)\n",
        "      txt=[]\n",
        "      emb[0]=self.emb_bos\n",
        "      wgt[0]=torch.ones(1)\n",
        "      count=self.max_length-1\n",
        "      NoWgt=True\n",
        "      for i in range(dtal):\n",
        "        emb0,wgt0=unit_arr[i].emb_wgt()\n",
        "        if wgt0[0] != 1.0:\n",
        "          NoWgt=False\n",
        "        msg = unit_arr[i].msg\n",
        "        if len(msg) > 1:\n",
        "          txt.append(msg)\n",
        "        emb[i+1]=emb0\n",
        "        wgt[i+1]=wgt0\n",
        "        count-=wgt0.size(0)\n",
        "        if count <= 0:\n",
        "          kcut=count-1\n",
        "          emb[i+1]=emb0[:kcut]\n",
        "          wgt[i+1]=wgt0[:kcut]\n",
        "          emb=emb[:i+3]\n",
        "          wgt=wgt[:i+3]\n",
        "          count=1\n",
        "          print('ignore after: '+msg)\n",
        "          break\n",
        "      \n",
        "      \n",
        "      emb[-1]=self.emb_eos.expand(count,-1)\n",
        "      if NoWgt:\n",
        "        wgt=None\n",
        "      else:\n",
        "        wgt[-1]=wgt[0].expand(count)\n",
        "      emb=torch.cat(emb)\n",
        "      \n",
        "      \n",
        "\n",
        "      #wgt=torch.cat(wgt)\n",
        "      if txt:\n",
        "        if len(txt) > 1:\n",
        "          txt=' # '.join(txt)\n",
        "        else:\n",
        "          txt=txt[0]\n",
        "      else:\n",
        "        txt=None\n",
        "      return emb, wgt, txt\n",
        "\n",
        "    def from_emb(self,emb0,wgt_arr=None,nsamp=1,cuda=True):\n",
        "      z = self.transformer( emb0.expand(1,-1,-1) )\n",
        "      if cuda:\n",
        "        z=z.cuda()\n",
        "      if wgt_arr is not None:\n",
        "        wgt=torch.cat(wgt_arr)\n",
        "        if cuda:\n",
        "          wgt=wgt.cuda()\n",
        "        ynt=z[:,0,:]\n",
        "        wgt /= torch.abs(wgt.mean())\n",
        "        z*=wgt.reshape(-1,1).expand(1,-1,-1)\n",
        "        z[:,0,:]=ynt\n",
        "      if nsamp > 1:\n",
        "        z=z.expand(nsamp,-1,-1)\n",
        "      return z\n",
        "\n",
        "    def encode0(self, text, nsamp):\n",
        "\n",
        "      if len(text) == 0:\n",
        "        return cond_getter(None)\n",
        "\n",
        "      units=pmpmtx_preproc([text],enable3d=False)[0]\n",
        "        \n",
        "      emb, wgt, txt = self.mk_emb_wgt(units,len(units))\n",
        "        \n",
        "\n",
        "      return cond_getter(emb,fast=0,nsamp=nsamp)\n",
        "\n",
        "    def encode2(self, text, nsamp):\n",
        "        batch_encoding = self.tokenizer(text, truncation=True, max_length=self.max_length, return_length=True,\n",
        "                            return_overflowing_tokens=False, padding=\"max_length\", return_tensors=\"pt\")\n",
        "        \n",
        "\n",
        "        return self.transformer( self.embedding( batch_encoding[\"input_ids\"].expand(nsamp,-1) ) )\n",
        "\n",
        "def loadKV_merge(pt):\n",
        "  state_dict=mkPreKV_dict(pt)\n",
        "  load_state_dict_with_low_memory(ldm_unet, state_dict,modifyfunc=None,fill=False)\n",
        "\n",
        "\n",
        "def loadKV(pt):\n",
        "  if EnableKVmerges:\n",
        "    loadKV_merge(pt)\n",
        "    return\n",
        "  wgt_k,wgt_v=torch.load(pt)[768]\n",
        "  load_state_dict_with_low_memory(preprocK,wgt_k)\n",
        "  load_state_dict_with_low_memory(preprocV,wgt_v)\n",
        "  preprocK.use=True\n",
        "  preprocV.use=True\n",
        "\n",
        "\n",
        "class CompVisDenoiser(DiscreteEpsDDPMDenoiser):\n",
        "    \"\"\"A wrapper for CompVis diffusion models.\"\"\"\n",
        "\n",
        "    def __init__(self, model, quantize=False, device='cpu'):\n",
        "        super().__init__(model, model.alphas_cumprod, quantize=quantize)\n",
        "\n",
        "    def get_eps(self, *args, **kwargs):\n",
        "        return apply_model(*args, **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def nDfmCodeBase():\n",
        "  if DfmCodeBase == 'JIT':\n",
        "    return 0\n",
        "  elif DfmCodeBase == 'ldm_SaveVram':\n",
        "    return 1\n",
        "  elif DfmCodeBase == 'ldm_xformers':\n",
        "    return 2\n",
        "  return 99\n",
        "\n",
        "\n",
        "class PreKV(nn.Module):\n",
        "    logic_multiplier = 1.0\n",
        "    def __init__(self, dim, heads=0):\n",
        "        super().__init__()\n",
        "        self.use=False\n",
        "        self.linear1 = torch.nn.Linear(dim, dim*2)\n",
        "        self.linear2 = torch.nn.Linear(dim*2, dim)\n",
        "\n",
        "    def forward(self, _x):\n",
        "        return _x + (self.linear2(self.linear1(_x)) * PreKV.logic_multiplier)\n",
        "\n",
        "\n",
        "class CFGDenoiser(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.inner_model = model\n",
        "\n",
        "    def forward(self, d, x, sigma, uncond, cond, cond_scale):\n",
        "        x_in = torch.cat([x] * 2)\n",
        "        sigma_in = torch.cat([sigma] * 2)\n",
        "        cond_in = torch.cat([uncond.get(d) , cond.get(d) ])\n",
        "        uncond, cond = self.inner_model(x_in, sigma_in, cond=cond_in,d=d).chunk(2)\n",
        "        return uncond + (cond - uncond) * cond_scale\n",
        "\n",
        "\n",
        "class SRDenoiser(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.inner_model = model\n",
        "\n",
        "    def forward(self, x, sigma, cond ):\n",
        "        cond = self.inner_model(x, sigma, cond=cond)\n",
        "        return cond\n",
        "\n",
        "prevSDver=''\n",
        "prevDfmCodeBase=''\n",
        "\n",
        "class CompVisJIT():\n",
        "  def __init__(self):\n",
        "    self.alphas_cumprod=torch.tensor(alphas_cumprod,device=cudev)\n",
        "    self.apply_model=apply_model\n",
        "\n",
        "class ifeeder():\n",
        "  def __init__(self):\n",
        "    self.getn=self.get_simp\n",
        "  def get_simp(self,n):\n",
        "    return self.bs\n",
        "  def setbs(self,in_bs):\n",
        "    self.bs=in_bs\n",
        "  \n",
        "  def get_npbins(self,n):\n",
        "    return torch.tensor(np.fromfile(self.pattern%(n+1),dtype=np.float32).reshape(self.shape),device=cudev)+self.noiseadd\n",
        "\n",
        "Karras=False\n",
        "model_wrap=None"
      ],
      "metadata": {
        "id": "YzmPpU9reaMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class vinfo:\n",
        "  def __init__(self, txt):\n",
        "    self.tag=txt\n",
        "    valid=False\n",
        "    cut=-1\n",
        "    if txt[-3] == ',':\n",
        "      cut=-3\n",
        "    key=txt[:cut]\n",
        "    if key in cond_stage_model.dedup:\n",
        "      valid=True\n",
        "      self.bazkey=key\n",
        "      ActivedPromptVars[txt]=cond_stage_model.dedup[key]\n",
        "    self.valid=valid\n",
        "\n",
        "\n",
        "\n",
        "  @property\n",
        "  def baz(self):\n",
        "    return cond_stage_model.dedup[self.bazkey]\n",
        "\n",
        "  def repl(self, unit, v):\n",
        "    inzt=copy.deepcopy(self.baz.varias[v])\n",
        "    idset=self.baz.ids[v]\n",
        "    _, p_wgt, p_sta, p_end = unit.nfo()\n",
        "\n",
        "    idmap=dict()\n",
        "    procid=False\n",
        "    if idset:\n",
        "      procid=True\n",
        "      for id in idset:\n",
        "        idmap[id]=rdmIDfunc(None)\n",
        "\n",
        "    if procid:\n",
        "      for yn in inzt:\n",
        "        if yn.id == 0:\n",
        "          yn.update_sta_end(p_wgt,p_sta,p_end)\n",
        "        else:\n",
        "          yn.id=idmap[yn.id]\n",
        "          erz=yn.eraz\n",
        "          if erz:\n",
        "            nyu_erz=dict()\n",
        "            for k in erz:\n",
        "              nyu_id=idmap[k]\n",
        "              nyu_erz[nyu_id]=nyu_id\n",
        "            yn.eraz=nyu_erz\n",
        "    else:\n",
        "      for yn in inzt:\n",
        "        yn.update_sta_end(p_wgt,p_sta,p_end)\n",
        "\n",
        "       \n",
        "    return inzt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class sentUnit:\n",
        "  def __init__(self, txt,\n",
        "               fast=-1,\n",
        "               p_wgt=None,p_sta=None,p_end=None\n",
        "               ):\n",
        "\n",
        "    self.emb_wgt = self.emb_wgt0\n",
        "    self.id=0\n",
        "    usig=0\n",
        "    self.repls=dict()\n",
        "    self.eraz=None\n",
        "    if fast == 0:\n",
        "      self.sig=usig\n",
        "      self.msg, self.wgt, self.upper, self.lower =txt,p_wgt,p_sta,p_end\n",
        "      return\n",
        "    elif fast == 1:\n",
        "      self.id=rdmIDfunc(txt) #id(self)#\n",
        "      self.sig=usig\n",
        "      self.msg, self.wgt, self.upper, self.lower =txt,0,p_sta,p_end\n",
        "      #wgt as group_len\n",
        "      self.eraz=dict()\n",
        "      self.yetproc=True\n",
        "      return\n",
        "\n",
        "    \n",
        "    \n",
        "    prepand=None\n",
        "    retThis=True\n",
        "    self.real_return=[]\n",
        "    if ':' in txt:\n",
        "      usig+=0x100\n",
        "    if '+' in txt:\n",
        "      usig+=0x200\n",
        "    if ';' in txt:\n",
        "      usig+=0x400\n",
        "    if '|' in txt:\n",
        "      usig+=0x800\n",
        "    \n",
        "    self.sig=usig\n",
        "\n",
        "    if usig < 0x100:\n",
        "      self.msg, self.wgt, self.upper, self.lower = txt,p_wgt,p_sta,p_end\n",
        "      self.real_return=[self]\n",
        "    else:\n",
        "      taps=mktaps(txt,  p_wgt=p_wgt,p_sta=p_sta,p_end=p_end)\n",
        "      msg, self.wgt, self.upper, self.lower=taps[0]\n",
        "      if '|' in msg:\n",
        "        self.msg='^'\n",
        "        retThis=False\n",
        "        taps2=mktaps(msg, sep='|', p_wgt=self.wgt,p_sta=self.upper,p_end=self.lower)\n",
        "        for m2,w2,s2,e2 in taps2:\n",
        "          dmm=sentUnit(m2,p_wgt=w2,p_sta=s2,p_end=e2)\n",
        "          for dmm2 in get_real_return(dmm):\n",
        "            self.real_return.append(dmm2)\n",
        "      else:\n",
        "        self.msg=msg\n",
        "        self.real_return=[self]\n",
        "      \n",
        "      len_taps=len(taps)\n",
        "      \n",
        "      if len_taps > 1:\n",
        "        prepand=sentUnit('[',fast=1, p_sta=self.upper,p_end=self.lower )\n",
        "        lazt_id=prepand.id\n",
        "        eraz_arr=[prepand]\n",
        "        for i in range(1,len_taps):\n",
        "          msg,wgt_bs,sta_bs,endo_bs = taps[i]\n",
        "          edb=sentUnit(']',fast=1)\n",
        "          edb.id=lazt_id\n",
        "          pp2=sentUnit('[',fast=1,p_sta=sta_bs,p_end=endo_bs)\n",
        "          lazt_id=pp2.id\n",
        "          if '|' in msg:\n",
        "            pral=[edb,pp2]\n",
        "            taps2=mktaps(msg, sep='|', p_wgt=wgt_bs,p_sta=sta_bs,p_end=endo_bs)\n",
        "            for m2,w2,s2,e2 in taps2:\n",
        "              dmm=sentUnit(m2,p_wgt=w2,p_sta=s2,p_end=e2)\n",
        "              for dmm2 in get_real_return(dmm):\n",
        "                pral.append(dmm2) \n",
        "            self.real_return+=pral\n",
        "\n",
        "          else:\n",
        "            dmm=sentUnit(msg,p_wgt=wgt_bs, p_sta=sta_bs,p_end=endo_bs)\n",
        "            dmm_rt=get_real_return(dmm)\n",
        "            self.real_return+=[edb,pp2]+dmm_rt\n",
        "          \n",
        "          for erz in eraz_arr:\n",
        "            erz.eraz[lazt_id]=lazt_id\n",
        "          eraz_arr.append(pp2)\n",
        "        edb=sentUnit(']',fast=1)\n",
        "        edb.id=lazt_id\n",
        "        self.real_return.append(edb)\n",
        "            \n",
        "\n",
        "    if retThis:\n",
        "      self.real_return=emb_and_v(self.msg,p_wgt=self.wgt,p_sta=self.upper,p_end=self.lower)+self.real_return[1:]\n",
        "      \n",
        "   \n",
        "\n",
        "    if prepand is not None:\n",
        "      self.real_return=[prepand]+self.real_return\n",
        " \n",
        "  def nfo(self,trans_wgt=True,trans_sta=False,trans_end=False,extra=False):\n",
        "    ret_wgt=self.wgt\n",
        "    ret_sta=self.upper\n",
        "    ret_end=self.lower\n",
        "    if trans_wgt and ret_wgt is None:\n",
        "      ret_wgt=1.0\n",
        "    if trans_sta and ret_sta is None:\n",
        "      ret_sta=0\n",
        "    if trans_end and ret_end is None:\n",
        "      ret_end=1.0\n",
        "    if extra:\n",
        "      return self.msg, ret_wgt, ret_sta, ret_end,[self.wgt is None,self.upper is None,self.lower is None]\n",
        "    else:\n",
        "      return self.msg, ret_wgt, ret_sta, ret_end\n",
        "\n",
        "  def get_sig(self):\n",
        "    ret=0\n",
        "    if self.eraz:\n",
        "      ret = 0x1000\n",
        "    if self.upper is not None:\n",
        "      return ret + 0x100\n",
        "    if self.lower is not None:\n",
        "      return ret + 0x100\n",
        "    return ret\n",
        "\n",
        "  def get_realstaend(self):\n",
        "    sta=0\n",
        "    endo=t_enc\n",
        "    if self.upper is not None:\n",
        "      sta=int(self.upper*t_enc +0.5)\n",
        "    if self.lower is not None:\n",
        "      endo=int(self.lower*t_enc +0.5)\n",
        "    return sta, endo\n",
        "\n",
        "  def get_realwgt(self):\n",
        "    if self.wgt is None:\n",
        "      return 1.0\n",
        "    return self.wgt\n",
        "\n",
        "\n",
        "  def set_emb(self,n):\n",
        "    if n == 1:\n",
        "      self.tok_len=1\n",
        "      self.emb_wgt = self.emb_wgt1\n",
        "      self.fast_emb= cond_stage_model.dedup[self.msg]\n",
        "      self.fast_tkl=self.fast_emb.size(0)\n",
        "\n",
        "\n",
        "  def emb_wgt1(self):\n",
        "    wgg=torch.ones(self.fast_tkl)\n",
        "    if self.wgt is not None:\n",
        "      wgg*=self.wgt\n",
        "\n",
        "    return self.fast_emb, wgg\n",
        "  def emb_wgt0(self):\n",
        "    tok=cond_stage_model.tok(self.msg)[1:-1]\n",
        "    tkl=tok.size(0)\n",
        "    self.tok_len=tkl\n",
        "    wgg=torch.ones(tkl)\n",
        "    if self.wgt is not None:\n",
        "      wgg*=self.wgt\n",
        "    amb = cond_stage_model.embedding(tok)\n",
        "    return amb,wgg\n",
        "\n",
        "  def update_sta_end(self, wgt, sta, endo):\n",
        "    if self.wgt is None:\n",
        "      self.wgt=wgt\n",
        "    if self.upper is None:\n",
        "      self.upper=sta\n",
        "    if self.lower is None:\n",
        "      self.lower=endo\n",
        "\n",
        "  def __repr__(self):\n",
        "    ret=stringlizeNfo(self)\n",
        "    if self.eraz:\n",
        "      ret+='\\n'+str(self.eraz)\n",
        "    return ret\n",
        "\n",
        "\n",
        "# arr=emb\n",
        "class cond_getter:\n",
        "  def __init__(self, arr, wgt_arr=None, reftxt=None, kndref=None, fast=-1, nsamp=1,cuda=True):\n",
        "    self.txt=[]\n",
        "    if reftxt is not None:\n",
        "      self.txt=reftxt\n",
        "\n",
        "    self.notSave=False\n",
        "    self.add_sta=0\n",
        "    self.d_sta=0\n",
        "    self.is_simp=True\n",
        "    self.get=self.get_simp\n",
        "    self.get_txt=self.txt_simp\n",
        "    if arr is None:\n",
        "      emb = cond_stage_model.get_empty()\n",
        "      self.arr = cond_stage_model.from_emb(emb,nsamp=nsamp,cuda=cuda)\n",
        "      return\n",
        "    if fast==0:\n",
        "      self.arr = cond_stage_model.from_emb(arr,wgt_arr=wgt_arr,nsamp=nsamp,cuda=cuda)\n",
        "      return\n",
        "    elif fast == 1:\n",
        "      self.arr=arr\n",
        "      return\n",
        "  \n",
        "    self.knd=kndref\n",
        "    self.is_simp=False\n",
        "    arr.append(arr[-1])\n",
        "    self.arr=arr\n",
        "    self.get=self.get_arr\n",
        "    self.get_txt=self.txt_arr\n",
        "      \n",
        "  def get_knd(self):\n",
        "    if self.is_simp:\n",
        "      return np.ones(t_enc,dtype=np.uint8)*0xff\n",
        "    return self.knd\n",
        "\n",
        "  def get_fullarr(self):\n",
        "    if self.is_simp:\n",
        "      return [self.arr]*t_enc\n",
        "    return self.arr\n",
        "\n",
        "\n",
        "  def get_simp(self,d):\n",
        "    return self.arr\n",
        "\n",
        "  def reset(self):\n",
        "    self.add_sta=0\n",
        "    self.d_sta=0\n",
        "\n",
        "\n",
        "  def txt_simp(self,d):\n",
        "    return self.txt\n",
        "  \n",
        "  def get_arr(self,d):\n",
        "    sd=d+self.add_sta\n",
        "    if self.d_sta > 1:\n",
        "      sd=int(0.5+d*self.d_sta)\n",
        "    return self.arr[sd]\n",
        "\n",
        "  def txt_arr(self,d):\n",
        "    return self.txt[d]\n",
        "\n",
        "  def save(self,pname='prmt'):\n",
        "    if self.notSave:\n",
        "      return None\n",
        "    sv=dict()\n",
        "    if self.is_simp:\n",
        "      sv[0]=True\n",
        "      savarr=self.arr[0]\n",
        "      dfarr=None\n",
        "    else:\n",
        "      sv[0]=False\n",
        "      sv[2]=self.knd\n",
        "      savarr, dfarr = kndmax_diff(self.knd)\n",
        "      knd_l=len(savarr)\n",
        "      for i in range(knd_l):\n",
        "        savarr[i]=self.arr[ savarr[i] ][0]\n",
        "\n",
        "    sv[1]=savarr\n",
        "    if self.txt is not None:\n",
        "      pname+=self.get_txt(0)[:20]\n",
        "    pname+='.compiled_prompt'\n",
        "    torch.save(sv,pname)\n",
        "    return dfarr\n",
        "\n",
        "\n",
        "\n",
        "  def load(self,nsamp=1,cuda=True):\n",
        "    sv=torch.load(self.arr[0],map_location=cudev)\n",
        "    simp=sv[0]\n",
        "    self.notSave=True\n",
        "    \n",
        "    self.get_txt=self.txt_simp\n",
        "    self.txt='===secret==='\n",
        "    if simp:\n",
        "      self.get=self.get_simp\n",
        "      self.arr=sv[1].expand(nsamp,-1,-1)\n",
        "    else:\n",
        "      self.knd=sv[2]\n",
        "      self.get=self.get_arr\n",
        "      karr=sv[1]\n",
        "      knd_l=len(karr)\n",
        "      for i in range(knd_l):\n",
        "        z=karr[i]\n",
        "        if cuda:\n",
        "          z=z.cuda()\n",
        "        else:\n",
        "          z=z.cpu()\n",
        "        karr[i]=z.expand(nsamp,-1,-1)\n",
        "\n",
        "      knduse = resizeknd(self.knd)\n",
        "      arr=[None]*t_enc\n",
        "      for i in range(t_enc):\n",
        "        arr[i]=karr[knduse[i]]\n",
        "      arr.append(arr[-1])\n",
        "      self.knd=knduse\n",
        "      self.arr=arr\n",
        "    self.is_simp=simp\n",
        "\n"
      ],
      "metadata": {
        "id": "xvbB9o3b9nrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_real_return(unit):\n",
        "  grr=unit.real_return\n",
        "  del unit.real_return\n",
        "  return grr\n",
        "\n",
        "def rdmIDfunc(yd):\n",
        "  #print(yd[:2])\n",
        "  return random.randint(0, 2**32)\n",
        "\n",
        "\n",
        "def resizeknd(knd):\n",
        "  ldl=len(knd)\n",
        "  jd_sta=0\n",
        "  if ldl > t_enc:\n",
        "    jd_sta=ldl/t_enc\n",
        "    knd2=knd\n",
        "  elif ldl < t_enc:\n",
        "    rpt=int(0.9999+t_enc/ldl)\n",
        "    knd2=knd.repeat( rpt )\n",
        "    jd_sta=ldl*rpt/t_enc\n",
        "  knduse=knd\n",
        "  if jd_sta!=0:\n",
        "    knduse=[None]*(t_enc+1)\n",
        "    for d in range(t_enc):\n",
        "      sd=int(0.5+d*jd_sta)\n",
        "      knduse[d]=knd2[sd]\n",
        "    knduse=knduse[:-1]\n",
        "  return knduse\n",
        "\n",
        "\n",
        "def kndmax_diff(knd):\n",
        "  curknd=knd[0]\n",
        "  dfarr=[]\n",
        "  knd_l=len(knd)\n",
        "  kndict=dict()\n",
        "  for i in range(knd_l):\n",
        "    wua=knd[i]\n",
        "    if wua != curknd:\n",
        "      curknd=knd[i]\n",
        "      dfarr.append(i)\n",
        "    kndict[wua]=i\n",
        "  revknd=[]\n",
        "  for i in range(knd_l):\n",
        "    if i in kndict:\n",
        "      revknd.append(kndict[i])\n",
        "    else:\n",
        "      break\n",
        "  return revknd, dfarr\n",
        "\n",
        "\n",
        "def stringlizeNfo(src):\n",
        "  msg,wgt,sta,endo = src.nfo()\n",
        "  if wgt != 1.0:\n",
        "    msg+='+'+str(wgt)\n",
        "  sig=0\n",
        "  if sta is not None:\n",
        "    sig+=1\n",
        "  if endo is not None:\n",
        "    sig+=2\n",
        "\n",
        "  if sig == 0:\n",
        "    return msg\n",
        "  elif sig==1:\n",
        "    return msg+':'+str(int(0.5+sta*100))+':'\n",
        "  elif sig==2:\n",
        "    return msg+'::'+str(int(0.5+endo*100))\n",
        "  elif sig==3:\n",
        "    return msg+':'+str(int(0.5+sta*100))+':'+str(int(0.5+endo*100))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def mkInsertor_pstz(string):\n",
        "  fna = 'UserEmb/'+string[1:]+'.txt'\n",
        "  with open(fna,'rt') as f:\n",
        "    stz=f.read().splitlines()\n",
        "  stz=('@'.join(stz)).replace('@@','^').split('^')\n",
        "  stz_l=len(stz)\n",
        "  stz_n=[]\n",
        "  for i in range(stz_l):\n",
        "    txt=stz[i]\n",
        "    if txt[0] == '#':\n",
        "      continue\n",
        "    if '@' in txt:\n",
        "      stz2=txt.split('@')\n",
        "      arr=[]\n",
        "      for s in stz2:\n",
        "        arr+=get_real_return(sentUnit(s))\n",
        "      stz_n.append(arr)\n",
        "    else:\n",
        "      stz_n.append( get_real_return(sentUnit(stz[i])) )\n",
        "  return stz_n\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def i2t(strr, ifempty=None):\n",
        "  if strr:\n",
        "    f = float(strr)\n",
        "    if f > 1:\n",
        "      f/=100 \n",
        "    return f\n",
        "  return ifempty\n",
        "\n",
        "def m2mw(strr,prev,p_wgt):\n",
        "  wgt=p_wgt\n",
        "  spl=strr.split('+')\n",
        "  if len(spl)>1:\n",
        "    wgt=float(spl[1])\n",
        "    strr=spl[0]\n",
        "    if strr == '':\n",
        "      strr=prev\n",
        "  return strr,wgt\n",
        "\n",
        "\n",
        "InfoChrs='1234567890+-:. '\n",
        "def findposiblesplit(str_in):\n",
        "  lstr=len(str_in)-1\n",
        "  for n in range(lstr,-1,-1):\n",
        "    if str_in[n] not in InfoChrs:\n",
        "      return n-lstr\n",
        "  return 0\n",
        "\n",
        "def mktaps(str,sep=';',p_wgt=None,p_sta=None,p_end=None):\n",
        "  Enbale_s_in_s = True\n",
        "  if sep != ';':\n",
        "    Enbale_s_in_s=False\n",
        "  segs=str.split(sep)\n",
        "  if len(segs[-1]) == 0:\n",
        "    segs=segs[:-1]\n",
        "  if len(segs[0]) == 0:\n",
        "    segs=segs[1:]\n",
        "  prevstr=''\n",
        "  ret=[]\n",
        "  for s in segs:\n",
        "    sta=p_sta\n",
        "    endo=p_end\n",
        "    repl_msg=None\n",
        "    info_s=s\n",
        "    s_in_s=False\n",
        "    if Enbale_s_in_s and '|' in s:\n",
        "      s_in_s=True\n",
        "      idx=findposiblesplit(s)\n",
        "      if idx == 0:\n",
        "        info_s = 'dummy'\n",
        "        repl_msg=s\n",
        "      else:\n",
        "        info_s = 'dummy'+s[idx:]\n",
        "        repl_msg = s[:idx]\n",
        "\n",
        "    msg=info_s.split(':')\n",
        "    if len(msg) > 2:\n",
        "      sta=i2t(msg[1],p_sta)\n",
        "      endo=i2t(msg[2],p_end)\n",
        "    msg, wgt=m2mw(msg[0],prevstr,p_wgt)\n",
        "    if s_in_s:\n",
        "      msg=repl_msg+msg[5:]\n",
        "\n",
        "    msg=msg.strip()\n",
        "    prevstr=msg\n",
        "    ret.append((msg,wgt,sta,endo))\n",
        "  return ret\n",
        "\n",
        "def m2unit(data,dtal,mtx):\n",
        "  ret=[]\n",
        "  for i in range(dtal):\n",
        "    if mtx[i] !=0xff:\n",
        "      ret.append(data[i])\n",
        "  return ret\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def chkrealexist(key,src_n):\n",
        "  if not src_n.repls:\n",
        "    return False\n",
        "  if key in src_n.repls:\n",
        "    return True\n",
        "\n",
        "  return False\n",
        "\n",
        "def vintzproc(src,k,v):\n",
        "  dtal=len(src)\n",
        "  for n in range(dtal):\n",
        "    if chkrealexist(k,src[n]):\n",
        "      vinfo=src[n].repls[k]\n",
        "      brd=vinfo.repl(src[n],v)\n",
        "      if len(brd) == 1:\n",
        "        src[n]=brd[0]\n",
        "      else:\n",
        "        src=src[:n]+brd+src[n+1:]\n",
        "  return src\n",
        "\n",
        "def recurflatten(seed,key_list):\n",
        "  k=key_list[-1]\n",
        "  n_pl=ActivedPromptVars[k].ll\n",
        "  n_seed=len(seed)\n",
        "  newseed=[]\n",
        "  for i in range(n_seed):\n",
        "    for v in range(n_pl):\n",
        "      src=copy.deepcopy(seed[i])\n",
        "      newseed.append( vintzproc(src,k,v) )\n",
        "  if len(key_list) > 1:\n",
        "    return recurflatten(newseed,key_list[:-1])\n",
        "  else:\n",
        "    return newseed\n",
        "\n",
        "def ActivedPromptVarsByCplx():\n",
        "  key_list=list(ActivedPromptVars.keys())\n",
        "  kl=len(key_list)\n",
        "  for n in range(kl):\n",
        "    key=key_list[n]\n",
        "    key_list[n]=('%08X'%ActivedPromptVars[key].cplxLevel(-1))+key\n",
        "  key_list.sort()\n",
        "  for n in range(kl):\n",
        "    key_list[n]=key_list[n][8:]\n",
        "  return key_list\n",
        "\n",
        "\n",
        "def proc3d(data):\n",
        "  key_list=ActivedPromptVarsByCplx()\n",
        "  arr= recurflatten([data],key_list)\n",
        "  arrl=len(arr)\n",
        "  for i in range(arrl):\n",
        "    arr[i]=trimgroup(arr[i])\n",
        "  return arr\n",
        "\n",
        "\n",
        "def proc1d(data):\n",
        "  return [trimgroup(data)]\n",
        "\n",
        "\n",
        "def trymakeemb(tag):\n",
        "  if tag in cond_stage_model.dedup:\n",
        "    return True\n",
        "  if os.path.isfile('UserEmb/'+tag[1:-1]+'.bin'):\n",
        "    cond_stage_model.insert(tag)\n",
        "    return True\n",
        "  return False\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def dfind_emb(txt,poz,l,p_wgt,p_sta,p_end):\n",
        "  i=poz\n",
        "  while i < l:\n",
        "    c=txt[i]\n",
        "    i+=1\n",
        "    if c == '>':\n",
        "      sig=txt[poz-1:i]\n",
        "      unit = sentUnit(sig,fast=0,p_wgt=p_wgt,p_sta=p_sta,p_end=p_end)\n",
        "      valid=trymakeemb(sig)\n",
        "      if valid:\n",
        "        unit.set_emb(1)\n",
        "      else:\n",
        "        unit.wgt=-333\n",
        "        unit.msg=sig[1:-1]\n",
        "      return unit, 0 ,i\n",
        "    \n",
        "\n",
        "def dfind_v(txt,poz,l,p_wgt,p_sta,p_end):\n",
        "  i=poz\n",
        "  while i < l:\n",
        "    c=txt[i]\n",
        "    i+=1\n",
        "    if c == '}':\n",
        "      sig=txt[poz-1:i]\n",
        "      unit = sentUnit('}',fast=0,p_wgt=p_wgt,p_sta=p_sta,p_end=p_end)\n",
        "      dmm=vinfo(sig)\n",
        "      if dmm.valid:\n",
        "        unit.repls[sig]=dmm\n",
        "      else:\n",
        "        unit.wgt=-333\n",
        "        unit.msg=sig[1:-1]\n",
        "      return unit, 0 ,i\n",
        "\n",
        "def dfind_v_dummy(txt,poz,l,p_wgt,p_sta,p_end):\n",
        "  i=poz\n",
        "  while i < l:\n",
        "    c=txt[i]\n",
        "    i+=1\n",
        "    if c == '}':\n",
        "      sig=txt[poz:i-1]\n",
        "      unit = sentUnit(sig,fast=0,p_wgt=p_wgt,p_sta=p_sta,p_end=p_end)\n",
        "      unit.wgt=-333\n",
        "      return unit, 0 ,i\n",
        "\n",
        "def dfind_head(txt,poz,l,p_wgt,p_sta,p_end):\n",
        "  i=poz\n",
        "  while i < l:\n",
        "    c=txt[i]\n",
        "    i+=1\n",
        "    if c == '<':\n",
        "      if i - poz > 1:\n",
        "        unit= sentUnit(txt[poz:i-1].strip(),fast=0,p_wgt=p_wgt,p_sta=p_sta,p_end=p_end)\n",
        "      else:\n",
        "        unit= sentUnit('empty',fast=0,p_wgt=-666)\n",
        "      return unit, 1 ,i\n",
        "    elif c == '{':\n",
        "      if i - poz > 1:\n",
        "        unit= sentUnit(txt[poz:i-1].strip(),fast=0,p_wgt=p_wgt,p_sta=p_sta,p_end=p_end)\n",
        "      else:\n",
        "        unit= sentUnit('empty',fast=0,p_wgt=-666)\n",
        "      return unit, 2 ,i\n",
        "  \n",
        "  fina=sentUnit(txt[poz:].strip(),fast=0,p_wgt=p_wgt,p_sta=p_sta,p_end=p_end)\n",
        "  fina.wgt=-333\n",
        "  return fina,0,l\n",
        "\n",
        "\n",
        "def canmerge(ret):\n",
        "  if len(ret) == 0:\n",
        "    return False\n",
        "  if len(ret[-1].msg) < 2:\n",
        "    return False\n",
        "  if ret[-1].msg[0] == '<':\n",
        "    return False\n",
        "  return True\n",
        "\n",
        "def emb_and_v(txt,p_wgt=None,p_sta=None,p_end=None,enable3d=True):\n",
        "  l=len(txt)\n",
        "  i=0\n",
        "  functbl=[dfind_head, dfind_emb, dfind_v]\n",
        "  if not enable3d:\n",
        "    functbl[2]=dfind_v_dummy\n",
        "\n",
        "  finderfunc=dfind_head\n",
        "  ret=[]\n",
        "  while i < l:\n",
        "    result, nfunc, i = finderfunc(txt,i,l,p_wgt,p_sta,p_end)\n",
        "    finderfunc=functbl[nfunc]\n",
        "    if result.wgt == -333:\n",
        "      if canmerge(ret):\n",
        "        ret[-1].msg+=' '+result.msg\n",
        "      else:\n",
        "        result.wgt=p_wgt\n",
        "        ret.append(result)\n",
        "    elif result.wgt != -666:\n",
        "      ret.append(result)\n",
        "\n",
        "  return ret\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def dumbunit(txt,wgt):\n",
        "  if wgt == 1:\n",
        "    wgt = None\n",
        "  return emb_and_v(txt,p_wgt=wgt)\n",
        "\n",
        "\n",
        "\n",
        "def filltimeinfo(arr,sta,endo,wgtfix):\n",
        "  if not arr:\n",
        "    return 0\n",
        "  for itm in arr:\n",
        "    itm.wgt+=wgtfix \n",
        "    itm.upper=sta\n",
        "    itm.lower=endo\n",
        "  return 1\n",
        "\n",
        "def pp_edb(sta,endo):\n",
        "  prepand=sentUnit('[',fast=1, p_sta=sta,p_end=endo )\n",
        "  lazt_id=prepand.id\n",
        "  edb=sentUnit(']',fast=1)\n",
        "  edb.id=lazt_id\n",
        "  return [prepand],[edb]\n",
        "\n",
        "def flattenretk(retk):\n",
        "  ret=retk[0]\n",
        "  retkl=len(retk)\n",
        "  if retkl == 2:\n",
        "    ret[0].wgt=float(retk[1][0].msg)\n",
        "  elif retkl > 2:\n",
        "    timeinfo=float(retk[2][0].msg)\n",
        "    hazcot=0\n",
        "    hazcot+=filltimeinfo(ret,None,timeinfo,0.1)\n",
        "    hazcot+=filltimeinfo(retk[1],timeinfo,None,0.1)\n",
        "    if hazcot > 1:\n",
        "      pp, edb = pp_edb(None,timeinfo)\n",
        "      ret=pp+ret+edb\n",
        "      pp, edb = pp_edb(timeinfo,None)\n",
        "      erz_id=pp[0].id\n",
        "      ret[0].eraz[erz_id]=erz_id\n",
        "      retk[1]=pp+retk[1]+edb\n",
        "\n",
        "    ret+=retk[1]\n",
        "\n",
        "  return ret\n",
        "\n",
        "\n",
        "\n",
        "def parsedumbformat(txt,sta=0,l=-1,wgt=1,sqq=False):\n",
        "  cut0=sta\n",
        "  if l < 0:\n",
        "    l=len(txt)\n",
        "  retk=[[]]\n",
        "  ptidx=0\n",
        "\n",
        "\n",
        "  i=sta\n",
        "  while i < l:\n",
        "    c=txt[i]\n",
        "    i+=1\n",
        "    if c == '(':\n",
        "      if i-cut0>1:\n",
        "        retk[ptidx]+=dumbunit(txt[cut0:i-1],wgt)\n",
        "      cut0, ret = parsedumbformat(txt,i,l,wgt+0.1,sqq=True)\n",
        "      i=cut0\n",
        "      retk[ptidx]+=ret\n",
        "    elif c == '[':\n",
        "      if i-cut0>1:\n",
        "        retk[ptidx]+= dumbunit(txt[cut0:i-1],wgt) \n",
        "      cut0, ret = parsedumbformat(txt,i,l,wgt-0.1,sqq=True)\n",
        "      i=cut0\n",
        "      retk[ptidx]+=ret\n",
        "    elif c == ')':\n",
        "      if i-cut0>1:\n",
        "        retk[ptidx]+= dumbunit(txt[cut0:i-1],wgt) \n",
        "      return i,flattenretk(retk)\n",
        "    elif c == ']':\n",
        "      if i-cut0>1:\n",
        "        retk[ptidx]+= dumbunit(txt[cut0:i-1],wgt)\n",
        "        return i,flattenretk(retk)\n",
        "    elif sqq and c == ':':\n",
        "      if i-cut0>1:\n",
        "        retk[ptidx]+= dumbunit(txt[cut0:i-1],wgt) \n",
        "      cut0=i\n",
        "      ptidx+=1\n",
        "      retk.append([])\n",
        "\n",
        "\n",
        "  retk=flattenretk(retk)\n",
        "  if cut0<l:\n",
        "    retk+= dumbunit(txt[cut0:],wgt) \n",
        "  return retk\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def pmpmtx(data_in,nsamp=1,cuda=True,fromtxt=True,enable3d=True):\n",
        "  if len(data_in[0]) == 0:\n",
        "    return [cond_getter(None,nsamp=nsamp,cuda=cuda)]\n",
        "  arr = pmpmtx_preproc(data_in,fromtxt=fromtxt,enable3d=enable3d)\n",
        "\n",
        "  arrl=len(arr)\n",
        "  for c in range(arrl):\n",
        "    arr_for_getter, fastmode,txt, kndref = to_arr_for_getter(arr[c],nsamp=nsamp,cuda=cuda)\n",
        "    arr[c]=cond_getter(arr_for_getter,fast=fastmode,reftxt=txt,kndref=kndref)\n",
        "  return arr\n",
        "\n",
        "\n",
        "def pmpmtx_preproc(data_in,fromtxt=True,enable3d=True):\n",
        "  global ActivedPromptVars\n",
        "  ActivedPromptVars=dict()\n",
        "  arr=[]\n",
        "\n",
        "  if fromtxt:\n",
        "    if len(data_in) == 1:\n",
        "      if '((' in data_in[0]:\n",
        "        arr=parsedumbformat(data_in[0])\n",
        "      else:\n",
        "        arr= emb_and_v(data_in[0], enable3d=enable3d)\n",
        "    else:\n",
        "      for d in data_in:\n",
        "        if d[0] != '#':\n",
        "          arr+=get_real_return(sentUnit(d))\n",
        "  else:\n",
        "    arr=data_in\n",
        "\n",
        "  if enable3d and ActivedPromptVars:\n",
        "    arr=proc3d(arr)\n",
        "  else:\n",
        "    ActivedPromptVars=dict()\n",
        "    arr=proc1d(arr)\n",
        "  return arr\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "def to_arr_for_getter(data,nsamp=1,cuda=True):\n",
        "  dtal=len(data)\n",
        "  cpy_ones=np.ones(t_enc,dtype=np.uint8)\n",
        "  cpy_eraz=cpy_ones*0xff\n",
        "  mtx=np.ones((dtal,t_enc),dtype=np.uint8)\n",
        "\n",
        "  txtid=-1\n",
        "  txtkole=[]\n",
        "  notTime=True\n",
        "  for i in range(dtal):\n",
        "    dta_i=data[i]\n",
        "    sig = dta_i.get_sig()\n",
        "    if sig > 0xFF:\n",
        "      notTime=False\n",
        "      mtx[i]*=0xFF\n",
        "\n",
        "      \n",
        "      sta0, end0 = dta_i.get_realstaend()\n",
        "      \n",
        "      mtx[i][sta0:end0]=cpy_ones[sta0:end0]\n",
        "\n",
        "      if sig > 0xfff:\n",
        "        erazd=dta_i.eraz\n",
        "        for k in erazd:\n",
        "          sta1, end1=erazd[k].get_realstaend()\n",
        "          mtx[i][sta1:end1]=cpy_eraz[sta1:end1]\n",
        "\n",
        "  if notTime:\n",
        "    emb, wgt, txt = cond_stage_model.mk_emb_wgt(data,dtal)\n",
        "    arr = cond_stage_model.from_emb(emb,wgt_arr=wgt,nsamp=nsamp,cuda=cuda)\n",
        "    return  arr, 1, txt, None #arr, fastmode, txt\n",
        "\n",
        "\n",
        "  mtx=mtx.transpose((1,0))\n",
        "  knd=np.ones(t_enc,dtype=np.uint8)\n",
        "  ar2i=dict()\n",
        "  i2txt=[]\n",
        "  txtid=0\n",
        "  for i in range(t_enc):\n",
        "    sig=str(mtx[i].tobytes())[2:-1].replace('\\\\','')\n",
        "    if sig in ar2i:\n",
        "      i_sig=ar2i[sig]\n",
        "    else:\n",
        "      ar2i[sig]=txtid\n",
        "      i2txt.append( m2unit(data,dtal,mtx[i]) )\n",
        "      i_sig=txtid\n",
        "      txtid+=1\n",
        "    knd[i]=i_sig\n",
        "  \n",
        "\n",
        "  if knd.sum() == 0:\n",
        "    emb, wgt, txt = cond_stage_model.mk_emb_wgt(i2txt[0])\n",
        "    arr = cond_stage_model.from_emb(emb,wgt_arr=wgt,nsamp=nsamp,cuda=cuda)\n",
        "    return  arr, 1, txt, None\n",
        "  \n",
        "  knd_arr=[None]*t_enc\n",
        "  knd_arr_txt=[None]*t_enc\n",
        "  enc_l=len(i2txt)\n",
        "\n",
        "  txtk=[None]*enc_l\n",
        "  for i in range(enc_l):\n",
        "    emb, wgt, txt = cond_stage_model.mk_emb_wgt(i2txt[i])\n",
        "    i2txt[i] = cond_stage_model.from_emb(emb,wgt_arr=wgt,nsamp=nsamp,cuda=cuda)\n",
        "    txtk[i]=txt\n",
        "  \n",
        "  for i in range(t_enc):\n",
        "    poo=knd[i]\n",
        "    knd_arr[i]=i2txt[poo]\n",
        "    knd_arr_txt[i]=txtk[poo]\n",
        "\n",
        "  return  knd_arr, -1, knd_arr_txt, knd\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def wgtfix0(wgt):\n",
        "  if wgt is None:\n",
        "    return None\n",
        "  elif wgt > 2:\n",
        "    return 1+0.1*wgt\n",
        "  elif wgt < -2:\n",
        "    return -1+0.1*wgt\n",
        "  else:\n",
        "    return wgt\n",
        "  \n",
        "  \n",
        "def wgtfix(b):\n",
        "  b.wgt=wgtfix0(b.wgt)\n",
        "  return b\n",
        "\n",
        "\n",
        "def trimdpth(dyp):\n",
        "  ret=[]\n",
        "  for i in range(9,-1,-1):\n",
        "    if dyp[i]:\n",
        "      ret+=list(dyp[i])\n",
        "  return ret\n",
        "\n",
        "def trimgroup(unit_arr):\n",
        "  bdict=dict()\n",
        "  stapoz=dict()\n",
        "  \n",
        "\n",
        "  ul=len(unit_arr)\n",
        "  dyp=[]\n",
        "  for i in range(10):\n",
        "    dyp.append(set())\n",
        "  depth=0\n",
        "  clean_ret=[]\n",
        "  for i in range(ul):\n",
        "    b=unit_arr[i]\n",
        "    bmsg=b.msg\n",
        "    if len(bmsg) == 1:\n",
        "      if bmsg == '[':\n",
        "        depth+=1\n",
        "        dyp[depth].add(b.id)\n",
        "        bdict[b.id]=b\n",
        "        stapoz[b.id]=[i+1,None]\n",
        "      elif bmsg == ']':\n",
        "        depth-=1\n",
        "        stapoz[b.id][1]=i\n",
        "    else:\n",
        "      clean_ret.append(wgtfix(b))\n",
        "\n",
        "  dyp=trimdpth(dyp)\n",
        "  if len(dyp) == 0:\n",
        "    return clean_ret\n",
        "  \n",
        "  for k in dyp:\n",
        "    sta, endo =stapoz[k]\n",
        "    bdict[k].wgt=endo-sta+1\n",
        "\n",
        "\n",
        "\n",
        "  for k in dyp:\n",
        "    sta, endo =stapoz[k]\n",
        "    b=bdict[k]\n",
        "    nfo=b.eraz\n",
        "    isany=False\n",
        "\n",
        "    for erzid in nfo:\n",
        "      cur=bdict[erzid]\n",
        "      b.eraz[erzid]=cur\n",
        "      isany=True\n",
        "      if cur.yetproc:\n",
        "        sta2, endo2 =stapoz[erzid]\n",
        "        _,_,cur_osta, cur_oendo = cur.nfo(trans_sta=True,trans_end=True)\n",
        "        for i in range(sta2,endo2):\n",
        "          msg,_, cmp_osta, cmp_oendo = unit_arr[i].nfo(trans_sta=True,trans_end=True)\n",
        "          if msg != ']':\n",
        "            if cmp_osta < cur_osta:\n",
        "              cur_osta=cmp_osta\n",
        "            if cmp_oendo > cur_oendo:\n",
        "              cur_oendo = cmp_oendo\n",
        "        cur.upper=cur_osta\n",
        "        cur.lower=cur_oendo\n",
        "        cur.yetproc=False\n",
        "        \n",
        "\n",
        "    if isany:\n",
        "      mergedict(unit_arr,b.eraz,sta,endo)\n",
        "      b.eraz=None\n",
        "\n",
        "\n",
        "   \n",
        "  return clean_ret\n",
        "\n",
        "\n",
        "def mergedict(unit_arr,b_eraz,sta,endo):\n",
        "  for n in range(sta,endo):\n",
        "    ue=unit_arr[n]\n",
        "    if ue.id == 0:\n",
        "      if ue.eraz:\n",
        "        for k in b_eraz:\n",
        "          ue.eraz[k]=b_eraz[k]\n",
        "      else:\n",
        "        ue.eraz=b_eraz\n",
        "\n",
        "def tenzclamp(tenz,tolen=77):\n",
        "  dup=int(0.9999+(tolen/tenz.size(0)))\n",
        "  return torch.cat([tenz]*dup)[:tolen]\n",
        "\n",
        "\n",
        "def prmt_bin(binfna,nsamp=1,cuda=True):\n",
        "  if '%' in binfna:\n",
        "    bink=[]\n",
        "    for i in range(78):\n",
        "      nfna=binfna%i\n",
        "      if os.path.isfile(nfna):\n",
        "        bink.append( torch.tensor( np.fromfile(nfna,dtype=np.float32) ).reshape((-1,768)) )\n",
        "    tenz = tenzclamp(torch.cat(bink))\n",
        "  else:\n",
        "    tenz = tenzclamp( torch.tensor( np.fromfile(binfna,dtype=np.float32) ).reshape((-1,768)) )\n",
        "\n",
        "  tenz=tenz.expand(nsamp,-1,-1)\n",
        "  if cuda:\n",
        "    tenz=tenz.cuda()\n",
        "\n",
        "  return [cond_getter(tenz,fast=1)]\n",
        "  \n",
        "\n",
        "def calcknd(knd_arr,ptxt):\n",
        "  knd_arr = np.stack(knd_arr).transpose((1,0))\n",
        "  ar2i=dict()\n",
        "  i2txt=[]\n",
        "  txtid=0\n",
        "  hgt,prmpl=knd_arr.shape\n",
        "\n",
        "  kndmap=np.ones(hgt,dtype=np.uint8)\n",
        "\n",
        "  for i in range(hgt):\n",
        "    sig=str(knd_arr[i].tobytes())[2:-1].replace('\\\\','')\n",
        "    if sig in ar2i:\n",
        "      i_sig=ar2i[sig]\n",
        "    else:\n",
        "      ar2i[sig]=txtid\n",
        "      i2txt.append( i )\n",
        "      i_sig=txtid\n",
        "      txtid+=1\n",
        "    kndmap[i]=i_sig\n",
        "\n",
        "\n",
        "  stk=len(i2txt)\n",
        "  for i in range(0,prmpl):\n",
        "    stacking=[None]*stk\n",
        "    ge=ptxt[i]\n",
        "\n",
        "    for n in range(stk):\n",
        "      stacking[n]=ge.get(i2txt[n])\n",
        "    ptxt[i]=torch.stack(stacking)\n",
        "\n",
        "  return kndmap\n",
        "\n",
        "def kmapout(kndmap,calc_result):\n",
        "  stk=kndmap.shape[0]\n",
        "  cout2=[None]*stk\n",
        "\n",
        "  for i in range(stk):\n",
        "    cout2[i]=calc_result[ kndmap[i] ]\n",
        "  return cout2\n",
        "\n",
        "def prmt_avg(ptxt,pwgt,prmpl):\n",
        "  knd_arr=[None]*prmpl\n",
        "  cplx=False\n",
        "  for i in range(prmpl):\n",
        "    if not ptxt[i].is_simp:\n",
        "      cplx=True\n",
        "    knd_arr[i] = ptxt[i].get_knd()\n",
        "  \n",
        "  if cplx:\n",
        "    kndmap =calcknd( knd_arr, ptxt )\n",
        "    \n",
        "\n",
        "    cout=ptxt[0]*pwgt[0]\n",
        "    for i in range(1,prmpl):\n",
        "      cout+=(ptxt[i]*pwgt[i])\n",
        "\n",
        "    \n",
        "    cout2=kmapout(kndmap,cout)\n",
        "    \n",
        "\n",
        "    return [ cond_getter( cout2,kndref=kndmap ) ]\n",
        "\n",
        "  cout=ptxt[0].get(0)*pwgt[0]\n",
        "  for i in range(1,prmpl):\n",
        "    cout+=(ptxt[i].get(0)*pwgt[i])\n",
        "  return [ cond_getter( cout,fast=1 )]\n",
        "\n",
        "\n",
        "def prmt_dymc(stz,cuda):\n",
        "  prmpl=len(stz)>>1\n",
        "  ptxt=[]\n",
        "  pstp=[0]\n",
        "  stpsum=1\n",
        "  for i in range(prmpl):\n",
        "    ptxt.append(  makeCs(stz[2*i],1, cuda=cuda,enable3d=False )[0]  )\n",
        "    soi=float(stz[2*i+1])\n",
        "    stpsum+=soi\n",
        "    pstp.append(  stpsum  )\n",
        "\n",
        "  for i in range(prmpl):\n",
        "    pstp[i+1]=int(0.5+(pstp[i+1]/stpsum)*t_enc)\n",
        "\n",
        "  bs_knd=ptxt[0].get_knd().astype(np.uint16)\n",
        "  bs_arr=ptxt[0].get_fullarr()\n",
        "  for i in range(1,prmpl):\n",
        "    cut0=pstp[i]\n",
        "    bs_knd[cut0:]=ptxt[i].get_knd()[cut0:].astype(np.uint16)+0x100*i\n",
        "    bs_arr[cut0:]=ptxt[i].get_fullarr()[cut0:]\n",
        "\n",
        "  return [ cond_getter( bs_arr, kndref=bs_knd ) ]\n",
        "\n",
        "\n",
        "def prmt_intp_cplx(ptxt,pstp,knd_arr,prmpl):\n",
        "  kndmap =calcknd( knd_arr, ptxt )\n",
        "\n",
        "  intpos=[]\n",
        "  for vv in range(prmpl):\n",
        "    c1=ptxt[vv]\n",
        "    c2=ptxt[vv+1]\n",
        "    stp=pstp[vv]\n",
        "    for i in range(stp):\n",
        "      cn= kmapout(kndmap, (c2*i+c1*(stp-i))/stp )\n",
        "      intpos.append( cond_getter(cn, kndref=kndmap) )\n",
        "\n",
        "  lztbk=pstp[-1]\n",
        "  if lztbk > 1:\n",
        "    c1=ptxt[prmpl]\n",
        "    c2=ptxt[0]\n",
        "    for i in range(lztbk):\n",
        "      cn=kmapout(kndmap, (c2*i+c1*(lztbk-i))/lztbk )\n",
        "      intpos.append( cond_getter(cn, kndref=kndmap) )\n",
        "  else:\n",
        "    cn = kmapout(kndmap,ptxt[-1])\n",
        "    intpos.append( cond_getter(cn, kndref=kndmap) )\n",
        "  return intpos\n",
        "\n",
        "def prmt_intp(stz,cuda):\n",
        "  prmpl=len(stz)>>1\n",
        "  ptxt=[None]*prmpl\n",
        "  pstp=[None]*prmpl\n",
        "  knd_arr=[None]*prmpl\n",
        "  cplx=False\n",
        "  for i in range(prmpl):\n",
        "    ge=makeCs(stz[2*i],1, cuda=cuda,enable3d=False )[0]\n",
        "    knd_arr[i] = ge.get_knd()\n",
        "    if not ge.is_simp:\n",
        "      cplx=True\n",
        "    ptxt[i]=  ge  \n",
        "    pstp[i]=  int(stz[2*i+1])+1  \n",
        "  prmpl-=1\n",
        "\n",
        "  if cplx:\n",
        "    return prmt_intp_cplx(ptxt,pstp,knd_arr,prmpl)\n",
        "  \n",
        "  intpos=[]\n",
        "  for vv in range(prmpl):\n",
        "    c1=ptxt[vv].get(0)\n",
        "    c2=ptxt[vv+1].get(0)\n",
        "    stp=pstp[vv]\n",
        "    for i in range(stp):\n",
        "      cn=(c2*i+c1*(stp-i))/stp\n",
        "      intpos.append( cond_getter(cn,fast=1) )\n",
        "\n",
        "  lztbk=pstp[-1]\n",
        "  if lztbk > 1:\n",
        "    c1=ptxt[prmpl].get(0)\n",
        "    c2=ptxt[0].get(0)\n",
        "    for i in range(lztbk):\n",
        "      cn=(c2*i+c1*(lztbk-i))/lztbk\n",
        "      intpos.append( cond_getter(cn,fast=1) )\n",
        "  else:\n",
        "    intpos.append(ptxt[-1])\n",
        "  return intpos\n",
        "\n",
        "def printprompts(detailed=False):\n",
        "  k=0\n",
        "  for c in c_list:\n",
        "    tstr='PromptV'+str(k)+' at step'\n",
        "    dfarr=None\n",
        "    if SaveCompiledPrompt:\n",
        "      dfarr=c.save(tstr)\n",
        "    k+=1\n",
        "    if c.txt:\n",
        "      print(tstr+'0:')\n",
        "      print(c.get_txt(0))\n",
        "      if detailed and c.knd is not None:\n",
        "        if dfarr is not None:\n",
        "          for j in dfarr:\n",
        "            print(tstr+str(j)+':')\n",
        "            print(c.get_txt(j))\n",
        "        else:\n",
        "          knd=c.knd\n",
        "          prev=knd[0]\n",
        "          knd_l=len(knd)\n",
        "          for j in range(knd_l):\n",
        "            cur=knd[j]\n",
        "            if cur != prev:\n",
        "              prev=cur\n",
        "              print(tstr+str(j)+':')\n",
        "              print(c.get_txt(j))\n",
        "\n",
        "depthLimit=10\n",
        "\n",
        "def txtErr(prmt0,msg):\n",
        "  print(msg)\n",
        "  prmt=prmt0.split('/')[-1][:-4]\n",
        "  print('err prompt: '+prmt)\n",
        "  return pmpmtx([prmt0],nsamp=n_samples,enable3d=False)\n",
        "\n",
        "\n",
        "def cmdtype(cmd0):\n",
        "  if cmd0.startswith('intp:'):\n",
        "    return 1\n",
        "  elif cmd0.startswith('dymc:'):\n",
        "    return 2\n",
        "  elif cmd0.startswith('mad:'):\n",
        "    return 10\n",
        "  elif cmd0.startswith('avg:'):\n",
        "    return 11\n",
        "  return 0\n",
        "\n",
        "rtdir=''\n",
        "def makeCs(prmt,depth=0,cuda=True,enable3d=True):\n",
        "  global rtdir\n",
        "  if prmt.endswith('.txt'):\n",
        "    if depth > depthLimit:\n",
        "      return txtErr(prmt,'Too many ref, probably circular reference.')\n",
        "    if depth==0:\n",
        "      rtdir=''\n",
        "      try:\n",
        "        rtdir=prmt[:prmt.rindex('/')+1]\n",
        "      except:\n",
        "        pass\n",
        "    depth+=1\n",
        "    if not os.path.isfile(prmt):\n",
        "      prmt=rtdir+prmt\n",
        "      if not os.path.isfile(prmt):\n",
        "        return txtErr(prmt,'ref not found.')\n",
        "    with open(prmt,'rt') as f:\n",
        "      stz=f.read().splitlines()\n",
        "    cmd=stz[0].replace(' ','').replace('\\t','').split('/')\n",
        "    cmd0=cmdtype(cmd[0])\n",
        "    if cmd0 == 0:\n",
        "      return pmpmtx(stz,nsamp=n_samples,cuda=cuda,enable3d=enable3d)\n",
        "    elif cmd0 == 1:\n",
        "      if depth > 1:\n",
        "        return txtErr(stz[1],'do not intp in ref')\n",
        "      return prmt_intp(stz[1:],cuda=cuda)\n",
        "    elif cmd0 == 2:\n",
        "      return prmt_dymc(stz[1:],cuda=cuda)\n",
        "\n",
        "\n",
        "    prmpl=(len(stz)-1)>>1\n",
        "    stz=stz[1:]\n",
        "    ptxt=[]\n",
        "    pwgt=[]\n",
        "    wgtsum=0\n",
        "    for i in range(prmpl):\n",
        "      ptxt.append(  makeCs(stz[2*i],depth, cuda=cuda,enable3d=False )[0]  )\n",
        "      wgt=float(stz[2*i+1])\n",
        "      wgtsum+=wgt\n",
        "      pwgt.append(  wgt  )\n",
        "    if cmd0 == 11:\n",
        "      for i in range(prmpl):\n",
        "        pwgt[i]=pwgt[i]/wgtsum\n",
        "    \n",
        "    return prmt_avg(ptxt,pwgt,prmpl)\n",
        "\n",
        "  elif prmt.endswith('.bin'):\n",
        "    return prmt_bin(prmt,nsamp=n_samples,cuda=cuda)\n",
        "  elif prmt.endswith('.compiled_prompt'):\n",
        "    kn=cond_getter([prmt],fast=1)\n",
        "    kn.load(nsamp=n_samples,cuda=cuda)\n",
        "    return [kn]\n",
        "  else:\n",
        "    return pmpmtx([prmt],nsamp=n_samples,cuda=cuda,enable3d=enable3d)"
      ],
      "metadata": {
        "id": "Jh2EIFS99q3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if not os.path.isfile('PromptFuncsExample/MultiPrompt_average.txt'):\n",
        "  t3 = Thread(target = dlpromptexample)\n",
        "  a3 = t3.start()\n",
        "if os.path.isfile('web/svr.py_one'):\n",
        "  !mv web/svr.py_one web/svr.py\n",
        "!rm /content/sample_data/izh.txt"
      ],
      "metadata": {
        "id": "yh5Ms_sLOk_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Super Resolution 4x<br>\n",
        "Select ONE -- I SAY, JUST ONE -- of these tasks: Super Resolution, txt2img"
      ],
      "metadata": {
        "id": "115Y_TyxYkbk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4vrU_GL_6f3",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "jit=False #@param {type:'boolean'}\n",
        "\n",
        "import os\n",
        "\n",
        "if not os.path.isfile('fsd_pnnx.pt'):\n",
        "  !wget https://huggingface.co/Larvik/LDMjit/resolve/main/alphas_cumprod.npy\n",
        "  !wget https://huggingface.co/Larvik/LDMjit/resolve/main/dm_pnnx.pt\n",
        "  !wget https://huggingface.co/Larvik/LDMjit/resolve/main/fsd_pnnx.pt\n",
        "\n",
        "\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import functools\n",
        "import torch\n",
        "\n",
        "cudev=torch.device('cuda')\n",
        "\n",
        "alphas_cumprod = np.load('alphas_cumprod.npy')\n",
        "\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "torch.set_num_threads(os.cpu_count())\n",
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True\n",
        "\n",
        "\n",
        "# ======================\n",
        "# Arguemnt Parser Config\n",
        "# ======================\n",
        "\n",
        "def imread(filename, flags=cv2.IMREAD_COLOR):\n",
        "    if not os.path.isfile(filename):\n",
        "        print(f\"File does not exist: {filename}\")\n",
        "        sys.exit()\n",
        "    data = np.fromfile(filename, np.int8)\n",
        "    img = cv2.imdecode(data, flags)\n",
        "    return img\n",
        "\n",
        "def preprocessing_img(img):\n",
        "    if len(img.shape) < 3:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGRA)\n",
        "    elif img.shape[2] == 3:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2BGRA)\n",
        "    elif img.shape[2] == 1:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGRA)\n",
        "    return img\n",
        "\n",
        "\n",
        "def load_image(image_path):\n",
        "    if os.path.isfile(image_path):\n",
        "        img = imread(image_path, cv2.IMREAD_UNCHANGED)\n",
        "    else:\n",
        "        print(f'{image_path} not found.')\n",
        "    return preprocessing_img(img)\n",
        "\n",
        "\n",
        "\n",
        "def meshgrid(h, w):\n",
        "    y = torch.arange(0, h).view(h, 1, 1).repeat(1, w, 1)\n",
        "    x = torch.arange(0, w).view(1, w, 1).repeat(h, 1, 1)\n",
        "\n",
        "    arr = torch.cat([y, x], dim=-1)\n",
        "    return arr\n",
        "\n",
        "\n",
        "def delta_border(h, w):\n",
        "    \"\"\"\n",
        "    :param h: height\n",
        "    :param w: width\n",
        "    :return: normalized distance to image border,\n",
        "      wtith min distance = 0 at border and max dist = 0.5 at image center\n",
        "    \"\"\"\n",
        "    lower_right_corner = torch.tensor([h - 1, w - 1]).view(1, 1, 2)\n",
        "    arr = meshgrid(h, w) / lower_right_corner\n",
        "    dist_left_up = torch.min(arr, dim=-1, keepdims=True)[0]\n",
        "    dist_right_down = torch.min(1 - arr, dim=-1, keepdims=True)[0]\n",
        "    edge_dist = torch.min(torch.cat([dist_left_up, dist_right_down], dim=-1), dim=-1)[0]\n",
        "    return edge_dist\n",
        "\n",
        "\n",
        "\n",
        "def get_weighting(h, w, Ly, Lx, device):\n",
        "  clip_min_weight = 0.01\n",
        "  clip_max_weight = 0.5\n",
        "  weighting = delta_border(h, w)\n",
        "  weighting = torch.clip(weighting, clip_min_weight, clip_max_weight, )\n",
        "  weighting = weighting.view(1, h * w, 1).repeat(1, 1, Ly * Lx).to(device)\n",
        "\n",
        "\n",
        "  return weighting\n",
        "\n",
        "def get_fold_unfold(x, kernel_size, stride, uf=1, df=1):  # todo load once not every time, shorten code\n",
        "    \"\"\"\n",
        "    :param x: img of size (bs, c, h, w)\n",
        "    :return: n img crops of size (n, bs, c, kernel_size[0], kernel_size[1])\n",
        "    \"\"\"\n",
        "    bs, nc, h, w = x.shape\n",
        "\n",
        "    # number of crops in image\n",
        "    Ly = (h - kernel_size[0]) // stride[0] + 1\n",
        "    Lx = (w - kernel_size[1]) // stride[1] + 1\n",
        "\n",
        "    if uf == 1 and df == 1:\n",
        "        fold_params = dict(kernel_size=kernel_size, dilation=1, padding=0, stride=stride)\n",
        "        unfold = torch.nn.Unfold(**fold_params)\n",
        "\n",
        "        fold = torch.nn.Fold(output_size=x.shape[2:], **fold_params)\n",
        "\n",
        "        weighting = get_weighting(kernel_size[0], kernel_size[1], Ly, Lx, x.device).to(x.dtype)\n",
        "        normalization = fold(weighting).view(1, 1, h, w)  # normalizes the overlap\n",
        "        weighting = weighting.view((1, 1, kernel_size[0], kernel_size[1], Ly * Lx))\n",
        "\n",
        "    elif uf > 1 and df == 1:\n",
        "        fold_params = dict(kernel_size=kernel_size, dilation=1, padding=0, stride=stride)\n",
        "        unfold = torch.nn.Unfold(**fold_params)\n",
        "\n",
        "        fold_params2 = dict(kernel_size=(kernel_size[0] * uf, kernel_size[0] * uf),\n",
        "                            dilation=1, padding=0,\n",
        "                            stride=(stride[0] * uf, stride[1] * uf))\n",
        "        fold = torch.nn.Fold(output_size=(x.shape[2] * uf, x.shape[3] * uf), **fold_params2)\n",
        "\n",
        "        weighting = get_weighting(kernel_size[0] * uf, kernel_size[1] * uf, Ly, Lx, x.device).to(x.dtype)\n",
        "        normalization = fold(weighting).view(1, 1, h * uf, w * uf)  # normalizes the overlap\n",
        "        weighting = weighting.view((1, 1, kernel_size[0] * uf, kernel_size[1] * uf, Ly * Lx))\n",
        "\n",
        "    elif df > 1 and uf == 1:\n",
        "        fold_params = dict(kernel_size=kernel_size, dilation=1, padding=0, stride=stride)\n",
        "        unfold = torch.nn.Unfold(**fold_params)\n",
        "\n",
        "        fold_params2 = dict(kernel_size=(kernel_size[0] // df, kernel_size[0] // df),\n",
        "                            dilation=1, padding=0,\n",
        "                            stride=(stride[0] // df, stride[1] // df))\n",
        "        fold = torch.nn.Fold(output_size=(x.shape[2] // df, x.shape[3] // df), **fold_params2)\n",
        "\n",
        "        weighting = get_weighting(kernel_size[0] // df, kernel_size[1] // df, Ly, Lx, x.device).to(x.dtype)\n",
        "        normalization = fold(weighting).view(1, 1, h // df, w // df)  # normalizes the overlap\n",
        "        weighting = weighting.view((1, 1, kernel_size[0] // df, kernel_size[1] // df, Ly * Lx))\n",
        "\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    return fold, unfold, normalization, weighting\n",
        "\n",
        "\n",
        "\n",
        "def normalize_image(image, normalize_type='255'):\n",
        "    \"\"\"\n",
        "    Normalize image\n",
        "    Parameters\n",
        "    ----------\n",
        "    image: numpy array\n",
        "        The image you want to normalize\n",
        "    normalize_type: string\n",
        "        Normalize type should be chosen from the type below.\n",
        "        - '255': simply dividing by 255.0\n",
        "        - '127.5': output range : -1 and 1\n",
        "        - 'ImageNet': normalize by mean and std of ImageNet\n",
        "        - 'None': no normalization\n",
        "    Returns\n",
        "    -------\n",
        "    normalized_image: numpy array\n",
        "    \"\"\"\n",
        "    if normalize_type == 'None':\n",
        "        return image\n",
        "    elif normalize_type == '255':\n",
        "        return image / 255.0\n",
        "    elif normalize_type == '127.5':\n",
        "        return image / 127.5 - 1.0\n",
        "    elif normalize_type == 'ImageNet':\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        image = image / 255.0\n",
        "        for i in range(3):\n",
        "            image[:, :, i] = (image[:, :, i] - mean[i]) / std[i]\n",
        "        return image\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "\n",
        "\n",
        "def preprocess(img):\n",
        "    im_h, im_w, _ = img.shape\n",
        "\n",
        "    up_f = 4\n",
        "    oh, ow = up_f * im_h, up_f * im_w\n",
        "\n",
        "    img = normalize_image(img, normalize_type='255')\n",
        "\n",
        "    c = img * 2 - 1\n",
        "    c = c.transpose(2, 0, 1)  # HWC -> CHW\n",
        "    c = np.expand_dims(c, axis=0)\n",
        "    c = c.astype(np.float32)\n",
        "\n",
        "    return None, c\n",
        "\n",
        "\n",
        "def postprocess(sample):\n",
        "    sample = np.clip(sample, -1., 1.)\n",
        "    sample = (sample + 1.) / 2. * 255\n",
        "    sample = np.transpose(sample, (1, 2, 0))\n",
        "    sample = sample[:, :, ::-1]  # RGB -> BGR\n",
        "    sample = sample.astype(np.uint8)\n",
        "\n",
        "    return sample\n",
        "\n",
        "\n",
        "def decode_first_stage(z):\n",
        "    ks = (128, 128)\n",
        "    stride = (64, 64)\n",
        "    uf = 4\n",
        "\n",
        "    bs, nc, h, w = z.shape\n",
        "\n",
        "    fold, unfold, normalization, weighting = get_fold_unfold(z, ks, stride, uf=uf)\n",
        "\n",
        "    z = unfold(z)  # (bn, nc * prod(**ks), L)\n",
        "\n",
        "    # Reshape to img shape\n",
        "    z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n",
        "\n",
        "\n",
        "    print('first_stage_decode...')\n",
        "\n",
        "    outputs = []\n",
        "    for i in range(z.shape[-1]):\n",
        "        x = z[:, :, :, :, i]\n",
        "        output = first_stage_decode(x)\n",
        "        outputs.append(output[0])\n",
        "\n",
        "    o = torch.stack(outputs, axis=-1)  # # (bn, nc, ks[0], ks[1], L)\n",
        "    o = o * weighting\n",
        "    # Reverse 1. reshape to img shape\n",
        "    o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n",
        "    # stitch crops together\n",
        "    decoded = fold(o)\n",
        "    decoded = decoded / normalization  # norm is shape (1, 1, h, w)\n",
        "    return decoded\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ddpm\n",
        "def apply_model(x, t, cond,d):\n",
        "    x_noisy=x\n",
        "    ks = (128, 128)\n",
        "    stride = (64, 64)\n",
        "\n",
        "    h, w = x_noisy.shape[-2:]\n",
        "\n",
        "    fold, unfold, normalization, weighting = get_fold_unfold(x_noisy, ks, stride)\n",
        "\n",
        "\n",
        "    z = unfold(x_noisy)  # (bn, nc * prod(**ks), L)\n",
        "    # Reshape to img shape\n",
        "    z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n",
        "    z_list = [z[:, :, :, :, i] for i in range(z.shape[-1])]\n",
        "\n",
        "    c = unfold(cond)\n",
        "    c = c.view((c.shape[0], -1, ks[0], ks[1], c.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n",
        "    cond_list = [c[:, :, :, :, i] for i in range(c.shape[-1])]\n",
        "\n",
        "    # apply model by loop over crops\n",
        "    \n",
        "    outputs = []\n",
        "    for i in range(z.shape[-1]):\n",
        "        x = z_list[i]\n",
        "        cond = cond_list[i]\n",
        "        xc = torch.cat([x, cond], dim=1)\n",
        "        \n",
        "        \n",
        "        output = diffusion_model(xc, t)\n",
        "\n",
        "\n",
        "        outputs.append(output[0])\n",
        "\n",
        "    o = torch.stack(outputs, axis=-1)\n",
        "    o = o * weighting\n",
        "    # Reverse reshape to img shape\n",
        "    o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n",
        "    # stitch crops together\n",
        "    x_recon = fold(o) / normalization\n",
        "\n",
        "\n",
        "    return x_recon\n",
        "\n",
        "def warmup():\n",
        "  v_0 = torch.rand(1,6,128,128, dtype=torch.float).half().cuda()\n",
        "  v_1 = torch.randint(10, (1, ), dtype=torch.long).cuda()\n",
        "\n",
        "  for d in range(2):\n",
        "    with torch.cuda.amp.autocast(dtype=torch.float16):\n",
        "      uaa = diffusion_model(v_0,v_1)\n",
        "  v_0 = torch.rand(1,3,128,128, dtype=torch.float).cuda()\n",
        "  for d in range(2):\n",
        "    uaa = first_stage_decode(v_0)\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "UseSamplr=sample_euler_ancestral\n",
        "def predict(c):\n",
        "    \n",
        "    c=torch.tensor(c,device=cudev)\n",
        "\n",
        "\n",
        "    sigmas = f_sigmas()\n",
        "\n",
        "    noise = torch.randn(c.shape, dtype=torch.float,device=cudev)\n",
        "    \n",
        "    extra_args = {'cond': c}\n",
        "    df=detail_strength/(detail_strength-1+float(sigmas[0]))\n",
        "    print(df)\n",
        "    with torch.cuda.amp.autocast(dtype=torch.float16):\n",
        "        samples = UseSamplr(model_wrap_cfg, noise * sigmas[0] * df , sigmas, extra_args=extra_args, disable=False)\n",
        "   \n",
        "    x_sample = decode_first_stage(samples)\n",
        "\n",
        "    img = postprocess(x_sample[0].cpu().numpy())\n",
        "\n",
        "    return img\n",
        "\n",
        "if model_wrap is None:\n",
        "  first_stage_decode=torch.jit.load('/content/fsd_pnnx.pt').eval().cuda()\n",
        "  diffusion_model=torch.jit.load('/content/dm_pnnx.pt').eval().half().cuda()\n",
        "  warmup()\n",
        "  torch.cuda.empty_cache()\n",
        "  model_wrap = CompVisDenoiser(CompVisJIT())\n",
        "  model_wrap_cfg = SRDenoiser(model_wrap)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "image_path='/content/sample_data/10_0x0v1.png' #@param {type:'string'}\n",
        "\n",
        "\"\"\"\n",
        "ddim_timesteps\n",
        "\"\"\"\n",
        "ddim_eta = 0.75  #@param {type:'number'}\n",
        "ddim_num_steps = 100  #@param {type:'number'}\n",
        "ddpm_num_timesteps = 1000 #@param {type:'number'}\n",
        "\n",
        "detail_strength=20000  #@param {type:'number'}\n",
        "\n",
        "ddim_timesteps = make_ddim_timesteps(ddim_num_steps, ddpm_num_timesteps)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "ddim sampling parameters\n",
        "\"\"\"\n",
        "\n",
        "ddim_sigmas, ddim_alphas, ddim_alphas_prev = \\\n",
        "    make_ddim_sampling_parameters(\n",
        "        alphacums=alphas_cumprod,\n",
        "        ddim_timesteps=ddim_timesteps,\n",
        "        eta=ddim_eta)\n",
        "\n",
        "#ddim_sigmas=torch.tensor(ddim_sigmas.astype(np.float32),device=cudev)\n",
        "\n",
        "ddim_sqrt_one_minus_alphas = np.sqrt(1. - ddim_alphas)\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# inference\n",
        "print('Start inference...')\n",
        "if image_path.endswith('.npy'):\n",
        "  c=latdec(image_path).detach()\n",
        "else:\n",
        "  img = load_image(image_path)\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
        "  img = img[:, :, ::-1]  # BGR -> RGB\n",
        "  _, c = preprocess(img)\n",
        "  \n",
        "\n",
        "img = predict(c)\n",
        "\n",
        "# plot result\n",
        "savepath = image_path[:-4]+'_4x.png'\n",
        "print(f'saved at : {savepath}')\n",
        "cv2.imwrite(savepath, img)\n",
        "\n"
      ],
      "metadata": {
        "id": "xrSkT1_5AJSr",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Sampler='euler_a' #@param ['euler', 'euler_a', 'heun','dpm_2','dpm_2_a','lms']\n",
        "f_sampler()\n",
        "\n",
        "Karras=False #@param {type:'boolean'}\n",
        "KarrasRho = 7.0 #@param {type:'number'}"
      ],
      "metadata": {
        "id": "2UYWGCLIKSFS",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional: SD lat decoder"
      ],
      "metadata": {
        "id": "_jVyaZSmEfPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latent='4x6_1x1v1.npy' #@param {type:'string'}\n",
        "ext='.png' #@param ['.png', '.jpg']\n",
        "lat=latdec(latent)\n",
        "k=0\n",
        "for lla in lat:\n",
        "  ymg=Image.fromarray( (( ( lla +1)*127.5 ).cpu().numpy()).transpose(1,2,0).clip(0,255).astype(np.uint8) )\n",
        "  ymg.save(latent[:-4].replace('x1v','x'+str(k)+'v')+ext)\n",
        "  k+=1\n",
        "ymg"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TUrKQWmQFo7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional: GFPgan-jit"
      ],
      "metadata": {
        "id": "c79C9jECmn-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import glob\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "import math\n",
        "\n",
        "\n",
        "from torchvision.transforms.functional import normalize\n",
        "from itertools import product\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def imwrite(img, file_path, params=None, auto_mkdir=True):\n",
        "\n",
        "    if auto_mkdir:\n",
        "        dir_name = os.path.abspath(os.path.dirname(file_path))\n",
        "        os.makedirs(dir_name, exist_ok=True)\n",
        "    ok = cv2.imwrite(file_path, img, params)\n",
        "    if not ok:\n",
        "        raise IOError('Failed in writing images.')\n",
        "\n",
        "\n",
        "\n",
        "def bb_intersection_over_union(boxA, boxB):\n",
        "    # determine the (x, y)-coordinates of the intersection rectangle\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[2], boxB[2])\n",
        "    yB = min(boxA[3], boxB[3])\n",
        "    # compute the area of intersection rectangle\n",
        "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
        "    # compute the area of both the prediction and ground-truth\n",
        "    # rectangles\n",
        "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
        "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
        "    # compute the intersection over union by taking the intersection\n",
        "    # area and dividing it by the sum of prediction + ground-truth\n",
        "    # areas - the interesection area\n",
        "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
        "    # return the intersection over union value\n",
        "    return iou\n",
        "\n",
        "\n",
        "def nms_boxes(boxes, scores, iou_thres):\n",
        "    # Performs non-maximum suppression (NMS) on the boxes according to their intersection-over-union (IoU).\n",
        "\n",
        "    keep = []\n",
        "    for i, box_a in enumerate(boxes):\n",
        "        is_keep = True\n",
        "        for j in range(i):\n",
        "            if not keep[j]:\n",
        "                continue\n",
        "            box_b = boxes[j]\n",
        "            iou = bb_intersection_over_union(box_a, box_b)\n",
        "            if iou >= iou_thres:\n",
        "                if scores[i] > scores[j]:\n",
        "                    keep[j] = False\n",
        "                else:\n",
        "                    is_keep = False\n",
        "                    break\n",
        "\n",
        "        keep.append(is_keep)\n",
        "\n",
        "    return np.array(keep).nonzero()[0]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_anchor(image_size):\n",
        "    \n",
        "    min_sizes = [[16, 32], [64, 128], [256, 512]]\n",
        "    steps = [8, 16, 32]\n",
        "    feature_maps = [[math.ceil(image_size[0] / step), math.ceil(image_size[1] / step)] for step in steps]\n",
        "\n",
        "    anchors = []\n",
        "    for k, f in enumerate(feature_maps):\n",
        "        m_sizes = min_sizes[k]\n",
        "        for i, j in product(range(f[0]), range(f[1])):\n",
        "            for min_size in m_sizes:\n",
        "                s_kx = min_size / image_size[1]\n",
        "                s_ky = min_size / image_size[0]\n",
        "                dense_cx = [x * steps[k] / image_size[1] for x in [j + 0.5]]\n",
        "                dense_cy = [y * steps[k] / image_size[0] for y in [i + 0.5]]\n",
        "                for cy, cx in product(dense_cy, dense_cx):\n",
        "                    anchors.extend([cx, cy, s_kx, s_ky])\n",
        "\n",
        "    output = np.array(anchors).reshape(-1, 4)\n",
        "    return output\n",
        "\n",
        "\n",
        "# Adapted from https://github.com/Hakuyume/chainer-ssd\n",
        "def decode(loc, priors, variances):\n",
        "    \"\"\"Decode locations from predictions using priors to undo\n",
        "    the encoding we did for offset regression at train time.\n",
        "    Args:\n",
        "        loc (tensor): location predictions for loc layers,\n",
        "            Shape: [num_priors,4]\n",
        "        priors (tensor): Prior boxes in center-offset form.\n",
        "            Shape: [num_priors,4].\n",
        "        variances: (list[float]) Variances of priorboxes\n",
        "    Return:\n",
        "        decoded bounding box predictions\n",
        "    \"\"\"\n",
        "    boxes = np.concatenate(\n",
        "        (priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\n",
        "         priors[:, 2:] * np.exp(loc[:, 2:] * variances[1])), 1)\n",
        "    boxes[:, :2] -= boxes[:, 2:] / 2\n",
        "    boxes[:, 2:] += boxes[:, :2]\n",
        "\n",
        "    return boxes\n",
        "\n",
        "\n",
        "def decode_landm(pre, priors, variances):\n",
        "    \"\"\"Decode landm from predictions using priors to undo\n",
        "    the encoding we did for offset regression at train time.\n",
        "    Args:\n",
        "        pre (tensor): landm predictions for loc layers,\n",
        "            Shape: [num_priors,10]\n",
        "        priors (tensor): Prior boxes in center-offset form.\n",
        "            Shape: [num_priors,4].\n",
        "        variances: (list[float]) Variances of priorboxes\n",
        "    Return:\n",
        "        decoded landm predictions\n",
        "    \"\"\"\n",
        "    tmp = (\n",
        "        priors[:, :2] + pre[:, :2] * variances[0] * priors[:, 2:],\n",
        "        priors[:, :2] + pre[:, 2:4] * variances[0] * priors[:, 2:],\n",
        "        priors[:, :2] + pre[:, 4:6] * variances[0] * priors[:, 2:],\n",
        "        priors[:, :2] + pre[:, 6:8] * variances[0] * priors[:, 2:],\n",
        "        priors[:, :2] + pre[:, 8:10] * variances[0] * priors[:, 2:],\n",
        "    )\n",
        "    landms = np.concatenate(tmp, axis=1)\n",
        "\n",
        "    return landms\n",
        "\n",
        "\n",
        "\n",
        "def detect_faces(\n",
        "        image,\n",
        "        conf_threshold=0.8,\n",
        "        nms_threshold=0.4,\n",
        "        use_origin_size=True,\n",
        "    ):\n",
        "        \n",
        "        height, width = image.shape[:2]\n",
        "        image = image.transpose(2, 0, 1).astype(np.float32)\n",
        "        image = torch.from_numpy(image).to(cudevg).unsqueeze(0)\n",
        "\n",
        "        image = image - torch.tensor([[[[104.]], [[117.]], [[123.]]]])\n",
        "\n",
        "        loc, conf, landmarks = RetinaFace(image)\n",
        "        priors = get_anchor((height, width))\n",
        "\n",
        "        variance = [0.1, 0.2]\n",
        "        scale = np.array([width, height, width, height])\n",
        "        scale1 = np.array([\n",
        "            width, height, width, height, width, height, width, height, width, height\n",
        "        ])\n",
        "\n",
        "        boxes = decode(loc[0].cpu().numpy(), priors, variance)\n",
        "        boxes = boxes * scale\n",
        "        \n",
        "\n",
        "        scores = conf[0][:, 1].cpu().numpy()\n",
        "\n",
        "        landmarks = decode_landm(landmarks[0].cpu().numpy(), priors, variance)\n",
        "        landmarks = landmarks * scale1\n",
        "        \n",
        "\n",
        "        # ignore low scores\n",
        "        inds = np.where(scores > conf_threshold)[0]\n",
        "        boxes, landmarks, scores = boxes[inds], landmarks[inds], scores[inds]\n",
        "\n",
        "        # sort\n",
        "        order = scores.argsort()[::-1]\n",
        "        boxes, landmarks, scores = boxes[order], landmarks[order], scores[order]\n",
        "\n",
        "        # do NMS\n",
        "        bounding_boxes = np.hstack((boxes, scores[:, np.newaxis])).astype(np.float32, copy=False)\n",
        "        keep = nms_boxes(bounding_boxes[:, :4], bounding_boxes[:, 4], nms_threshold)\n",
        "        bounding_boxes, landmarks = bounding_boxes[keep, :], landmarks[keep]\n",
        "        return np.concatenate((bounding_boxes, landmarks), axis=1)\n",
        "\n",
        "def get_largest_face(det_faces, h, w):\n",
        "\n",
        "    def get_location(val, length):\n",
        "        if val < 0:\n",
        "            return 0\n",
        "        elif val > length:\n",
        "            return length\n",
        "        else:\n",
        "            return val\n",
        "\n",
        "    face_areas = []\n",
        "    for det_face in det_faces:\n",
        "        left = get_location(det_face[0], w)\n",
        "        right = get_location(det_face[2], w)\n",
        "        top = get_location(det_face[1], h)\n",
        "        bottom = get_location(det_face[3], h)\n",
        "        face_area = (right - left) * (bottom - top)\n",
        "        face_areas.append(face_area)\n",
        "    largest_idx = face_areas.index(max(face_areas))\n",
        "    return det_faces[largest_idx], largest_idx\n",
        "\n",
        "\n",
        "def get_center_face(det_faces, h=0, w=0, center=None):\n",
        "    if center is not None:\n",
        "        center = np.array(center)\n",
        "    else:\n",
        "        center = np.array([w / 2, h / 2])\n",
        "    center_dist = []\n",
        "    for det_face in det_faces:\n",
        "        face_center = np.array([(det_face[0] + det_face[2]) / 2, (det_face[1] + det_face[3]) / 2])\n",
        "        dist = np.linalg.norm(face_center - center)\n",
        "        center_dist.append(dist)\n",
        "    center_idx = center_dist.index(min(center_dist))\n",
        "    return det_faces[center_idx], center_idx\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def img2tensor(imgs, bgr2rgb=True, float32=True):\n",
        "\n",
        "\n",
        "    def _totensor(img, bgr2rgb, float32):\n",
        "        if img.shape[2] == 3 and bgr2rgb:\n",
        "            if img.dtype == 'float64':\n",
        "                img = img.astype('float32')\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = torch.from_numpy(img.transpose(2, 0, 1))\n",
        "        if float32:\n",
        "            img = img.float()\n",
        "        return img\n",
        "\n",
        "    if isinstance(imgs, list):\n",
        "        return [_totensor(img, bgr2rgb, float32) for img in imgs]\n",
        "    else:\n",
        "        return _totensor(imgs, bgr2rgb, float32)\n",
        "\n",
        "\n",
        "\n",
        "def read_image(img):\n",
        "    \"\"\"img can be image path or cv2 loaded image.\"\"\"\n",
        "    # self.input_img is Numpy array, (h, w, c), BGR, uint8, [0, 255]\n",
        "\n",
        "\n",
        "    if np.max(img) > 256:  # 16-bit image\n",
        "        img = (img / 65535) * 255\n",
        "    if len(img.shape) == 2:  # gray image\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
        "    elif img.shape[2] == 4:  # RGBA image with alpha channel\n",
        "        img = img[:, :, 0:3]\n",
        "\n",
        "    return img\n",
        "'''\n",
        "def srproc(img,fac):\n",
        "  return cv2.resize(img, None,fx=fac,fy=fac, interpolation=cv2.INTER_LINEAR)\n",
        "'''\n",
        "def srproc(img,fac):\n",
        "  _,c=preprocess(img[:, :, ::-1])\n",
        "  return predict(c)\n",
        "\n",
        "class faceimg:\n",
        "  def __init__(self, image,\n",
        "                 face_size=512,\n",
        "                 crop_ratio=(1, 1),\n",
        "                 save_ext='png',\n",
        "                 template_3points=False,\n",
        "                 pad_blur=False,\n",
        "                 use_parse=False,\n",
        "                 device=None):\n",
        "    self.nXimage=read_image(image)\n",
        "    downscale=1/upscale\n",
        "    self.input_img=cv2.resize(self.nXimage,None,fx=downscale,fy=downscale,interpolation=cv2.INTER_AREA)\n",
        "    self.template_3points = template_3points  # improve robustness\n",
        "    self.upscale_factor = upscale\n",
        "    # the cropped face ratio based on the square face\n",
        "    self.crop_ratio = crop_ratio  # (h, w)\n",
        "    assert (self.crop_ratio[0] >= 1 and self.crop_ratio[1] >= 1), 'crop ration only supports >=1'\n",
        "    self.face_size = (int(face_size * self.crop_ratio[1]), int(face_size * self.crop_ratio[0]))\n",
        "\n",
        "    if self.template_3points:\n",
        "        self.face_template = np.array([[192, 240], [319, 240], [257, 371]])\n",
        "    else:\n",
        "        # standard 5 landmarks for FFHQ faces with 512 x 512\n",
        "        self.face_template = np.array([[192.98138, 239.94708], [318.90277, 240.1936], [256.63416, 314.01935],\n",
        "                                        [201.26117, 371.41043], [313.08905, 371.15118]])\n",
        "    self.face_template = self.face_template * (face_size / 512.0)\n",
        "    if self.crop_ratio[0] > 1:\n",
        "        self.face_template[:, 1] += face_size * (self.crop_ratio[0] - 1) / 2\n",
        "    if self.crop_ratio[1] > 1:\n",
        "        self.face_template[:, 0] += face_size * (self.crop_ratio[1] - 1) / 2\n",
        "    self.save_ext = save_ext\n",
        "    self.pad_blur = pad_blur\n",
        "    if self.pad_blur is True:\n",
        "        self.template_3points = False\n",
        "\n",
        "    self.all_landmarks_5 = []\n",
        "    self.det_faces = []\n",
        "    self.affine_matrices = []\n",
        "    self.inverse_affine_matrices = []\n",
        "    self.cropped_faces = []\n",
        "    self.pad_input_imgs = []\n",
        "    self.restored_faces=[]\n",
        "\n",
        "\n",
        "    # init face parsing model\n",
        "    self.use_parse = use_parse\n",
        "  def get_face_landmarks_5(self,\n",
        "              only_keep_largest=False,\n",
        "              only_center_face=False,\n",
        "              resize=None,\n",
        "              blur_ratio=0.01,\n",
        "              eye_dist_threshold=None):\n",
        "    if resize is None:\n",
        "        scale = 1\n",
        "        input_img = self.input_img\n",
        "    else:\n",
        "        h, w = self.input_img.shape[0:2]\n",
        "        scale = min(h, w) / resize\n",
        "        h, w = int(h / scale), int(w / scale)\n",
        "        input_img = cv2.resize(self.input_img, (w, h), interpolation=cv2.INTER_LANCZOS4)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        bboxes = detect_faces( input_img ) * scale #0.97\n",
        "    for bbox in bboxes:\n",
        "        # remove faces with too small eye distance: side faces or too small faces\n",
        "        eye_dist = np.linalg.norm([bbox[6] - bbox[8], bbox[7] - bbox[9]])\n",
        "        if eye_dist_threshold is not None and (eye_dist < eye_dist_threshold):\n",
        "            continue\n",
        "\n",
        "        if self.template_3points:\n",
        "            landmark = np.array([[bbox[i], bbox[i + 1]] for i in range(5, 11, 2)])\n",
        "        else:\n",
        "            landmark = np.array([[bbox[i], bbox[i + 1]] for i in range(5, 15, 2)])\n",
        "        self.all_landmarks_5.append(landmark)\n",
        "        self.det_faces.append(bbox[0:5])\n",
        "    if len(self.det_faces) == 0:\n",
        "        return 0\n",
        "    if only_keep_largest:\n",
        "        h, w, _ = self.input_img.shape\n",
        "        self.det_faces, largest_idx = get_largest_face(self.det_faces, h, w)\n",
        "        self.all_landmarks_5 = [self.all_landmarks_5[largest_idx]]\n",
        "    elif only_center_face:\n",
        "        h, w, _ = self.input_img.shape\n",
        "        self.det_faces, center_idx = get_center_face(self.det_faces, h, w)\n",
        "        self.all_landmarks_5 = [self.all_landmarks_5[center_idx]]\n",
        "\n",
        "    # pad blurry images\n",
        "    if self.pad_blur:\n",
        "        self.pad_input_imgs = []\n",
        "        for landmarks in self.all_landmarks_5:\n",
        "            # get landmarks\n",
        "            eye_left = landmarks[0, :]\n",
        "            eye_right = landmarks[1, :]\n",
        "            eye_avg = (eye_left + eye_right) * 0.5\n",
        "            mouth_avg = (landmarks[3, :] + landmarks[4, :]) * 0.5\n",
        "            eye_to_eye = eye_right - eye_left\n",
        "            eye_to_mouth = mouth_avg - eye_avg\n",
        "\n",
        "            # Get the oriented crop rectangle\n",
        "            # x: half width of the oriented crop rectangle\n",
        "            x = eye_to_eye - np.flipud(eye_to_mouth) * [-1, 1]\n",
        "            #  - np.flipud(eye_to_mouth) * [-1, 1]: rotate 90 clockwise\n",
        "            # norm with the hypotenuse: get the direction\n",
        "            x /= np.hypot(*x)  # get the hypotenuse of a right triangle\n",
        "            rect_scale = 1.5\n",
        "            x *= max(np.hypot(*eye_to_eye) * 2.0 * rect_scale, np.hypot(*eye_to_mouth) * 1.8 * rect_scale)\n",
        "            # y: half height of the oriented crop rectangle\n",
        "            y = np.flipud(x) * [-1, 1]\n",
        "\n",
        "            # c: center\n",
        "            c = eye_avg + eye_to_mouth * 0.1\n",
        "            # quad: (left_top, left_bottom, right_bottom, right_top)\n",
        "            quad = np.stack([c - x - y, c - x + y, c + x + y, c + x - y])\n",
        "            # qsize: side length of the square\n",
        "            qsize = np.hypot(*x) * 2\n",
        "            border = max(int(np.rint(qsize * 0.1)), 3)\n",
        "\n",
        "            # get pad\n",
        "            # pad: (width_left, height_top, width_right, height_bottom)\n",
        "            pad = (int(np.floor(min(quad[:, 0]))), int(np.floor(min(quad[:, 1]))), int(np.ceil(max(quad[:, 0]))),\n",
        "                    int(np.ceil(max(quad[:, 1]))))\n",
        "            pad = [\n",
        "                max(-pad[0] + border, 1),\n",
        "                max(-pad[1] + border, 1),\n",
        "                max(pad[2] - self.input_img.shape[0] + border, 1),\n",
        "                max(pad[3] - self.input_img.shape[1] + border, 1)\n",
        "            ]\n",
        "\n",
        "            if max(pad) > 1:\n",
        "                # pad image\n",
        "                pad_img = np.pad(self.input_img, ((pad[1], pad[3]), (pad[0], pad[2]), (0, 0)), 'reflect')\n",
        "                # modify landmark coords\n",
        "                landmarks[:, 0] += pad[0]\n",
        "                landmarks[:, 1] += pad[1]\n",
        "                # blur pad images\n",
        "                h, w, _ = pad_img.shape\n",
        "                y, x, _ = np.ogrid[:h, :w, :1]\n",
        "                mask = np.maximum(1.0 - np.minimum(np.float32(x) / pad[0],\n",
        "                                                    np.float32(w - 1 - x) / pad[2]),\n",
        "                                  1.0 - np.minimum(np.float32(y) / pad[1],\n",
        "                                                    np.float32(h - 1 - y) / pad[3]))\n",
        "                blur = int(qsize * blur_ratio)\n",
        "                if blur % 2 == 0:\n",
        "                    blur += 1\n",
        "                blur_img = cv2.boxFilter(pad_img, 0, ksize=(blur, blur))\n",
        "                # blur_img = cv2.GaussianBlur(pad_img, (blur, blur), 0)\n",
        "\n",
        "                pad_img = pad_img.astype('float32')\n",
        "                pad_img += (blur_img - pad_img) * np.clip(mask * 3.0 + 1.0, 0.0, 1.0)\n",
        "                pad_img += (np.median(pad_img, axis=(0, 1)) - pad_img) * np.clip(mask, 0.0, 1.0)\n",
        "                pad_img = np.clip(pad_img, 0, 255)  # float32, [0, 255]\n",
        "                self.pad_input_imgs.append(pad_img)\n",
        "            else:\n",
        "                self.pad_input_imgs.append(np.copy(self.input_img))\n",
        "\n",
        "    return len(self.all_landmarks_5)\n",
        "  def align_warp_face(self, save_cropped_path=None, border_mode='constant'):\n",
        "    \"\"\"Align and warp faces with face template.\n",
        "    \"\"\"\n",
        "    if self.pad_blur:\n",
        "        assert len(self.pad_input_imgs) == len(\n",
        "            self.all_landmarks_5), f'Mismatched samples: {len(self.pad_input_imgs)} and {len(self.all_landmarks_5)}'\n",
        "    for idx, landmark in enumerate(self.all_landmarks_5):\n",
        "        # use 5 landmarks to get affine matrix\n",
        "        # use cv2.LMEDS method for the equivalence to skimage transform\n",
        "        # ref: https://blog.csdn.net/yichxi/article/details/115827338\n",
        "        affine_matrix = cv2.estimateAffinePartial2D(landmark, self.face_template, method=cv2.LMEDS)[0]\n",
        "        self.affine_matrices.append(affine_matrix)\n",
        "        # warp and crop faces\n",
        "        if border_mode == 'constant':\n",
        "            border_mode = cv2.BORDER_CONSTANT\n",
        "        elif border_mode == 'reflect101':\n",
        "            border_mode = cv2.BORDER_REFLECT101\n",
        "        elif border_mode == 'reflect':\n",
        "            border_mode = cv2.BORDER_REFLECT\n",
        "        if self.pad_blur:\n",
        "            input_img = self.pad_input_imgs[idx]\n",
        "        else:\n",
        "            input_img = self.input_img\n",
        "        cropped_face = cv2.warpAffine(\n",
        "            input_img, affine_matrix, self.face_size, borderMode=border_mode, borderValue=(135, 133, 132))  # gray\n",
        "        self.cropped_faces.append(cropped_face)\n",
        "        # save the cropped face\n",
        "        if save_cropped_path is not None:\n",
        "            path = os.path.splitext(save_cropped_path)[0]\n",
        "            save_path = f'{path}_{idx:02d}.{self.save_ext}'\n",
        "            imwrite(cropped_face, save_path)\n",
        "  def add_restored_face(self, face):\n",
        "    self.restored_faces.append(face)\n",
        "  def get_inverse_affine(self, save_inverse_affine_path=None):\n",
        "    \"\"\"Get inverse affine matrix.\"\"\"\n",
        "    for idx, affine_matrix in enumerate(self.affine_matrices):\n",
        "        inverse_affine = cv2.invertAffineTransform(affine_matrix)\n",
        "        inverse_affine[:, 2]*= self.upscale_factor\n",
        "        #inverse_affine *= self.upscale_factor\n",
        "        self.inverse_affine_matrices.append(inverse_affine)\n",
        "        # save inverse affine matrices\n",
        "        if save_inverse_affine_path is not None:\n",
        "            path, _ = os.path.splitext(save_inverse_affine_path)\n",
        "            save_path = f'{path}_{idx:02d}.pth'\n",
        "            torch.save(inverse_affine, save_path)\n",
        "  def paste_faces_to_input_image(self, save_path=None):\n",
        "    h, w, _ = self.input_img.shape\n",
        "    h_up, w_up = int(h * self.upscale_factor), int(w * self.upscale_factor)\n",
        "\n",
        "    upsample_img = self.nXimage\n",
        "\n",
        "    assert len(self.restored_faces) == len(\n",
        "        self.inverse_affine_matrices), ('length of restored_faces and affine_matrices are different.')\n",
        "    maskpool=None\n",
        "    restorepool=None\n",
        "    for restored_face, inverse_affine in zip(self.restored_faces, self.inverse_affine_matrices):\n",
        "        \n",
        "        if (inverse_affine[0][0]*self.upscale_factor) < 1.5:\n",
        "          inverse_affine[:, 2]/= self.upscale_factor\n",
        "          inverse_affine*=self.upscale_factor\n",
        "          restored_face=restored_face.astype('uint8')\n",
        "        else:\n",
        "          restored_face=srproc(restored_face,self.upscale_factor).astype('uint8')\n",
        "          \n",
        "\n",
        "        if self.upscale_factor > 1:\n",
        "            extra_offset = 0.5 * self.upscale_factor\n",
        "        else:\n",
        "            extra_offset = 0\n",
        "        inverse_affine[:, 2] += extra_offset\n",
        "        \n",
        "        inv_restored = cv2.warpAffine(restored_face, inverse_affine, (w_up, h_up))\n",
        "\n",
        "        \n",
        "        # inference\n",
        "        face_input = cv2.resize(restored_face, (512, 512), interpolation=cv2.INTER_LINEAR)\n",
        "        face_input = img2tensor(face_input.astype('float32') / 255., bgr2rgb=True, float32=True)\n",
        "        normalize(face_input, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5), inplace=True)\n",
        "        face_input = torch.unsqueeze(face_input, 0).to(cudevg)\n",
        "        with torch.no_grad():\n",
        "            out = face_parse(face_input)[0]\n",
        "        out = out.argmax(dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "        mask = np.zeros(out.shape)\n",
        "        MASK_COLORMAP = [0, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 0, 255, 0, 0, 0]\n",
        "        for idx, color in enumerate(MASK_COLORMAP):\n",
        "            mask[out == idx] = color\n",
        "        #  blur the mask\n",
        "        mask = cv2.GaussianBlur(mask, (101, 101), 11)\n",
        "        mask = cv2.GaussianBlur(mask, (101, 101), 11)\n",
        "        # remove the black borders\n",
        "        thres = 10\n",
        "        mask[:thres, :] = 0\n",
        "        mask[-thres:, :] = 0\n",
        "        mask[:, :thres] = 0\n",
        "        mask[:, -thres:] = 0\n",
        "        mask = mask / 255.\n",
        "\n",
        "        mask = cv2.resize(mask, restored_face.shape[:2])\n",
        "        mask = cv2.warpAffine(mask, inverse_affine, (w_up, h_up), flags=3)\n",
        "        inv_soft_mask = mask[:, :, None]\n",
        "        pasted_face = inv_restored\n",
        "\n",
        "        if maskpool is None:\n",
        "          maskpool=inv_soft_mask\n",
        "          restorepool=np.zeros(inv_soft_mask.shape)\n",
        "          blanc=np.ones(inv_soft_mask.shape)\n",
        "        else:\n",
        "          maskpool = inv_soft_mask*blanc+(1 - inv_soft_mask)*maskpool\n",
        "\n",
        "        inv_hard_mask=np.array(inv_soft_mask, copy=True)\n",
        "        inv_hard_mask[np.where(inv_hard_mask!=0)]=1.0\n",
        "        restorepool = inv_hard_mask * pasted_face + (1 - inv_hard_mask) * restorepool\n",
        "\n",
        "    if np.max(upsample_img) > 256:  # 16-bit image\n",
        "        upsample_img = np.concatenate((restorepool, maskpool*65535), axis=2).astype(np.uint16)\n",
        "    else:\n",
        "        upsample_img = np.concatenate((restorepool, maskpool*255), axis=2).astype(np.uint8)\n",
        "    if save_path is not None:\n",
        "        path = os.path.splitext(save_path)[0]\n",
        "        save_path = f'{path}.{self.save_ext}'\n",
        "        imwrite(upsample_img, save_path)\n",
        "    return upsample_img\n",
        "\n",
        "\n",
        "def tensor2img(tensor, rgb2bgr=True, out_type=np.uint8, min_max=(0, 1)):\n",
        "\n",
        "    if not (torch.is_tensor(tensor) or (isinstance(tensor, list) and all(torch.is_tensor(t) for t in tensor))):\n",
        "        raise TypeError(f'tensor or list of tensors expected, got {type(tensor)}')\n",
        "\n",
        "    if torch.is_tensor(tensor):\n",
        "        tensor = [tensor]\n",
        "    result = []\n",
        "    for _tensor in tensor:\n",
        "        _tensor = _tensor.squeeze(0).float().detach().cpu().clamp_(*min_max)\n",
        "        _tensor = (_tensor - min_max[0]) / (min_max[1] - min_max[0])\n",
        "\n",
        "        n_dim = _tensor.dim()\n",
        "        if n_dim == 4:\n",
        "            img_np = make_grid(_tensor, nrow=int(math.sqrt(_tensor.size(0))), normalize=False).numpy()\n",
        "            img_np = img_np.transpose(1, 2, 0)\n",
        "            if rgb2bgr:\n",
        "                img_np = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
        "        elif n_dim == 3:\n",
        "            img_np = _tensor.numpy()\n",
        "            img_np = img_np.transpose(1, 2, 0)\n",
        "            if img_np.shape[2] == 1:  # gray image\n",
        "                img_np = np.squeeze(img_np, axis=2)\n",
        "            else:\n",
        "                if rgb2bgr:\n",
        "                    img_np = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
        "        elif n_dim == 2:\n",
        "            img_np = _tensor.numpy()\n",
        "        else:\n",
        "            raise TypeError(f'Only support 4D, 3D or 2D tensor. But received with dimension: {n_dim}')\n",
        "        if out_type == np.uint8:\n",
        "            # Unlike MATLAB, numpy.unit8() WILL NOT round by default.\n",
        "            img_np = (img_np * 255.0).round()\n",
        "        img_np = img_np.astype(out_type)\n",
        "        result.append(img_np)\n",
        "    if len(result) == 1:\n",
        "        result = result[0]\n",
        "    return result\n",
        "\n",
        "def doenh_gfp(cropped_face_t):\n",
        "  global gfpgan_enc\n",
        "  global gfpgan_dec\n",
        "  if gfpgan_enc is None:\n",
        "    gfpgan_enc =torch.jit.load('gfpgan_enc_pnnx.pt').eval().to(cudevg)\n",
        "    gfpgan_dec =torch.jit.load('gfpgan_dec_pnnx.pt').eval().to(cudevg)\n",
        "  latent, conditions = gfpgan_enc(cropped_face_t)\n",
        "  output = gfpgan_dec(latent,*conditions)\n",
        "  return output\n",
        "\n",
        "doenh=doenh_gfp\n",
        "\n",
        "@torch.no_grad()\n",
        "def enhance(img, has_aligned=False, only_center_face=False, paste_back=True):\n",
        "  \n",
        "  faces=faceimg(img)\n",
        "\n",
        "  if has_aligned:  # the inputs are already aligned\n",
        "      img = cv2.resize(img, (512, 512))\n",
        "      faces.cropped_faces = [img]\n",
        "  else:\n",
        "      faces.get_face_landmarks_5(only_center_face=only_center_face, eye_dist_threshold=5)\n",
        "      # eye_dist_threshold=5: skip faces whose eye distance is smaller than 5 pixels\n",
        "      # TODO: even with eye_dist_threshold, it will still introduce wrong detections and restorations.\n",
        "      # align and warp each face\n",
        "      faces.align_warp_face()\n",
        "\n",
        "  # face restoration\n",
        "  for cropped_face in faces.cropped_faces:\n",
        "      # prepare data\n",
        "      cropped_face_t = img2tensor(cropped_face / 255., bgr2rgb=True, float32=True)\n",
        "      normalize(cropped_face_t, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5), inplace=True)\n",
        "      cropped_face_t = cropped_face_t.unsqueeze(0).to(cudevg)\n",
        "\n",
        "      output = doenh(cropped_face_t)\n",
        "      restored_face = tensor2img(output[0].cpu(), rgb2bgr=True, min_max=(-1, 1))\n",
        "  \n",
        "\n",
        "      restored_face = restored_face\n",
        "      faces.add_restored_face(restored_face)\n",
        "\n",
        "  if not has_aligned and paste_back:\n",
        "      # upsample the background\n",
        "      \n",
        "\n",
        "      faces.get_inverse_affine(None)\n",
        "      # paste each restored face to the input image\n",
        "      restored_img = faces.paste_faces_to_input_image()\n",
        "      return faces, restored_img\n",
        "  else:\n",
        "      return faces, None\n",
        "\n",
        "\n",
        "\n",
        "if not os.path.isfile('retinaface_pnnx.pt'):\n",
        "  !wget https://huggingface.co/Larvik/GFPGANjit/resolve/main/face_parse_pnnx.pt\n",
        "  !wget https://huggingface.co/Larvik/GFPGANjit/resolve/main/gfpgan_dec_pnnx.pt\n",
        "  !wget https://huggingface.co/Larvik/GFPGANjit/resolve/main/gfpgan_enc_pnnx.pt\n",
        "  !wget https://huggingface.co/Larvik/GFPGANjit/resolve/main/retinaface_pnnx.pt\n",
        "\n",
        "GFPgan_device='cpu' #@param ['cpu', 'cuda']\n",
        "cudevg=torch.device(GFPgan_device)\n",
        "\n",
        "\n",
        "gfpgan_enc=None\n",
        "RetinaFace =torch.jit.load('retinaface_pnnx.pt').eval().to(cudevg)\n",
        "face_parse =torch.jit.load('face_parse_pnnx.pt').eval().to(cudevg)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "yKhdSfpTmsiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input='/content/aaa2_4x.png' #@param {type:'string'}\n",
        "output='results'\n",
        "\n",
        "upscale=4\n",
        "suffix=None\n",
        "only_center_face=False\n",
        "aligned=False\n",
        "ext='auto'\n",
        "\n",
        "\n",
        "# ------------------------ input & output ------------------------\n",
        "if input.endswith('/'):\n",
        "    input = input[:-1]\n",
        "if os.path.isfile(input):\n",
        "    img_list = [input]\n",
        "else:\n",
        "    img_list = sorted(glob.glob(os.path.join(input, '*')))\n",
        "\n",
        "os.makedirs(output, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------ restore ------------------------\n",
        "for img_path in img_list:\n",
        "    # read image\n",
        "    img_name = os.path.basename(img_path)\n",
        "    print(f'Processing {img_name} ...')\n",
        "    basename, ext = os.path.splitext(img_name)\n",
        "    input_img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "\n",
        "    # restore faces and background if necessary\n",
        "    faces, restored_img = enhance(input_img, has_aligned=aligned, only_center_face=only_center_face, paste_back=True)\n",
        "\n",
        "    # save faces\n",
        "    for idx, (cropped_face, restored_face) in enumerate(zip(faces.cropped_faces, faces.restored_faces)):\n",
        "        # save cropped face\n",
        "        save_crop_path = os.path.join(output, 'cropped_faces', f'{basename}_{idx:02d}.png')\n",
        "        imwrite(cropped_face, save_crop_path)\n",
        "        # save restored face\n",
        "        if suffix is not None:\n",
        "            save_face_name = f'{basename}_{idx:02d}_{suffix}.png'\n",
        "        else:\n",
        "            save_face_name = f'{basename}_{idx:02d}.png'\n",
        "        save_restore_path = os.path.join(output, 'restored_faces', save_face_name)\n",
        "        imwrite(restored_face, save_restore_path)\n",
        "        # save comparison image\n",
        "        cmp_img = np.concatenate((cropped_face, restored_face), axis=1)\n",
        "        imwrite(cmp_img, os.path.join(output, 'cmp', f'{basename}_{idx:02d}.png'))\n",
        "\n",
        "    # save restored img\n",
        "    if restored_img is not None:\n",
        "        if ext == 'auto':\n",
        "            extension = ext[1:]\n",
        "        else:\n",
        "            extension = ext\n",
        "\n",
        "        if suffix is not None:\n",
        "            save_restore_path = os.path.join(output, 'restored_imgs', f'{basename}_{suffix}.png')\n",
        "        else:\n",
        "            save_restore_path = os.path.join(output, 'restored_imgs', f'{basename}.png')\n",
        "        imwrite(restored_img, save_restore_path)\n",
        "\n",
        "print(f'Results are in the [{output}] folder.')\n"
      ],
      "metadata": {
        "id": "OOqZ0K2Rm_G4",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# txt2img"
      ],
      "metadata": {
        "id": "PWCzxzNkYpEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SDver='470k' #@param ['440k', '470k']\n",
        "Dfm='Orig' #@param ['Orig','_inpaint','_imgemb','_a19561','_a17750','_a17750_e9750','_e26500','_z313000']\n",
        "DfmCodeBase='JIT' #@param ['JIT', 'ldm_xformers']\n",
        "EnableKVmerges=False #@param {type:'boolean'}\n",
        "\n",
        "INP=False\n",
        "\n",
        "if Dfm=='Orig':\n",
        "  Dfm=''\n",
        "elif Dfm=='_inpaint':\n",
        "  INP=True\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "SDver=f_dljit(SDver,Dfm)\n",
        "\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import PIL\n",
        "from IPython.core.display import HTML\n",
        "from transformers import CLIPTokenizer\n",
        "\n",
        "import torch\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "torch.set_num_threads(os.cpu_count())\n",
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True\n",
        "\n",
        "alphas_cumprod = np.load('alphas_cumprod.npz')['a']\n",
        "\n",
        "cudev=torch.device('cuda')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "preimg=None\n",
        "revpreimg=None\n",
        "\n",
        "\n",
        "from accelerate import init_empty_weights\n",
        "with init_empty_weights():\n",
        "  preprocK=PreKV(768)\n",
        "  preprocV=PreKV(768)\n",
        "\n",
        "# ddpm\n",
        "def apply_model_jit(x, t, cond,d):\n",
        "\n",
        "  x_in=x\n",
        "\n",
        "  if INP:\n",
        "    x_in = torch.cat([x, image_embed], dim=1)\n",
        "\n",
        "  h, emb, hs = diffusion_emb(x_in, t, cond)\n",
        "  \n",
        "  h = diffusion_mid(h, emb, cond, *hs[6:])\n",
        "  hlog0.Arevpre(h,d)\n",
        "  output = diffusion_out(h, emb, cond, *hs[:6])\n",
        "\n",
        "  return output\n",
        "\n",
        "def apply_model_ldm(x, t, cond,d):\n",
        "\n",
        "  x_in=x\n",
        "\n",
        "  if INP:\n",
        "    x_in = torch.cat([x, image_embed], dim=1)\n",
        "\n",
        "  cond_k=cond\n",
        "  cond_v=cond\n",
        "  if preprocK.use:\n",
        "    cond_k=preprocK(cond)\n",
        "    cond_v=preprocV(cond)\n",
        "\n",
        "\n",
        "  h, emb, hs = ldm_unet.forward_crossattn(x_in, t, cond_k,cond_v) \n",
        "  \n",
        "  h = ldm_unet.forward2(h, emb, cond_k, *hs[6:],cond_v) \n",
        "  hlog0.Arevpre(h,d)\n",
        "  output = ldm_unet.forward3(h, emb, cond_k, *hs[:6],cond_v) \n",
        "\n",
        "  return output\n",
        "\n",
        "apply_model=apply_model_jit\n",
        "\n",
        "preview_mtx = torch.tensor( [\n",
        "    #   R       G       B\n",
        "    [ 0.298,  0.207,  0.208],  # L1\n",
        "    [ 0.187,  0.286,  0.173],  # L2\n",
        "    [-0.158,  0.189,  0.264],  # L3\n",
        "    [-0.184, -0.271, -0.473],  # L4\n",
        "],device=cudev)\n",
        "\n",
        "# decoder\n",
        "def decode_first_stage(z, hsz=-1):\n",
        "  if hsz < 0:\n",
        "    hsz=(n_samples*H*W)\n",
        "  if hsz>0x200000:\n",
        "    return (z.permute(0,2,3,1) @ preview_mtx).permute(0,3,1,2)\n",
        "\n",
        "  output = autoencoder(z/0.18215)\n",
        "  return output\n",
        "\n",
        "\n",
        "\n",
        "def load_img(path):\n",
        "    image = Image.open(path).convert(\"RGB\")\n",
        "    w, h = image.size\n",
        "    print(f\"loaded input image of size ({w}, {h}) from {path}\")\n",
        "    w2, h2 = map(lambda x: x - x % 32, (w, h))\n",
        "    if w!=w2 or h!=h2:\n",
        "      image = image.resize((w2, h2), resample=PIL.Image.LANCZOS)\n",
        "    image = np.array(image).astype(np.float32) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    return 2.*image - 1.\n",
        "\n",
        "def cutorexpand(tenz,dstbsz):\n",
        "  retsz=tenz.size(0)\n",
        "  if retsz > dstbsz:\n",
        "    return tenz[:dstbsz]\n",
        "  elif retsz < dstbsz:\n",
        "    return tenz[:1].expand(dstbsz,-1,-1,-1)\n",
        "\n",
        "def fiximgemb(nshape):\n",
        "  ret=image_embed\n",
        "  \n",
        "  if tuple(ret.shape[2:]) != nshape:\n",
        "    ret = torch.nn.functional.interpolate(ret ,size=nshape,mode='bicubic')\n",
        "  dstbsz=(noise.size(0)<<1)\n",
        "  ret=cutorexpand(ret,dstbsz)\n",
        "\n",
        "  return ret\n",
        "\n",
        "def waitingmask():\n",
        "  while not os.path.isfile('/content/user_mask.npy'):\n",
        "    time.sleep(5)\n",
        "\n",
        "def findnpy(maskpath):\n",
        "  if maskpath.endswith('.npy'):\n",
        "    return torch.tensor( np.load(maskpath).astype(np.float32) )\n",
        "  maa=maskpath[:-3]+'npy'\n",
        "  if os.path.isfile(maa):\n",
        "    return torch.tensor( np.load(maa).astype(np.float32) )\n",
        "  if os.path.isfile(maskpath):\n",
        "    mask = Image.open(maskpath).convert('RGB')\n",
        "    mask = mask.resize(( lat.size(3) , lat.size(2) )).point( lambda p: 255 if p > 128 else 0 ).convert('1')\n",
        "    return torch.tensor(np.array(mask).astype(np.float32))\n",
        "  else:\n",
        "    print(\"Make a mask with webui (u shouldn't waste colab gpu time doing such thing)\")\n",
        "    localhttp()\n",
        "    if os.path.isfile('web/svr.py'):\n",
        "      runpyproc('web/svr')\n",
        "    web_masking()\n",
        "    waitingmask()\n",
        "    return torch.tensor( np.load('/content/user_mask.npy').astype(np.float32) )\n",
        "\n",
        "\n",
        "\n",
        "def do_masking(maskpath, lat=None):\n",
        "  if lat is None:\n",
        "    lat=preimg\n",
        "  mask = findnpy(maskpath)\n",
        "  \n",
        "    \n",
        "  return (lat*mask).cuda()\n",
        "\n",
        "\n",
        "def encodepatt():\n",
        "  ozi=output_pattern.split('/')[-1]\n",
        "  pdir=output_pattern[:-len(ozi)-1]\n",
        "  flist=os.listdir(pdir)\n",
        "  flist.sort()\n",
        "  pdir+='/'\n",
        "  rpt=load_img(pdir+flist[0])\n",
        "  vB=1\n",
        "  vH=rpt.size(2)\n",
        "  vW=rpt.size(3)\n",
        "  thsize=torch.Size([vB,4,vH>>3,vW>>3])\n",
        "  noyaz=torch.randn(thsize)\n",
        "  zadd=0\n",
        "  for f in flist:\n",
        "    if f.endswith('.png'):\n",
        "      vlat=imgenc(  load_img( pdir+f) ,  noyaz )*0.18215\n",
        "      vlat.numpy().tofile(pdir+f[:-3]+'bin')\n",
        "      zadd+=1\n",
        "  with open(output_pattern[:-3].replace('%','!@!')+'txt','wt') as f:\n",
        "    f.write(str(list(thsize))[1:-1]+'\\n'+str(zadd))\n",
        "  !rm {pdir}*.png\n",
        "\n",
        "\n",
        "\n",
        "def warmup():\n",
        "  global image_embed\n",
        "  v_0 = torch.rand(2, 4, 32, 32, dtype=torch.float).half().cuda()\n",
        "  if INP:\n",
        "    image_embed=v_0\n",
        "  v_1 = torch.randint(10, (2, ), dtype=torch.long).cuda()\n",
        "  v_2 = torch.rand(2, 77, 768, dtype=torch.float).half().cuda()\n",
        "  for d in range(2):\n",
        "    with torch.cuda.amp.autocast(dtype=torch.float16):\n",
        "      uaa = apply_model(v_0,v_1,v_2,d)\n",
        "  v_0 = torch.rand(1, 4, 32, 32, dtype=torch.float).cuda()\n",
        "  for d in range(2):\n",
        "    uaa = autoencoder(v_0)\n",
        "  if INP:\n",
        "    del image_embed\n",
        "  torch.cuda.empty_cache()\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "fext='_%dx%dv%d.png'\n",
        "def saver():\n",
        "  global x_samples\n",
        "  i=iita\n",
        "  np.save( (outputp+fext%(i,1,ktta))[:-4] + '.npy', samples)\n",
        "  x_samples = np.clip((x_samples.numpy() + 1.0) / 2.0, a_min=0.0, a_max=1.0)\n",
        "  k=0\n",
        "  for x_sample in x_samples:\n",
        "      x_sample = x_sample.transpose(1, 2, 0)  # CHW -> HWC\n",
        "      x_sample = x_sample * 255\n",
        "      img = x_sample.astype(np.uint8)\n",
        "      img = img[:, :, ::-1]  # RGB -> BGR\n",
        "      cv2.imwrite(outputp+fext%(i,k,ktta), img)\n",
        "      k+=1\n",
        "\n",
        "\n",
        "\n",
        "fextWENS='_%s_%dx%dv%d.png'\n",
        "WENSsig=['W','E','N','S']\n",
        "def saverWENS():\n",
        "  global x_samples\n",
        "  i=iita\n",
        "  otp2=init_img[:-4]\n",
        "  dfn=fn\n",
        "  if dfn>3:\n",
        "    dfn-=4\n",
        "  sigg=WENSsig[dfn]\n",
        "  np.save( (otp2+fextWENS%(sigg,i,1,ktta))[:-4] + '.npy', samples)\n",
        "  x_samples = np.clip((x_samples.numpy() + 1.0) / 2.0, a_min=0.0, a_max=1.0)\n",
        "  k=0\n",
        "  for x_sample in x_samples:\n",
        "      x_sample = x_sample.transpose(1, 2, 0)  # CHW -> HWC\n",
        "      x_sample = x_sample * 255\n",
        "      img = x_sample.astype(np.uint8)\n",
        "      img = img[:, :, ::-1]  # RGB -> BGR\n",
        "      cv2.imwrite(otp2+fextWENS%(sigg,i,k,ktta), img)\n",
        "      k+=1\n",
        "\n",
        "\n",
        "UseSamplr=sample_lms\n",
        "\n",
        "def predict(c_list, uc,noi=None):\n",
        "    global x_samples\n",
        "    global samples\n",
        "    global ktta\n",
        "    global tmpfeeder\n",
        "    global noise\n",
        "\n",
        "\n",
        "\n",
        "    feeder=ifeeder()\n",
        "\n",
        "    sigmas = f_sigmas()\n",
        "\n",
        "    shape_alters=[]\n",
        "    if len(shape) > 4:\n",
        "      shape_r=shape[:4]\n",
        "      shape_alters=shape[4:]\n",
        "    else:\n",
        "      shape_r=shape\n",
        "\n",
        "    if noi is None:\n",
        "      noise = torch.randn(shape_r, dtype=torch.float,device=cudev)\n",
        "    else:\n",
        "      noise = noi\n",
        "    if preimg is not None:\n",
        "      vt_enc= t_enc-1\n",
        "      sigma_sched = sigmas[ddim_num_steps - vt_enc - 1:]\n",
        "      if preimg.dim()==1:\n",
        "        cmd0=int(preimg[0])\n",
        "        if cmd0 == 2:\n",
        "          feeder.pattern=tmpfeeder.pattern\n",
        "          feeder.shape=tmpfeeder.shape\n",
        "          feeder.getn=feeder.get_npbins\n",
        "          feeder.noiseadd=noise * sigmas[ddim_num_steps - vt_enc - 1]\n",
        "          c_list=[c_list[0]]*tmpfeeder.xpenlen\n",
        "          feeder.xpenlen=tmpfeeder.xpenlen\n",
        "          tmpfeeder=feeder\n",
        "      else:\n",
        "        img = preimg.cuda() + noise * sigmas[ddim_num_steps - vt_enc - 1]\n",
        "        feeder.setbs(img)\n",
        "    else:\n",
        "      img = noise*sigmas[0]\n",
        "      sigma_sched=sigmas\n",
        "      feeder.setbs(img)\n",
        "\n",
        "\n",
        "    ktta=0\n",
        "    for c in c_list:\n",
        "      c.reset()\n",
        "      samples = zemp0(feeder.getn(ktta),c,sigma_sched,shape_alters)\n",
        "      ktta+=1\n",
        "      \n",
        "      \n",
        "      x_samples = decode_first_stage(  samples ).cpu()\n",
        "      samples=samples.cpu()\n",
        "      t3 = Thread(target = saver)\n",
        "      a3 = t3.start()\n",
        "\n",
        "    return\n",
        "predict_orig=predict\n",
        "\n",
        "def zemp(noi,c,sigmas):\n",
        "  global noise\n",
        "  noise=noi\n",
        "  extra_args = {'cond': c, 'uncond': uc, 'cond_scale': cfg_scale}\n",
        "  c.reset()\n",
        "  with torch.cuda.amp.autocast(dtype=torch.float16):\n",
        "    samples = UseSamplr(model_wrap_cfg, noise*sigmas[0], sigmas, extra_args=extra_args, disable=False)\n",
        "  return samples\n",
        "\n",
        "\n",
        "def ConcatNoise_shuf(noi, new_h, new_w):\n",
        "  shape_new = list(noi.shape)\n",
        "  m64_orig=(shape_new[-1]*shape_new[-2])>>6\n",
        "  shape_new[-1]=m64_orig\n",
        "  shape_new[-2]=64\n",
        "  r1=noi.permute(1,3,0,2).reshape(shape_new)\n",
        "\n",
        "  shape_new[-1]=new_w\n",
        "  shape_new[-2]=new_h\n",
        "  m64_new=(new_w*new_h)>>6\n",
        "  dup=int(0.9999+(m64_new/m64_orig))\n",
        "  return (torch.cat([r1]*dup,dim=3)[:,:,:,:m64_new]).permute(1,3,0,2).reshape(shape_new)\n",
        "\n",
        "def ConcatNoise_rdm(noi, new_h, new_w):\n",
        "  shape_r=list(noi.shape)\n",
        "  shape_r[-1]=new_w\n",
        "  shape_r[-2]=new_h\n",
        "  return torch.randn(shape_r, dtype=torch.float,device=cudev)\n",
        "\n",
        "ConcatNoise=ConcatNoise_shuf\n",
        "\n",
        "def zemp0(noi,c,sigmas,shape_alters):\n",
        "  global noise\n",
        "  global image_embed\n",
        "  samples=noi\n",
        "  curnoise=noise\n",
        "  shape_alt=[[(0,0),0,None,0]]\n",
        "  if len(shape_alters) > 0:\n",
        "    sigml=len(sigmas)\n",
        "    l2=len(shape_alters)//3\n",
        "    shape_alt=[None]*(l2+1)\n",
        "    shape_alt[0]=[(0,0),0,None,0]\n",
        "    for t in range(l2):\n",
        "      shape_alt[t+1]=[(0,0),None,None,0]\n",
        "      shape_alt[t+1][0]= ( shape_alters[t*3+1], shape_alters[t*3+2] )\n",
        "      tima=int(sigml*shape_alters[t*3])\n",
        "      shape_alt[t+1][3]=shape_alters[t*3]\n",
        "      shape_alt[t][2]=tima\n",
        "      shape_alt[t+1][1]=tima-1\n",
        "      shape_alt[t]=tuple(shape_alt[t])\n",
        "    dsth,dstw=shape_alt[-1][0]\n",
        "  else:\n",
        "    dsth=curnoise.size(2)\n",
        "    dstw=curnoise.size(3)\n",
        "  shape_alt[-1]=tuple(shape_alt[-1])\n",
        "\n",
        "\n",
        "  extra_args = {'cond': c, 'uncond': uc, 'cond_scale': cfg_scale}\n",
        "  with torch.cuda.amp.autocast(dtype=torch.float16):\n",
        "    kot1=0\n",
        "    for r_shape, sta, endo, o_rate in shape_alt:\n",
        "      new_h,new_w=r_shape\n",
        "      v_sta=sta>>1\n",
        "      if o_rate > 0.4:\n",
        "        v_sta=int(sta*o_rate)\n",
        "      sub_sigma=sigmas[v_sta:endo]\n",
        "      if new_h != 0:\n",
        "        c.add_sta=v_sta\n",
        "        noise_new=ConcatNoise(curnoise,new_h,new_w)\n",
        "        noise = noise_new\n",
        "        samples=torch.nn.functional.interpolate(samples-curnoise*sigmas[sta] ,size=r_shape,mode='bicubic')+noise_new*sub_sigma[0]\n",
        "        curnoise=noise_new\n",
        "      else:\n",
        "        new_h=curnoise.size(2)\n",
        "        new_w=curnoise.size(3)\n",
        "      if INP:\n",
        "        image_embed = fiximgemb((new_h,new_w))\n",
        "\n",
        "      hlog0.set_multinm(kot1,new_h,dsth,new_w,dstw)\n",
        "      samples = UseSamplr(model_wrap_cfg, samples, sub_sigma, extra_args=extra_args, disable=False)\n",
        "      kot1+=1\n",
        "  return samples\n",
        "\n",
        "def calcsz(tenz):\n",
        "  return (tenz.size(0)*tenz.size(2)*tenz.size(3))<<6\n",
        "\n",
        "def UnCrop(c_list, uc):\n",
        "  global x_samples\n",
        "  global samples\n",
        "  global ktta\n",
        "  global fn\n",
        "  global image_embed\n",
        "\n",
        "  if not INP:\n",
        "    print('Not inpainting weights, fail back to old buggy dumb uncrop')\n",
        "    UnCrop_old(c_list, uc)\n",
        "    return\n",
        "\n",
        "  \n",
        "  sigmas = f_sigmas()\n",
        "\n",
        "  a4=area4(preimg)\n",
        "  noise_whole = torch.randn(a4.skey, dtype=torch.float,device=cudev)\n",
        "\n",
        "  embdstsz=noise_whole.size(0)<<1\n",
        "  zero_whole=torch.zeros(a4.skey, dtype=torch.float,device=cudev)\n",
        "  zero_whole[:,:,a4.Npad:a4.O_h+a4.Npad,a4.Wpad:a4.O_w+a4.Wpad]=preimg\n",
        "\n",
        "  corner8=a4.getshapes()\n",
        "  ktta=0\n",
        "  for c in c_list:\n",
        "    nxktta=ktta+1\n",
        "    for kU,kB,kL,kR,fn in corner8:\n",
        "\n",
        "      image_embed=cutorexpand(zero_whole[:,:,kU:kB,kL:kR],  embdstsz )\n",
        "      samples=zemp(noise_whole[:,:,kU:kB,kL:kR],c,sigmas)\n",
        "      zero_whole[:,:,kU:kB,kL:kR]=samples\n",
        "      zero_whole[:,:,a4.Npad:a4.O_h+a4.Npad,a4.Wpad:a4.O_w+a4.Wpad]=preimg\n",
        "\n",
        "      x_samples = decode_first_stage(  samples, calcsz(noise) ).cpu()\n",
        "      samples=samples.cpu()\n",
        "      ktta=nxktta\n",
        "      t3 = Thread(target = saverWENS)\n",
        "      a3 = t3.start()\n",
        "    x_samples = decode_first_stage(  zero_whole, calcsz(zero_whole) ).cpu()\n",
        "    samples=zero_whole.cpu()\n",
        "    ktta=999\n",
        "    t3 = Thread(target = saver)\n",
        "    a3 = t3.start()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def UnCrop_old(c_list, uc):\n",
        "  global x_samples\n",
        "  global samples\n",
        "  global ktta\n",
        "  global fn\n",
        "  global hlog0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  sigmas = f_sigmas()\n",
        "\n",
        "  a4=area4(preimg)\n",
        "  noise_whole = torch.randn(a4.skey, dtype=torch.float,device=cudev)\n",
        "  zero_whole=torch.zeros(a4.skey, dtype=torch.float,device=cudev)\n",
        "  zero_whole[:,:,a4.Npad:a4.O_h+a4.Npad,a4.Wpad:a4.O_w+a4.Wpad]=preimg\n",
        "\n",
        "\n",
        "  hlog0.h_bs=[None]*ddim_num_steps\n",
        "  corner8=a4.getshapes()\n",
        "\n",
        "  ktta=0\n",
        "  for c in c_list:\n",
        "    if CopyDFout:\n",
        "      hlog0.setfuncb('1s')\n",
        "      hlog0.setfunc('loghs')\n",
        "\n",
        "    zemp(noise_whole[:,:,a4.Npad:a4.O_h+a4.Npad,a4.Wpad:a4.O_w+a4.Wpad],c,sigmas)\n",
        "    hlog0.setbsB(-10,preimg)\n",
        "\n",
        "    hlog0.setfuncb('0')\n",
        "    nxktta=ktta+1\n",
        "    for kU,kB,kL,kR,fn in corner8:\n",
        "      if CopyDFout:\n",
        "        hlog0.setfuncN(fn)\n",
        "      hlog0.setfuncNb(fn)\n",
        "      samples=zemp(noise_whole[:,:,kU:kB,kL:kR],c,sigmas)\n",
        "\n",
        "      zero_whole[:,:,kU:kB,kL:kR]=samples\n",
        "      #zero_whole[:,:,a4.Npad:a4.O_h+a4.Npad,a4.Wpad:a4.O_w+a4.Wpad]=preimg\n",
        "\n",
        "      hlog0.setbsB(fn,samples)\n",
        "      x_samples = decode_first_stage(  samples, calcsz(noise) ).cpu()\n",
        "      samples=samples.cpu()\n",
        "      ktta=nxktta\n",
        "      t3 = Thread(target = saverWENS)\n",
        "      a3 = t3.start()\n",
        "    x_samples = decode_first_stage(  zero_whole, calcsz(zero_whole) ).cpu()\n",
        "    samples=zero_whole.cpu()\n",
        "    ktta=999\n",
        "    t3 = Thread(target = saver)\n",
        "    a3 = t3.start()\n",
        "  hlog0=hlogger()\n",
        "\n",
        "\n",
        "def imgthumb(z):\n",
        "  x_sample= (z.permute(1,2,0) @ preview_mtx).permute(2,0,1)\n",
        "  x_sample = np.clip((x_sample.cpu().numpy() + 1.0) / 2.0, a_min=0.0, a_max=1.0)\n",
        "\n",
        "  x_sample = x_sample.transpose(1, 2, 0)  # CHW -> HWC\n",
        "  x_sample = x_sample * 255\n",
        "  img = x_sample.astype(np.uint8)\n",
        "  img = img[:, :, ::-1]  # RGB -> BGR\n",
        "  cv2.imwrite('/content/tb.png', img)\n",
        "\n",
        "def readhtml(fna):\n",
        "  with open(fna,'rt') as f:\n",
        "    kt=f.read()\n",
        "  return kt\n",
        "\n",
        "psg0=readhtml('web/psg0.htm')\n",
        "psg1=readhtml('web/psg1.htm')\n",
        "\n",
        "\n",
        "def web_masking():\n",
        "  if initymgtyp == 0:\n",
        "    popic=init_img[:-4]\n",
        "    if os.path.isfile(popic):\n",
        "      bsimg=popic\n",
        "    elif os.path.isfile(popic+'.png'):\n",
        "      bsimg=popic+'.png'\n",
        "    else:\n",
        "      imgthumb(preimg[0].cuda())\n",
        "      bsimg='tb.png'\n",
        "  elif initymgtyp == 1:\n",
        "    bsimg=init_img\n",
        "  strW=str(W>>3)\n",
        "  strH=str(H>>3)\n",
        "  bsimg=bsimg.replace('/content/','')\n",
        "  display(HTML(psg0+ strW +';\\nvar dYh='+strH+\";\\nvar imgfna='\"+bsimg+\"';\\n\\n\"+psg1))\n",
        "  with open('web/curmsk.txt','wt') as f:\n",
        "    f.write('\\n'.join([strW,strH,bsimg]))\n",
        "\n",
        "\n",
        "def dumplogs(lbg):\n",
        "  arnnstr='/content/sample_data/vyi'+str(lbg)\n",
        "  !mkdir {arnnstr}\n",
        "  lu=hlog0.latlog_arr[lbg]\n",
        "  lul=len(lu)-1\n",
        "  psta='0'\n",
        "  for n in range(lul,-1,-1):\n",
        "    if lu[n] is None:\n",
        "      psta=str(n)\n",
        "      break\n",
        "    Image.fromarray( (( ( latdec2(lu[n])[0] +1)*127.5 ).cpu().numpy()).transpose(1,2,0).clip(0,255).astype(np.uint8) ).save(arnnstr+'/stp%05d.png'%n)\n",
        "  !ffmpeg -framerate 3 -start_number {psta} -i {arnnstr}/stp%05d.png -pix_fmt yuv420p intp{psta}.mp4\n",
        "\n",
        "def EnLarge2seed_size(enl):\n",
        "  l_enl = len(enl)//3\n",
        "  if l_enl == 0:\n",
        "    return H, W, []\n",
        "  l_enlx3=l_enl*3\n",
        "  newH=enl[l_enlx3-3]\n",
        "  newW=enl[l_enlx3-2]\n",
        "  if l_enl > 1:\n",
        "    for i in range(l_enl-1,0,-1):\n",
        "      enl[i*3]=enl[i*3-3]\n",
        "      enl[i*3+1]=enl[i*3-2]\n",
        "\n",
        "  enl[0]=H\n",
        "  enl[1]=W\n",
        "  return newH,newW,enl\n",
        "\n",
        "\n",
        "def init_img_type():\n",
        "  global init_img\n",
        "  global tmpfeeder\n",
        "  if init_img.endswith('.npy'):\n",
        "    return 0\n",
        "  elif init_img.endswith('.jpg') or init_img.endswith('.png'):\n",
        "    if os.path.isfile(init_img+'.npy'):\n",
        "      init_img+='.npy'\n",
        "      return 0\n",
        "    else:\n",
        "      return 1\n",
        "  elif init_img.endswith('.txt'):\n",
        "    return 2\n",
        "  else:\n",
        "    return 99\n",
        "initymgtyp=99\n",
        "\n",
        "def fixb64(dx2,dx1):\n",
        "  ruz=dx1&7\n",
        "  if ruz !=0:\n",
        "    dx1-=ruz\n",
        "    dx2+=ruz>>1\n",
        "  return (dx2,dx1)\n",
        "\n",
        "\n",
        "def mkUnCrop4():\n",
        "  vbdr=UnCrop4\n",
        "  if len(vbdr) != 8:\n",
        "    vbdr+=[0,0,0,0]\n",
        "  refwh=[W,W,H,H]\n",
        "  for i in range(4):\n",
        "    bsex=vbdr[i]\n",
        "    if bsex<1:\n",
        "      refwh[i]=(0,0)\n",
        "      continue\n",
        "    x=vbdr[4+i]\n",
        "    if x==0:\n",
        "      x=refwh[i]>>1\n",
        "    x=(x>>4)+1\n",
        "    xrf=x<<4\n",
        "    bsex=(((xrf+bsex)&0xffc0)+64)-xrf\n",
        "    refwh[i]=fixb64(x,bsex>>3)\n",
        "  return refwh\n",
        "\n",
        "def calcUnCrop4(n):\n",
        "  lap,pad=UnCrop4[n]\n",
        "  whole=pad+(lap<<1)\n",
        "  return lap,pad,whole\n",
        "\n",
        "\n",
        "if model_wrap is None:\n",
        "  cond_stage_model = BERTEmbedder(torch.jit.load('transformer_pnnx.pt').eval())\n",
        "  autoencoder = torch.jit.load('autoencoder_pnnx.pt').eval().cuda()\n",
        "  SDlatDEC=autoencoder\n",
        "  imgenc = torch.jit.load('imgencoder_pnnx.pt').eval()\n",
        "  model_wrap = CompVisDenoiser(CompVisJIT())\n",
        "  model_wrap_cfg = CFGDenoiser(model_wrap)\n",
        "if SDver != prevSDver:\n",
        "  diffusion_emb = torch.jit.load(SDver+'diffusion_emb_pnnx.pt').eval().half().cuda()\n",
        "  diffusion_mid = torch.jit.load(SDver+'diffusion_mid_pnnx.pt').eval().half().cuda()\n",
        "  diffusion_out = torch.jit.load(SDver+'diffusion_out_pnnx.pt').eval().half().cuda()\n",
        "  if DfmCodeBase != prevDfmCodeBase:\n",
        "    prevDfmCodeBase=DfmCodeBase\n",
        "    DfmCodeBase=nDfmCodeBase()\n",
        "    if DfmCodeBase > 0:\n",
        "      apply_model=apply_model_ldm\n",
        "      if DfmCodeBase == 2:\n",
        "        install_xformer()\n",
        "      mdfy=-1\n",
        "      if EnableKVmerges:\n",
        "        mdfy=1\n",
        "      ldm_unet=init_ldm(DfmCodeBase,mdfy)\n",
        "    else:\n",
        "      apply_model=apply_model_jit\n",
        "      warmup()\n",
        "    torch.cuda.empty_cache()\n",
        "elif DfmCodeBase != prevDfmCodeBase:\n",
        "  prevDfmCodeBase=DfmCodeBase\n",
        "  DfmCodeBase=nDfmCodeBase()\n",
        "  if DfmCodeBase > 0:\n",
        "    apply_model=apply_model_ldm\n",
        "    if DfmCodeBase == 2:\n",
        "      install_xformer()\n",
        "    mdfy=-1\n",
        "    if EnableKVmerges:\n",
        "      mdfy=1\n",
        "    ldm_unet=init_ldm(DfmCodeBase,mdfy)\n",
        "  torch.cuda.empty_cache()\n",
        "prevSDver=SDver\n",
        "\n"
      ],
      "metadata": {
        "id": "fmYgI8PLYudK",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "👇Optional👇"
      ],
      "metadata": {
        "id": "E4dG4ZxdW4y-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sampler Selector"
      ],
      "metadata": {
        "id": "tHe29ijI3e2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Sampler='euler_a' #@param ['euler', 'euler_a', 'heun','dpm_2','dpm_2_a','lms']\n",
        "f_sampler()\n",
        "\n",
        "Karras=False #@param {type:'boolean'}\n",
        "KarrasRho = 7.0 #@param {type:'number'}"
      ],
      "metadata": {
        "id": "fQlZQgl7uxkx",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insert TI"
      ],
      "metadata": {
        "id": "3WLYv80T3htJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cond_stage_model.insert('<majipuri>')\n",
        "cond_stage_model.insert('<pekora>')"
      ],
      "metadata": {
        "id": "x0KSuj8yEZmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insert prompt variables<br>\n",
        "e.g. `A {animals} in water` will make `A dog in water` and `A cat in water` two images"
      ],
      "metadata": {
        "id": "Ilfed7713l6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cond_stage_model.insert_prompt_vars('animals')\n",
        "cond_stage_model.insert_prompt_vars('artists')"
      ],
      "metadata": {
        "id": "y9hFuhPnE4DQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So-called img2img thing, `masking` only works for `Dfm` ends with `_inpaint`"
      ],
      "metadata": {
        "id": "dSko3E-i4CSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_img='xxx' #@param {type:'string'}\n",
        "strength=0.5 #@param {type:'number'}\n",
        "EnLarge = [     ]  #@param {type:'raw'}\n",
        "UnCrop4=[     ] #@param {type:'raw'}\n",
        "#200,300,250,330 \n",
        "initymgtyp=init_img_type()\n",
        "if initymgtyp == 0:\n",
        "  preimg=torch.tensor(np.load(init_img), device='cpu')\n",
        "  n_samples=preimg.size(0)\n",
        "  H=preimg.size(2)<<3\n",
        "  W=preimg.size(3)<<3\n",
        "  H, W, seed_size = EnLarge2seed_size(EnLarge)\n",
        "elif initymgtyp == 1:\n",
        "  n_samples=1\n",
        "  rpt=load_img(init_img)\n",
        "  H=rpt.size(2)\n",
        "  W=rpt.size(3)\n",
        "  preimg=imgenc(  rpt, torch.randn(torch.Size([n_samples,4,H>>3,W>>3]))  )*0.18215\n",
        "  np.save(init_img+'.npy',preimg.numpy())\n",
        "  H, W, seed_size = EnLarge2seed_size(EnLarge)\n",
        "elif initymgtyp == 2:\n",
        "  tmpfeeder=ifeeder()\n",
        "  tmpfeeder.pattern=init_img[:-3].replace('!@!','%')+'bin'\n",
        "  with open(init_img,'rt') as f:\n",
        "    stz=f.read().replace(' ','').replace('\\t','').splitlines()\n",
        "  tmpfeeder.xpenlen=int(stz[1])\n",
        "  stz=stz[0].split(',')\n",
        "  tmpfeeder.shape=[ int(stz[0]), int(stz[1]), int(stz[2]), int(stz[3]) ]\n",
        "  n_samples=tmpfeeder.shape[0]\n",
        "  H=tmpfeeder.shape[2]<<3\n",
        "  W=tmpfeeder.shape[3]<<3\n",
        "  preimg=torch.tensor([2])\n",
        "else:\n",
        "  hlog0.setfuncb('0')\n",
        "  preimg=None\n",
        "revpreimg=None\n",
        "\n",
        "if initymgtyp<2:\n",
        "  if len(UnCrop4)>3:\n",
        "    strength=999\n",
        "    initymgtyp=10\n",
        "    UnCrop4=mkUnCrop4()\n",
        "\n",
        "masking=False #@param {type:'boolean'}\n",
        "if (masking or INP) and strength<1:\n",
        "  mask_path='msk.png'  #@param {type:'string'}\n",
        "  image_embed=do_masking(mask_path)\n",
        "  UnCrop4=[]\n",
        "  initymgtyp=0\n",
        "  if strength == 0:\n",
        "    preimg=None"
      ],
      "metadata": {
        "id": "I1FtIJmB2Ojd",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt interpolation with latent re-feeding<br>tick `Revert2Orig` to disable it"
      ],
      "metadata": {
        "id": "B6VrT6bw0tUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Revert2Orig=False #@param {type:'boolean'}\n",
        "\n",
        "\n",
        "def predict(c_list, uc, noi=None):\n",
        "    global x_samples\n",
        "    global samples\n",
        "    global ktta\n",
        "    global tmpfeeder\n",
        "    global noise\n",
        "    preimg=None\n",
        "\n",
        "\n",
        "    feeder=ifeeder()\n",
        "\n",
        "    \n",
        "    \n",
        "    sigmas = f_sigmas()\n",
        "    shape_alters=[]\n",
        "    if len(shape) > 4:\n",
        "      shape_r=shape[:4]\n",
        "      shape_alters=shape[4:]\n",
        "    else:\n",
        "      shape_r=shape\n",
        "\n",
        "    if noi is None:\n",
        "      noise = torch.randn(shape_r, dtype=torch.float,device=cudev)\n",
        "    else:\n",
        "      noise = noi\n",
        "\n",
        "    ktta=0\n",
        "    for c in c_list:\n",
        "      c.reset()\n",
        "      if preimg is not None:\n",
        "        noise=torch.permute(noise, (0,3,1,2)).reshape(noise.shape)\n",
        "        vt_enc= int(strength * ddim_num_steps)\n",
        "        c.d_sta=t_enc/vt_enc\n",
        "        sigma_sched = sigmas[ddim_num_steps - vt_enc - 1:]\n",
        "        if preimg.shape != noise.shape:\n",
        "          print('resiz')\n",
        "          preimg=torch.nn.functional.interpolate(preimg ,size=noise.shape,mode='area')\n",
        "        img = preimg +  noise* sigma_sched[0]\n",
        "        feeder.setbs(img)        \n",
        "      else:\n",
        "        img = noise*sigmas[0]\n",
        "        sigma_sched=sigmas\n",
        "        feeder.setbs(img)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      samples = zemp0(feeder.getn(ktta),c,sigma_sched,shape_alters)\n",
        "\n",
        "      ktta+=1\n",
        "      preimg=samples\n",
        "      x_samples = decode_first_stage( samples ).cpu()\n",
        "      samples=samples.cpu()\n",
        "      \n",
        "      t3 = Thread(target = saver)\n",
        "      a3 = t3.start()\n",
        "    \n",
        "    return\n",
        "if Revert2Orig:\n",
        "  predict=predict_orig\n",
        "else:\n",
        "  preimg=None\n",
        "  strength=0.75 #@param {type:'number'}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Pgy-4fQn0sT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NoiseMap interpolation<br>re-feed previous when strength > 0"
      ],
      "metadata": {
        "id": "Ht69eCNYt5VW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Revert2Orig=False #@param {type:'boolean'}\n",
        "\n",
        "\n",
        "def mknoises():\n",
        "  sil=len(Seed_Interval_list)>>1\n",
        "  nolist=[]\n",
        "  resize_noise=None\n",
        "  shape_alters=[]\n",
        "  if len(shape) > 4:\n",
        "    shape_r=shape[:4]\n",
        "    shape_alters=shape[4:]\n",
        "  else:\n",
        "    shape_r=shape\n",
        "  for n in range(sil):\n",
        "    zeed=Seed_Interval_list[n*2]\n",
        "    if zeed < 1:\n",
        "      zeed=random.randint(0, 2**32)\n",
        "      print('seed%d='%n)\n",
        "      print(zeed)\n",
        "    torch.manual_seed(zeed)    \n",
        "    nolist.append( torch.randn(shape_r, dtype=torch.float,device=cudev) )\n",
        "  nolist.append(nolist[0])\n",
        "  interpos=[]\n",
        "  DOT_THRESHOLD=0.9995\n",
        "  for n in range(sil):\n",
        "    stp=Seed_Interval_list[n*2+1]+1\n",
        "    v0=nolist[n]\n",
        "    v1=nolist[n+1]\n",
        "    dot = torch.sum(v0 * v1 / (torch.linalg.norm(v0) * torch.linalg.norm(v1)))\n",
        "    if torch.abs(dot) > DOT_THRESHOLD:\n",
        "      for j in range(stp):\n",
        "        t=j/stp\n",
        "        interpos.append( (1 - t) * v0 + t * v1 )\n",
        "    else:\n",
        "      theta_0 = torch.acos(dot)\n",
        "      sin_theta_0 = torch.sin(theta_0)\n",
        "      for j in range(stp):\n",
        "        t=j/stp\n",
        "        theta_t = theta_0 * t\n",
        "        sin_theta_t = torch.sin(theta_t)\n",
        "        s0 = torch.sin(theta_0 - theta_t) / sin_theta_0\n",
        "        s1 = sin_theta_t / sin_theta_0\n",
        "        interpos.append( s0 * v0 + s1 * v1 )\n",
        "\n",
        "  return interpos, shape_alters\n",
        "\n",
        "\n",
        "\n",
        "def predict(c_list, uc,noi=None):\n",
        "    global x_samples\n",
        "    global samples\n",
        "    global ktta\n",
        "    global tmpfeeder\n",
        "    global noise\n",
        "    preimg=None\n",
        "\n",
        "\n",
        "    feeder=ifeeder()\n",
        "\n",
        "\n",
        "\n",
        "    sigmas = f_sigmas()\n",
        "\n",
        "    noises, shape_alters = mknoises()\n",
        "    c_list=c_list*len(noises)\n",
        "\n",
        "    ktta=0\n",
        "    for c in c_list:\n",
        "      c.reset()\n",
        "      noise=noises[ktta]\n",
        "      \n",
        "      if preimg is not None:\n",
        "        vt_enc= int(strength * ddim_num_steps)\n",
        "        c.d_sta=t_enc/vt_enc\n",
        "        sigma_sched = sigmas[ddim_num_steps - vt_enc - 1:]\n",
        "        if preimg.shape != noise.shape:\n",
        "          print('resiz')\n",
        "          preimg=torch.nn.functional.interpolate(preimg ,size=noise.shape[-2:],mode='area')\n",
        "        img = preimg +  noise* sigma_sched[0]\n",
        "        feeder.setbs(img)        \n",
        "      else:\n",
        "        img = noise*sigmas[0]\n",
        "        sigma_sched=sigmas\n",
        "        feeder.setbs(img)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      samples = zemp0(feeder.getn(ktta),c,sigma_sched,shape_alters)\n",
        "\n",
        "      ktta+=1\n",
        "      if strength > 0:\n",
        "        preimg=samples #*(1-strength)\n",
        "      x_samples = decode_first_stage( samples ).cpu()\n",
        "      samples=samples.cpu()\n",
        "      \n",
        "      t3 = Thread(target = saver)\n",
        "      a3 = t3.start()\n",
        "    \n",
        "    return\n",
        "if Revert2Orig:\n",
        "  predict=predict_orig\n",
        "else:\n",
        "  preimg=None\n",
        "  strength=0 #@param {type:'number'}\n",
        "  Seed_Interval_list=[    775577,10,    881188,10,    996699,10    ] #@param {type:'raw'}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "JNB7IAIMt_YU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "☝️Optional☝️"
      ],
      "metadata": {
        "id": "QunwpFSGXKz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "InThread=False #@param {type:'boolean'}\n",
        "\n",
        "prompt = 'a photograph of an astronaut riding a horse' #@param {type:'string'}\n",
        "neg_prompt = '' #@param {type:'string'}\n",
        "\n",
        "n_iter = 1 #@param {type:'integer'}\n",
        "if preimg is None and revpreimg is None:\n",
        "  n_samples = 1 #@param {type:'integer'}\n",
        "  H=704 #@param {type:'integer'}\n",
        "  W=768 #@param {type:'integer'}\n",
        "  seed_size=[     ] #@param {type:'raw'}\n",
        "# 512,512,0.1\n",
        "# 512,512,0.3, 768,512,0.6, 512,768,0.8\n",
        "\n",
        "\n",
        "seed=0 #@param {type:'integer'}\n",
        "\n",
        "ddim_num_steps = 50  #@param {type:'integer'}\n",
        "ddpm_num_timesteps = 1000\n",
        "\n",
        "\n",
        "outputp='/content/sample_data' #@param {type:'string'}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "cfg_scale = 7.5 #@param {type:'number'}\n",
        "ddim_eta = 1.0  #@param {type:'number'}\n",
        "\n",
        "CopyDFout=True #@param {type:'boolean'}\n",
        "UnifiedNoise=False #@param {type:'boolean'}\n",
        "Overlap = 300  #@param {type:'integer'}\n",
        "Overlap=Overlap>>4\n",
        "\n",
        "outputp=outputp+'/'+str(len(os.listdir(outputp)))\n",
        "\n",
        "t_enc=ddim_num_steps\n",
        "if preimg is not None and strength < 1:\n",
        "  t_enc = int(strength * ddim_num_steps)+1\n",
        "\n",
        "shape = mk_shape()\n",
        "\n",
        "\n",
        "makerng()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('Start inference...')\n",
        "c_list = makeCs(prompt,0)\n",
        "uc = makeCs(neg_prompt,0,enable3d=False)[0]\n",
        "\n",
        "PrintPromptPhases=False #@param {type:'boolean'}\n",
        "SaveCompiledPrompt=False #@param {type:'boolean'}\n",
        "printprompts(PrintPromptPhases)\n",
        "\n",
        "  \n",
        "def wpa_orig():\n",
        "  global iita\n",
        "  torch.set_grad_enabled(False)\n",
        "  \n",
        "  for iita in range(n_iter):\n",
        "      print(\"iteration: %s\" % (iita + 1))\n",
        "      predict(c_list, uc)\n",
        "      \n",
        "  print('Script finished successfully.')\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "def wpa_wide():\n",
        "  global iita\n",
        "  global halfb\n",
        "  global hlog0\n",
        "  torch.set_grad_enabled(False)\n",
        "\n",
        "\n",
        "  halfa=(W>>3)\n",
        "  Elap2=Overlap<<1\n",
        "  hlog0.Elap2=Elap2\n",
        "  halfb=halfa-Elap2\n",
        "  unoise=torch.randn([n_samples, 4, H>>3 , (halfb*n_iter)+halfa ], dtype=torch.float,device=cudev)\n",
        "  hlog0.Elap=Overlap\n",
        "\n",
        "\n",
        "  if CopyDFout:\n",
        "    hlog0.h_bs=[None]*ddim_num_steps\n",
        "    hlog0.setfunc('logw0')\n",
        "  iita=0\n",
        "  predict(c_list, uc,unoise[:,:,:,0:halfa])\n",
        "  if CopyDFout:\n",
        "    hlog0.setfunc('logw')\n",
        "  hlog0.setfuncNc(1,cache=True)\n",
        "  hlog0.setbsB(-11,samples)\n",
        "  for iita in range(1,n_iter):\n",
        "      print(\"iteration: %s\" % (iita + 1))\n",
        "      sta=iita*halfb\n",
        "      endo=sta+halfa\n",
        "      predict(c_list, uc,unoise[:,:,:,sta:endo])\n",
        "      hlog0.setbsB(-11,samples)\n",
        "\n",
        "  print('Wide finished successfully.')\n",
        "  torch.cuda.empty_cache()\n",
        "  hlog0=hlogger()\n",
        "\n",
        "def wpa_uncrop():\n",
        "  global iita\n",
        "  torch.set_grad_enabled(False)\n",
        "  \n",
        "  for iita in range(n_iter):\n",
        "      print(\"iteration: %s\" % (iita + 1))\n",
        "      UnCrop(c_list, uc)\n",
        "      \n",
        "  print('UnCrop finished successfully.')\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "wpa=wpa_orig\n",
        "if initymgtyp == 10:\n",
        "  wpa=wpa_uncrop\n",
        "  if not INP:\n",
        "    initymgtyp=99\n",
        "  preimg=preimg.cuda()\n",
        "elif UnifiedNoise:\n",
        "  print('RealOverlap=%d'%(Overlap<<3))\n",
        "  wpa=wpa_wide\n",
        "\n",
        "if InThread:\n",
        "  t1 = Thread(target = wpa)\n",
        "  a1 = t1.start()\n",
        "else:\n",
        "  wpa()\n"
      ],
      "metadata": {
        "id": "V5xaKk_uYwBK",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "pH0ExQ__p8Cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ffmpeg -framerate 3 -i /content/sample_data/48_0x3v%d.png intp03.mp4"
      ],
      "metadata": {
        "id": "Yby8cQ9HqkuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tools\n",
        "designed for the gen proc running with `InThread` or gradio app<br>\n",
        "so imgenc (image->latent encoder) is on cpu"
      ],
      "metadata": {
        "id": "vFqWsqyVNB2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hlog0.latlog=[]\n",
        "hlog0.setfuncb('log')"
      ],
      "metadata": {
        "id": "uiJvF6-N2pUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dumplogs(0)\n",
        "#hlog0.latlog_arr=[]"
      ],
      "metadata": {
        "id": "QeNF5VjH2hqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.link('/content/sample_data/64_7x0v1.png','/content/sample_data/vyi/stp00100.png')"
      ],
      "metadata": {
        "id": "YVZn3yJA20g_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hlog0.latlog=[]"
      ],
      "metadata": {
        "id": "ICOv0Vv42rnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ConcatNoise=ConcatNoise_rdm #ConcatNoise_shuf"
      ],
      "metadata": {
        "id": "t9SaQkepZTLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gif/Video to latent pack"
      ],
      "metadata": {
        "id": "_9d7uJg9NEPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_anim  = '/content/senpai.gif' #@param {type:'string'}\n",
        "output_pattern = '/content/ijj/senpai_%04d.png' #@param {type:'string'}\n",
        "!ffmpeg -i {input_anim} {output_pattern}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "MmRwF523NIZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resize the output to `(64*n)x(64*m)` first"
      ],
      "metadata": {
        "id": "GLrbLP9cN8oh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encodepatt()"
      ],
      "metadata": {
        "id": "ckwYthEQOJSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ImageEmbSetup  = False #@param {type:'boolean'}\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "def load_im(im_path):\n",
        "    if im_path.startswith(\"http\"):\n",
        "        response = requests.get(im_path)\n",
        "        response.raise_for_status()\n",
        "        im = Image.open(BytesIO(response.content))\n",
        "    else:\n",
        "        im = Image.open(im_path).convert(\"RGB\")\n",
        "    tforms = transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.CenterCrop((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    inp = tforms(im).unsqueeze(0)\n",
        "    return inp*2-1\n",
        "if not os.path.isfile('imgemb.pt'):\n",
        "  !wget https://huggingface.co/Larvik/imgemb/resolve/main/imgemb.pt\n",
        "imgemb=torch.jit.load('imgemb.pt').float()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GxQjN-yyggf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgemb(load_im('/content/chaz512.jpg')).numpy().tofile('chaz.bin')"
      ],
      "metadata": {
        "id": "qk9SKnUkgifF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio Gui\n",
        "tho I don't really understand why you want a webui inside another webui"
      ],
      "metadata": {
        "id": "bJsLMq9pplwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gradio=False #@param {type:'boolean'}\n",
        "\n",
        "!pip install gradio\n",
        "from google.colab import output\n",
        "import gradio as gr\n",
        "\n",
        "def dream():\n",
        "  return\n",
        "\n",
        "\n",
        "dream_interface = gr.Interface(\n",
        "    dream,\n",
        "    inputs=[\n",
        "        gr.Textbox(placeholder=\"A corgi wearing a top hat as an oil painting.\", lines=1),\n",
        "        gr.Slider(minimum=1, maximum=150, step=1, label=\"Sampling Steps\", value=50),\n",
        "        gr.Checkbox(label='Enable PLMS sampling', value=False),\n",
        "        gr.Checkbox(label='Enable Fixed Code sampling', value=False),\n",
        "        gr.Slider(minimum=0.0, maximum=1.0, step=0.01, label=\"DDIM ETA\", value=0.0, visible=False),\n",
        "        gr.Slider(minimum=1, maximum=50, step=1, label='Sampling iterations', value=8),\n",
        "        gr.Slider(minimum=1, maximum=8, step=1, label='Samples per iteration', value=1),\n",
        "        gr.Slider(minimum=1.0, maximum=20.0, step=0.5, label='Classifier Free Guidance Scale', value=7.0),\n",
        "        gr.Number(label='Seed', value=-1),\n",
        "        gr.Slider(minimum=64, maximum=2048, step=64, label=\"Height\", value=704),\n",
        "        gr.Slider(minimum=64, maximum=2048, step=64, label=\"Width\", value=768),\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Gallery(),\n",
        "        gr.Number(label='Seed')\n",
        "    ],\n",
        "    title=\"Stable Diffusion Text-to-Image\",\n",
        "    description=\"Generate images from text with Stable Diffusion\",\n",
        ")\n",
        "\n",
        "\n",
        "gdemo = gr.TabbedInterface(interface_list=[dream_interface], tab_names=[\"Dream\"])\n",
        "\n",
        "\n",
        "output.serve_kernel_port_as_window(8233, path='/dl.htm')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mh-_HQ1jquy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy the link above to `GoogleLocal`"
      ],
      "metadata": {
        "id": "ePW4zpPPsVTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GoogleLocal = 'aaaaa' #@param {type:'string'}\n",
        "if '.googleusercontent.com' in GoogleLocal:\n",
        "  gdemo.launch()\n",
        "else:\n",
        "  print('set a valid GoogleLocal')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ndCPoghArlfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# glid-3-xl-stable"
      ],
      "metadata": {
        "id": "O6t9w7xdHv6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SDver='470k' #@param ['440k', '470k']\n",
        "Dfm='Orig' #@param ['Orig', '_imgemb','_a19561','_a17750','_a17750_e9750','_e26500']\n",
        "if Dfm=='Orig':\n",
        "  Dfm=''\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "torch.set_num_threads(os.cpu_count())\n",
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class imgencdec:\n",
        "  def encode(self,im):\n",
        "    nzmp=im.size(0)\n",
        "    H=im.size(2)\n",
        "    W=im.size(3)\n",
        "    return imgenc(  im, torch.randn(torch.Size([nzmp,4,H>>3,W>>3]))  )\n",
        "  def decode(self,im):\n",
        "    return autoencoder(im)\n",
        "\n",
        "\n",
        "SDver=f_dljit(SDver,Dfm)\n",
        "\n",
        "if not os.path.isfile('/content/guided_diffusion/unet.py'):\n",
        "  !wget https://raw.githubusercontent.com/TabuaTambalam/DalleWebms/main/docs/sd/jkt.py\n",
        "  !git clone https://github.com/Jack000/glid-3-xl-stable.git\n",
        "  !mv /content/glid-3-xl-stable/guided_diffusion /content/guided_diffusion \n",
        "\n",
        "from transformers import CLIPTokenizer\n",
        "cond_stage_model = BERTEmbedder(torch.jit.load('transformer_pnnx.pt').eval())\n",
        "diffusion_emb = torch.jit.load(SDver+'diffusion_emb_pnnx.pt').eval().cuda()\n",
        "diffusion_mid = torch.jit.load(SDver+'diffusion_mid_pnnx.pt').eval().cuda()\n",
        "diffusion_out = torch.jit.load(SDver+'diffusion_out_pnnx.pt').eval().cuda()\n",
        "autoencoder = torch.jit.load('autoencoder_pnnx.pt').eval().cuda()\n",
        "SDlatDEC=autoencoder\n",
        "imgenc = torch.jit.load('imgencoder_pnnx.pt').eval()"
      ],
      "metadata": {
        "id": "2cz08wFzH-8L",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://huggingface.co/Jack000/glid-3-xl-stable/tree/main/super_lg\n",
        "import gc\n",
        "import io\n",
        "import math\n",
        "import sys\n",
        "\n",
        "from PIL import Image, ImageOps\n",
        "import requests\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as TF\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from guided_diffusion.script_util import create_model_and_diffusion, model_and_diffusion_defaults\n",
        "\n",
        "\n",
        "from accelerate import init_empty_weights\n",
        "from einops import rearrange\n",
        "from math import log2, sqrt\n",
        "\n",
        "\n",
        "!mkdir output_npy\n",
        "!mkdir output\n",
        "\n",
        "def save_sample(i, sample, clip_score=False):\n",
        "    for k, image in enumerate(sample['pred_xstart'][:1]):\n",
        "        image /= 0.18215\n",
        "        im = image.unsqueeze(0)\n",
        "        out = ldm.decode(im)\n",
        "\n",
        "        npy_filename = f'output_npy/{i * batchsz + k:05}.npy'\n",
        "        with open(npy_filename, 'wb') as outfile:\n",
        "            np.save(outfile, image.detach().cpu().numpy())\n",
        "\n",
        "        out = TF.to_pil_image(out.squeeze(0).add(1).div(2).clamp(0, 1))\n",
        "\n",
        "        filename = f'output/{i * batchsz + k:05}.png'\n",
        "        out.save(filename)\n",
        "\n",
        "\n",
        "# Create a classifier-free guidance sampling function\n",
        "def model_fn(x_t, ts, **kwargs):\n",
        "    half = x_t[: len(x_t) // 2]\n",
        "    combined = torch.cat([half, half], dim=0)\n",
        "    model_out = model(combined, ts, **kwargs)\n",
        "    eps, rest = model_out[:, :3], model_out[:, 3:]\n",
        "    cond_eps, uncond_eps = torch.split(eps, len(eps) // 2, dim=0)\n",
        "    half_eps = uncond_eps + guidance_scale * (cond_eps - uncond_eps)\n",
        "    eps = torch.cat([half_eps, half_eps], dim=0)\n",
        "    return torch.cat([eps, rest], dim=1)\n",
        "\n",
        "device = torch.device('cuda:0')\n",
        "print('Using device:', device)\n",
        "\n",
        "\n",
        "\n",
        "model_params = {\n",
        "    'attention_resolutions': '32,16,8',\n",
        "    'class_cond': False,\n",
        "    'diffusion_steps': 1000,\n",
        "    'rescale_timesteps': True,\n",
        "    'timestep_respacing': '50',  # Modify this value to decrease the number of\n",
        "                                 # timesteps.\n",
        "    'image_size': 32,\n",
        "    'learn_sigma': False,\n",
        "    'noise_schedule': 'linear',\n",
        "    'num_channels': 320,\n",
        "    'num_heads': 8,\n",
        "    'num_res_blocks': 2,\n",
        "    'resblock_updown': False,\n",
        "    'use_fp16': False,\n",
        "    'use_scale_shift_norm': False,\n",
        "    'clip_embed_dim': None, #768,\n",
        "    'image_condition': False,\n",
        "    #'image_condition': True if model_state_dict['input_blocks.0.0.weight'].shape[1] == 8 else False,\n",
        "    'super_res_condition': False,\n",
        "}\n",
        "\n",
        "model_params['timestep_respacing'] = '100'\n",
        "\n",
        "model_config = model_and_diffusion_defaults()\n",
        "model_config.update(model_params)\n",
        "\n",
        "\n",
        "model_config['use_fp16'] = True\n",
        "\n",
        "# Load models\n",
        "with init_empty_weights():\n",
        "  model, diffusion = create_model_and_diffusion(**model_config)\n",
        "\n",
        "load_state_dict_with_low_memory(model,mkmodel_state_dict())\n",
        "\n",
        "if model_config['use_fp16']:\n",
        "  model.convert_to_fp16()"
      ],
      "metadata": {
        "id": "Qpp_f7ZWIIWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.requires_grad_(False).eval().to(device)\n",
        "\n",
        "\n",
        "torch.manual_seed(114514)\n",
        "\n",
        "\n",
        "# vae\n",
        "\n",
        "ldm=imgencdec()\n",
        "\n",
        "\n",
        "guidance_scale=7\n",
        "height=832\n",
        "width=896\n",
        "batchsz=1\n",
        "\n",
        "\n",
        "args_text='thicc farm girl, long blonde hair, japanimation, by Alfons Maria Mucha, cinematic lightning, cinematic wallpaper'\n",
        "args_negative=''\n",
        "# clip context\n",
        "\n",
        "n_samples=batchsz\n",
        "t_enc=100\n",
        "text_emb = makeCs(args_text)[0].get(0)\n",
        "text_emb_blank = makeCs(args_negative)[0].get(0)\n",
        "\n",
        "image_embed = None\n",
        "\n",
        "\n",
        "\n",
        "input_image = torch.zeros(batchsz, 4, height//8, width//8, device=device)\n",
        "'''\n",
        "lat=torch.tensor(np.load('96_4x1v1.npy'))\n",
        "\n",
        "\n",
        "input_image[0][:,:,:32]=lat[0][:,:,:32]\n",
        "'''\n",
        "\n",
        "      \n",
        "image_embed = None #torch.cat(batchsz*2*[input_image], dim=0).float()\n",
        "\n",
        "\n",
        "\n",
        "kwargs = {\n",
        "    \"context\": torch.cat([text_emb, text_emb_blank], dim=0).half().cuda(),\n",
        "    \"clip_embed\": None,\n",
        "    \"image_embed\": image_embed\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "cur_t = None\n",
        "\n",
        "sample_fn = diffusion.plms_sample_loop_progressive\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "init = Image.open('xipooh.jpg').convert('RGB')\n",
        "\n",
        "init = TF.to_tensor(init).to(device).unsqueeze(0).clamp(0,1)\n",
        "h = ldm.encode(init * 2 - 1) *  0.18215\n",
        "init = torch.cat(1*2*[h], dim=0)\n",
        "'''\n",
        "init=None\n",
        "\n",
        "for i in range(1):\n",
        "    cur_t = diffusion.num_timesteps - 1\n",
        "    with torch.cuda.amp.autocast(dtype=torch.float16):\n",
        "      samples = sample_fn(\n",
        "          model_fn,\n",
        "          (batchsz*2, 4, height>>3, width>>3),\n",
        "          clip_denoised=False,\n",
        "          model_kwargs=kwargs,\n",
        "          cond_fn=None,\n",
        "          device=device,\n",
        "          progress=True,\n",
        "          init_image=init,\n",
        "          skip_timesteps=0,\n",
        "      )\n",
        "\n",
        "    for j, sample in enumerate(samples):\n",
        "        cur_t -= 1\n",
        "\n",
        "    save_sample(i, sample)\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "62dodU08IN9i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}