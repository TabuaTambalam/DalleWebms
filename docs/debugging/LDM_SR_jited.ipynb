{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LDM_SR_jited.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_VZUcnVveXny",
        "115Y_TyxYkbk",
        "bJsLMq9pplwM",
        "O6t9w7xdHv6D"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Common\n",
        "Always run this, when start/restart the runtime"
      ],
      "metadata": {
        "id": "_VZUcnVveXny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "from scipy import integrate\n",
        "from threading import Thread\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm.auto import trange, tqdm\n",
        "\n",
        "def append_dims(x, target_dims):\n",
        "    \"\"\"Appends dimensions to the end of a tensor until it has target_dims dimensions.\"\"\"\n",
        "    dims_to_append = target_dims - x.ndim\n",
        "    if dims_to_append < 0:\n",
        "        raise ValueError(f'input has {x.ndim} dims but target_dims is {target_dims}, which is less')\n",
        "    return x[(...,) + (None,) * dims_to_append]\n",
        "\n",
        "\n",
        "def append_zero(x):\n",
        "    return torch.cat([x, x.new_zeros([1])])\n",
        "\n",
        "\n",
        "def get_sigmas_karras(n, sigma_min, sigma_max, rho=7., device='cuda'):\n",
        "    \"\"\"Constructs the noise schedule of Karras et al. (2022).\"\"\"\n",
        "    ramp = torch.linspace(0, 1, n,device=device)\n",
        "    min_inv_rho = sigma_min ** (1 / rho)\n",
        "    max_inv_rho = sigma_max ** (1 / rho)\n",
        "    sigmas = (max_inv_rho + ramp * (min_inv_rho - max_inv_rho)) ** rho\n",
        "    return append_zero(sigmas).to(device)\n",
        "\n",
        "\n",
        "def get_sigmas_exponential(n, sigma_min, sigma_max, device='cpu'):\n",
        "    \"\"\"Constructs an exponential noise schedule.\"\"\"\n",
        "    sigmas = torch.linspace(math.log(sigma_max), math.log(sigma_min), n, device=device).exp()\n",
        "    return append_zero(sigmas)\n",
        "\n",
        "\n",
        "def get_sigmas_vp(n, beta_d=19.9, beta_min=0.1, eps_s=1e-3, device='cpu'):\n",
        "    \"\"\"Constructs a continuous VP noise schedule.\"\"\"\n",
        "    t = torch.linspace(1, eps_s, n, device=device)\n",
        "    sigmas = torch.sqrt(torch.exp(beta_d * t ** 2 / 2 + beta_min * t) - 1)\n",
        "    return append_zero(sigmas)\n",
        "\n",
        "\n",
        "def to_d(x, sigma, denoised):\n",
        "    \"\"\"Converts a denoiser output to a Karras ODE derivative.\"\"\"\n",
        "    return (x - denoised) / append_dims(sigma, x.ndim)\n",
        "\n",
        "\n",
        "def get_ancestral_step(sigma_from, sigma_to):\n",
        "    \"\"\"Calculates the noise level (sigma_down) to step down to and the amount\n",
        "    of noise to add (sigma_up) when doing an ancestral sampling step.\"\"\"\n",
        "    sigma_up = (sigma_to ** 2 * (sigma_from ** 2 - sigma_to ** 2) / sigma_from ** 2) ** 0.5\n",
        "    sigma_down = (sigma_to ** 2 - sigma_up ** 2) ** 0.5\n",
        "    return sigma_down, sigma_up\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_euler(model, x, sigmas, extra_args=None, callback=None, disable=None, s_churn=0., s_tmin=0., s_tmax=float('inf'), s_noise=1.):\n",
        "    \"\"\"Implements Algorithm 2 (Euler steps) from Karras et al. (2022).\"\"\"\n",
        "    extra_args = {} if extra_args is None else extra_args\n",
        "    s_in = x.new_ones([x.shape[0]])\n",
        "    for i in trange(len(sigmas) - 1, disable=disable):\n",
        "        gamma = min(s_churn / (len(sigmas) - 1), 2 ** 0.5 - 1) if s_tmin <= sigmas[i] <= s_tmax else 0.\n",
        "        eps = torch.randn_like(x) * s_noise\n",
        "        sigma_hat = sigmas[i] * (gamma + 1)\n",
        "        if gamma > 0:\n",
        "            x = x + eps * (sigma_hat ** 2 - sigmas[i] ** 2) ** 0.5\n",
        "        denoised = model( i, revpre(x,sigmas,i), sigma_hat * s_in, **extra_args)\n",
        "        d = to_d(x, sigma_hat, denoised)\n",
        "        if callback is not None:\n",
        "            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigma_hat, 'denoised': denoised})\n",
        "        dt = sigmas[i + 1] - sigma_hat\n",
        "        # Euler method\n",
        "        x = x + d * dt\n",
        "    return x\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None):\n",
        "    \"\"\"Ancestral sampling with Euler method steps.\"\"\"\n",
        "    extra_args = {} if extra_args is None else extra_args\n",
        "    s_in = x.new_ones([x.shape[0]])\n",
        "    for i in trange(len(sigmas) - 1, disable=disable):\n",
        "        denoised = model(  i,  revpre(x,sigmas,i), sigmas[i] * s_in, **extra_args)\n",
        "        sigma_down, sigma_up = get_ancestral_step(sigmas[i], sigmas[i + 1])\n",
        "        if callback is not None:\n",
        "            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n",
        "        d = to_d(x, sigmas[i], denoised)\n",
        "        # Euler method\n",
        "        dt = sigma_down - sigmas[i]\n",
        "        x = x + d * dt\n",
        "        x = x + torch.randn_like(x) * sigma_up\n",
        "    return x\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_heun(model, x, sigmas, extra_args=None, callback=None, disable=None, s_churn=0., s_tmin=0., s_tmax=float('inf'), s_noise=1.):\n",
        "    \"\"\"Implements Algorithm 2 (Heun steps) from Karras et al. (2022).\"\"\"\n",
        "    extra_args = {} if extra_args is None else extra_args\n",
        "    s_in = x.new_ones([x.shape[0]])\n",
        "    for i in trange(len(sigmas) - 1, disable=disable):\n",
        "        gamma = min(s_churn / (len(sigmas) - 1), 2 ** 0.5 - 1) if s_tmin <= sigmas[i] <= s_tmax else 0.\n",
        "        eps = torch.randn_like(x) * s_noise\n",
        "        sigma_hat = sigmas[i] * (gamma + 1)\n",
        "        if gamma > 0:\n",
        "            x = x + eps * (sigma_hat ** 2 - sigmas[i] ** 2) ** 0.5\n",
        "        denoised = model(  i,  revpre(x,sigmas,i), sigma_hat * s_in, **extra_args)\n",
        "        d = to_d(x, sigma_hat, denoised)\n",
        "        if callback is not None:\n",
        "            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigma_hat, 'denoised': denoised})\n",
        "        dt = sigmas[i + 1] - sigma_hat\n",
        "        if sigmas[i + 1] == 0:\n",
        "            # Euler method\n",
        "            x = x + d * dt\n",
        "        else:\n",
        "            # Heun's method\n",
        "            x_2 = x + d * dt\n",
        "            denoised_2 = model(i, x_2, sigmas[i + 1] * s_in, **extra_args)\n",
        "            d_2 = to_d(x_2, sigmas[i + 1], denoised_2)\n",
        "            d_prime = (d + d_2) / 2\n",
        "            x = x + d_prime * dt\n",
        "    return x\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_dpm_2(model, x, sigmas, extra_args=None, callback=None, disable=None, s_churn=0., s_tmin=0., s_tmax=float('inf'), s_noise=1.):\n",
        "    \"\"\"A sampler inspired by DPM-Solver-2 and Algorithm 2 from Karras et al. (2022).\"\"\"\n",
        "    extra_args = {} if extra_args is None else extra_args\n",
        "    s_in = x.new_ones([x.shape[0]])\n",
        "    for i in trange(len(sigmas) - 1, disable=disable):\n",
        "        gamma = min(s_churn / (len(sigmas) - 1), 2 ** 0.5 - 1) if s_tmin <= sigmas[i] <= s_tmax else 0.\n",
        "        eps = torch.randn_like(x) * s_noise\n",
        "        sigma_hat = sigmas[i] * (gamma + 1)\n",
        "        if gamma > 0:\n",
        "            x = x + eps * (sigma_hat ** 2 - sigmas[i] ** 2) ** 0.5\n",
        "        denoised = model(  i,  revpre(x,sigmas,i), sigma_hat * s_in, **extra_args)\n",
        "        d = to_d(x, sigma_hat, denoised)\n",
        "        if callback is not None:\n",
        "            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigma_hat, 'denoised': denoised})\n",
        "        # Midpoint method, where the midpoint is chosen according to a rho=3 Karras schedule\n",
        "        sigma_mid = ((sigma_hat ** (1 / 3) + sigmas[i + 1] ** (1 / 3)) / 2) ** 3\n",
        "        dt_1 = sigma_mid - sigma_hat\n",
        "        dt_2 = sigmas[i + 1] - sigma_hat\n",
        "        x_2 = x + d * dt_1\n",
        "        denoised_2 = model(i,x_2, sigma_mid * s_in, **extra_args)\n",
        "        d_2 = to_d(x_2, sigma_mid, denoised_2)\n",
        "        x = x + d_2 * dt_2\n",
        "    return x\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_dpm_2_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None):\n",
        "    \"\"\"Ancestral sampling with DPM-Solver inspired second-order steps.\"\"\"\n",
        "    extra_args = {} if extra_args is None else extra_args\n",
        "    s_in = x.new_ones([x.shape[0]])\n",
        "    for i in trange(len(sigmas) - 1, disable=disable):\n",
        "        denoised = model(  i,  revpre(x,sigmas,i), sigmas[i] * s_in, **extra_args)\n",
        "        sigma_down, sigma_up = get_ancestral_step(sigmas[i], sigmas[i + 1])\n",
        "        if callback is not None:\n",
        "            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n",
        "        d = to_d(x, sigmas[i], denoised)\n",
        "        # Midpoint method, where the midpoint is chosen according to a rho=3 Karras schedule\n",
        "        sigma_mid = ((sigmas[i] ** (1 / 3) + sigma_down ** (1 / 3)) / 2) ** 3\n",
        "        dt_1 = sigma_mid - sigmas[i]\n",
        "        dt_2 = sigma_down - sigmas[i]\n",
        "        x_2 = x + d * dt_1\n",
        "        denoised_2 = model(i, x_2, sigma_mid * s_in, **extra_args)\n",
        "        d_2 = to_d(x_2, sigma_mid, denoised_2)\n",
        "        x = x + d_2 * dt_2\n",
        "        x = x + torch.randn_like(x) * sigma_up\n",
        "    return x\n",
        "\n",
        "\n",
        "def linear_multistep_coeff(order, t, i, j):\n",
        "    if order - 1 > i:\n",
        "        raise ValueError(f'Order {order} too high for step {i}')\n",
        "    def fn(tau):\n",
        "        prod = 1.\n",
        "        for k in range(order):\n",
        "            if j == k:\n",
        "                continue\n",
        "            prod *= (tau - t[i - k]) / (t[i - j] - t[i - k])\n",
        "        return prod\n",
        "    return integrate.quad(fn, t[i], t[i + 1], epsrel=1e-4)[0]\n",
        "\n",
        "latlog=[]\n",
        "def revpre0_log(img,sigmas,t):\n",
        "  latlog.append((img-noise * sigmas[t]).cpu().numpy())\n",
        "  return img\n",
        "\n",
        "def revpre0(img,sigmas,t):\n",
        "  return img\n",
        "\n",
        "def revpre1(img,sigmas,t):\n",
        "  return (revpreimg+noise * sigmas[t])*(1-zamask)+img*zamask\n",
        "\n",
        "revpre=revpre0\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_lms(model, x, sigmas, extra_args=None, callback=None, disable=None, order=4):\n",
        "    extra_args = {} if extra_args is None else extra_args\n",
        "    s_in = x.new_ones([x.shape[0]])\n",
        "    ds = []\n",
        "    for i in trange(len(sigmas) - 1, disable=disable):\n",
        "        denoised = model(  i,  revpre(x,sigmas,i) , sigmas[i] * s_in, **extra_args)\n",
        "        d = to_d(x, sigmas[i], denoised)\n",
        "        ds.append(d)\n",
        "        if len(ds) > order:\n",
        "            ds.pop(0)\n",
        "        if callback is not None:\n",
        "            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n",
        "        cur_order = min(i + 1, order)\n",
        "        coeffs = [linear_multistep_coeff(cur_order, sigmas.cpu(), i, j) for j in range(cur_order)]\n",
        "        x = x + sum(coeff * d for coeff, d in zip(coeffs, reversed(ds)))\n",
        "    return x\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def log_likelihood(model, x, sigma_min, sigma_max, extra_args=None, atol=1e-4, rtol=1e-4):\n",
        "    extra_args = {} if extra_args is None else extra_args\n",
        "    s_in = x.new_ones([x.shape[0]])\n",
        "    v = torch.randint_like(x, 2) * 2 - 1\n",
        "    fevals = 0\n",
        "    def ode_fn(sigma, x):\n",
        "        nonlocal fevals\n",
        "        with torch.enable_grad():\n",
        "            x = x[0].detach().requires_grad_()\n",
        "            denoised = model(x, sigma * s_in, **extra_args)\n",
        "            d = to_d(x, sigma, denoised)\n",
        "            fevals += 1\n",
        "            grad = torch.autograd.grad((d * v).sum(), x)[0]\n",
        "            d_ll = (v * grad).flatten(1).sum(1)\n",
        "        return d.detach(), d_ll\n",
        "    x_min = x, x.new_zeros([x.shape[0]])\n",
        "    t = x.new_tensor([sigma_min, sigma_max])\n",
        "    sol = odeint(ode_fn, x_min, t, atol=atol, rtol=rtol, method='dopri5')\n",
        "    latent, delta_ll = sol[0][-1], sol[1][-1]\n",
        "    ll_prior = torch.distributions.Normal(0, sigma_max).log_prob(latent).flatten(1).sum(1)\n",
        "    return ll_prior + delta_ll, {'fevals': fevals}\n",
        "\n",
        "\n",
        "\n",
        "class DiscreteSchedule(nn.Module):\n",
        "    \"\"\"A mapping between continuous noise levels (sigmas) and a list of discrete noise\n",
        "    levels.\"\"\"\n",
        "\n",
        "    def __init__(self, sigmas, quantize):\n",
        "        super().__init__()\n",
        "        self.register_buffer('sigmas', sigmas)\n",
        "        self.quantize = quantize\n",
        "\n",
        "    def get_sigmas(self, n=None):\n",
        "        if n is None:\n",
        "            return append_zero(self.sigmas.flip(0))\n",
        "        t_max = len(self.sigmas) - 1\n",
        "        t = torch.linspace(t_max, 0, n, device=self.sigmas.device)\n",
        "        return append_zero(self.t_to_sigma(t))\n",
        "\n",
        "    def sigma_to_t(self, sigma, quantize=None):\n",
        "        quantize = self.quantize if quantize is None else quantize\n",
        "        \n",
        "        dists = torch.abs(sigma - self.sigmas[:, None])\n",
        "        if quantize:\n",
        "            return torch.argmin(dists, dim=0).view(sigma.shape)\n",
        "        low_idx, high_idx = torch.sort(torch.topk(dists, dim=0, k=2, largest=False).indices, dim=0)[0]\n",
        "        low, high = self.sigmas[low_idx], self.sigmas[high_idx]\n",
        "        w = (low - sigma) / (low - high)\n",
        "        w = w.clamp(0, 1)\n",
        "        t = (1 - w) * low_idx + w * high_idx\n",
        "        return t.view(sigma.shape)\n",
        "\n",
        "    def t_to_sigma(self, t):\n",
        "        t = t.float()\n",
        "        low_idx, high_idx, w = t.floor().long(), t.ceil().long(), t.frac()\n",
        "        return (1 - w) * self.sigmas[low_idx] + w * self.sigmas[high_idx]\n",
        "\n",
        "\n",
        "class DiscreteEpsDDPMDenoiser(DiscreteSchedule):\n",
        "    \"\"\"A wrapper for discrete schedule DDPM models that output eps (the predicted\n",
        "    noise).\"\"\"\n",
        "\n",
        "    def __init__(self, model, alphas_cumprod, quantize):\n",
        "        super().__init__(((1 - alphas_cumprod) / alphas_cumprod) ** 0.5, quantize)\n",
        "        self.inner_model = model\n",
        "        self.sigma_data = 1.\n",
        "\n",
        "    def get_scalings(self, sigma):\n",
        "        c_out = -sigma\n",
        "        c_in = 1 / (sigma ** 2 + self.sigma_data ** 2) ** 0.5\n",
        "        return c_out, c_in\n",
        "\n",
        "    def get_eps(self, *args, **kwargs):\n",
        "        return self.inner_model(*args, **kwargs)\n",
        "\n",
        "    def loss(self, input, noise, sigma, **kwargs):\n",
        "        c_out, c_in = [append_dims(x, input.ndim) for x in self.get_scalings(sigma)]\n",
        "        noised_input = input + noise * append_dims(sigma, input.ndim)\n",
        "        eps = self.get_eps(noised_input * c_in, self.sigma_to_t(sigma), **kwargs)\n",
        "        return (eps - noise).pow(2).flatten(1).mean(1)\n",
        "\n",
        "    def forward(self, input, sigma, **kwargs):\n",
        "        c_out, c_in = [append_dims(x, input.ndim) for x in self.get_scalings(sigma)]\n",
        "        eps = self.get_eps(input * c_in, self.sigma_to_t(sigma), **kwargs)\n",
        "        return input + eps * c_out\n",
        "\n",
        "\n",
        "\n",
        "def make_ddim_timesteps(num_ddim_timesteps, num_ddpm_timesteps):\n",
        "    c = num_ddpm_timesteps // num_ddim_timesteps\n",
        "    ddim_timesteps = np.asarray(list(range(0, num_ddpm_timesteps, c)))\n",
        "\n",
        "    # add one to get the final alpha values right (the ones from first scale to data during sampling)\n",
        "    steps_out = ddim_timesteps + 1\n",
        "\n",
        "    return steps_out\n",
        "\n",
        "\n",
        "def make_ddim_sampling_parameters(alphacums, ddim_timesteps, eta):\n",
        "    # select alphas for computing the variance schedule\n",
        "    alphas = alphacums[ddim_timesteps]\n",
        "    alphas_prev = np.asarray([alphacums[0]] + alphacums[ddim_timesteps[:-1]].tolist())\n",
        "\n",
        "    # according the the formula provided in https://arxiv.org/abs/2010.02502\n",
        "    sigmas = eta * np.sqrt((1 - alphas_prev) / (1 - alphas) * (1 - alphas / alphas_prev))\n",
        "\n",
        "    return sigmas, alphas, alphas_prev\n",
        "\n",
        "def makerng():\n",
        "  if seed == 0:\n",
        "    rng=random.randint(0, 2**32)\n",
        "    np.random.seed(rng)\n",
        "    torch.manual_seed(rng)\n",
        "    print('random seed=')\n",
        "    print(rng)\n",
        "  else:\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "def dlpromptexample():\n",
        "  !wget https://github.com/TabuaTambalam/DalleWebms/releases/download/0.1/pexmp.7z\n",
        "  !7z x pexmp.7z\n",
        "  \n",
        "\n",
        "def mkmodel_state_dict():\n",
        "  try:\n",
        "    import jkt\n",
        "  except:\n",
        "    !wget https://raw.githubusercontent.com/TabuaTambalam/DalleWebms/main/docs/sd/jkt.py\n",
        "    import jkt\n",
        "  \n",
        "  difjit=[diffusion_emb,diffusion_mid,diffusion_out]\n",
        "  model_state_dict = {}\n",
        "  jna1=jkt.nam1\n",
        "  for i in range(3):\n",
        "    sd=difjit[i].state_dict()\n",
        "    jna2=jkt.nam2[i]\n",
        "    for k in sd:\n",
        "      uwa=sd[k]\n",
        "      if 'pnnx' in k:\n",
        "        model_state_dict[jna2[k]]=uwa\n",
        "      else:\n",
        "        model_state_dict[jna1[k]]=uwa\n",
        "  return model_state_dict\n",
        "\n",
        "SDlatDEC=None\n",
        "def latdec(fna,scale=5.5):\n",
        "  global SDlatDEC\n",
        "  if SDlatDEC is None:\n",
        "    if not os.path.isfile('autoencoder_pnnx.pt'):\n",
        "      !wget https://huggingface.co/Larvik/sd470k/resolve/main/autoencoder_pnnx.pt\n",
        "    SDlatDEC=torch.jit.load('autoencoder_pnnx.pt').cuda()\n",
        "  lat=torch.tensor(np.load(fna)).cuda()\n",
        "  return SDlatDEC(lat*scale)\n",
        "\n",
        "\n",
        "def localhttp(root='/'):\n",
        "  global HTML\n",
        "  if not os.path.isfile('/content/sample_data/izh.txt'):\n",
        "    from IPython.core.display import HTML\n",
        "    !nohup python3 -m http.server -d {root} 8233 > /content/sample_data/izh.txt &\n",
        "\n",
        "\n",
        "def f_sampler():\n",
        "  global UseSamplr\n",
        "  if Sampler == 'euler':\n",
        "    UseSamplr = sample_euler\n",
        "  elif Sampler == 'euler_a':\n",
        "    UseSamplr = sample_euler_ancestral\n",
        "  elif Sampler == 'heun':\n",
        "    UseSamplr = sample_heun\n",
        "  elif Sampler == 'dpm_2':\n",
        "    UseSamplr = sample_dpm_2\n",
        "  elif Sampler == 'dpm_2_a':\n",
        "    UseSamplr = sample_dpm_2_ancestral\n",
        "  elif Sampler == 'lms':\n",
        "    UseSamplr = sample_lms\n",
        "\n",
        "def f_sigmas():\n",
        "  if Karras:\n",
        "    return ddim_eta*get_sigmas_karras(ddim_num_steps,model_wrap.sigmas[0].item(),model_wrap.sigmas[-1].item(),rho=KarrasRho, device=cudev )\n",
        "  else:\n",
        "    return ddim_eta*model_wrap.get_sigmas(ddim_num_steps)\n",
        "\n",
        "def fixver(ver,dfsver):\n",
        "  if ver != '470k':\n",
        "    return ''\n",
        "  return dfsver\n",
        "def f_dljit(ver='470k',dfsver=''):\n",
        "  dfsver=fixver(ver,dfsver)\n",
        "  if not os.path.isfile('diffusion_out_pnnx.pt'):\n",
        "    !pip install ftfy transformers einops accelerate\n",
        "    !wget https://huggingface.co/Larvik/sd{ver}/resolve/main/alphas_cumprod.npz\n",
        "    !wget https://huggingface.co/Larvik/tfmod/resolve/main/transformer_pnnx.pt\n",
        "    !wget https://huggingface.co/Larvik/sd{ver}/resolve/main/autoencoder_pnnx.pt\n",
        "    !wget https://huggingface.co/Larvik/sd{ver}/resolve/main/imgencoder_pnnx.pt\n",
        "    ver+=dfsver\n",
        "    !wget https://huggingface.co/Larvik/sd{ver}/resolve/main/diffusion_emb_pnnx.pt\n",
        "    !wget https://huggingface.co/Larvik/sd{ver}/resolve/main/diffusion_mid_pnnx.pt\n",
        "    !wget https://huggingface.co/Larvik/sd{ver}/resolve/main/diffusion_out_pnnx.pt\n",
        "\n",
        "\n",
        "class Insertor:\n",
        "  def __init__(self, string, n):\n",
        "    self.rpla=string\n",
        "    self.rplb=inzdict[n]\n",
        "    self.token=inzdict[n+1]\n",
        "    self.idx=[]\n",
        "    self.emb=torch.tensor(np.fromfile('UserEmb/'+string[1:-1]+'.bin',dtype=np.float32))\n",
        "  def repl(self, string):\n",
        "    return string.replace(self.rpla,self.rplb)\n",
        "\n",
        "\n",
        "# encoder\n",
        "class BERTEmbedder:\n",
        "    \"\"\" Uses a pretrained BERT tokenizer by huggingface. Vocab size: 30522 (?)\"\"\"\n",
        "\n",
        "    def __init__(self, transformer, max_length=77):\n",
        "        self.tokenizer = CLIPTokenizer.from_pretrained('openai/clip-vit-large-patch14')\n",
        "        self.max_length = max_length\n",
        "        self.insertor=[]\n",
        "        self.transformer = transformer\n",
        "        self.embedding = torch.nn.Embedding.from_pretrained(self.transformer.state_dict()['text_model_embeddings_token_embedding.weight'])\n",
        "        self.encode = self.encode0\n",
        "        \n",
        "\n",
        "    def insert(self,inz):\n",
        "      if len(inz)<3:\n",
        "        self.insertor=[]\n",
        "        return\n",
        "      n=len(self.insertor)\n",
        "      if n > 4:\n",
        "        return\n",
        "      self.insertor.append(Insertor(inz,n*2))\n",
        "      \n",
        "\n",
        "    def encode0(self, text, nsamp):\n",
        "        HasInz=False\n",
        "        if self.insertor and len(text) > 0:\n",
        "          HasInz=True\n",
        "          for iz in self.insertor:\n",
        "            text=iz.repl(text)\n",
        "            iz.idx=[]\n",
        "        batch_encoding = self.tokenizer(text, truncation=True, max_length=self.max_length, return_length=True,\n",
        "                            return_overflowing_tokens=False, padding=\"max_length\", return_tensors=\"pt\")\n",
        "\n",
        "        tokens = batch_encoding[\"input_ids\"]\n",
        "        \n",
        "        if HasInz:\n",
        "          for iz in self.insertor:\n",
        "            iz.idx=torch.where(tokens == iz.token)[1]\n",
        "\n",
        "        \n",
        "        tokens = tokens.expand(nsamp,-1)\n",
        "\n",
        "        amb=self.embedding(tokens)\n",
        "        if HasInz:\n",
        "          nl=amb.size(0)\n",
        "          for iz in self.insertor:\n",
        "            for pidx in iz.idx:\n",
        "              for i in range(nl):\n",
        "                amb[i][pidx]=iz.emb\n",
        "\n",
        "        z = self.transformer(amb)\n",
        "        return z.cuda()\n",
        "\n",
        "    def encode2(self, text, nsamp):\n",
        "        batch_encoding = self.tokenizer(text, truncation=True, max_length=self.max_length, return_length=True,\n",
        "                            return_overflowing_tokens=False, padding=\"max_length\", return_tensors=\"pt\")\n",
        "\n",
        "        tokens = batch_encoding[\"input_ids\"]\n",
        "        tokens = torch.tensor(tokens.numpy()).expand(nsamp,-1)\n",
        "\n",
        "        z = self.transformer(tokens)\n",
        "        return z.cuda()\n",
        "\n",
        "    def encode3(self, text, nsamp):\n",
        "      batch_encoding = self.tokenizer(text, truncation=True, max_length=self.max_length, return_length=True,\n",
        "                            return_overflowing_tokens=False, padding=\"max_length\", return_tensors=\"pt\")\n",
        "\n",
        "      return batch_encoding[\"input_ids\"]\n",
        "\n",
        "    def encode4(self, tokens):\n",
        "       \n",
        "         #.expand(ebb.size(0),1,-1) #.expand(1,77,-1)\n",
        "\n",
        "        z = self.transformer(tokens)\n",
        "        return z.cuda()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class CompVisDenoiser(DiscreteEpsDDPMDenoiser):\n",
        "    \"\"\"A wrapper for CompVis diffusion models.\"\"\"\n",
        "\n",
        "    def __init__(self, model, quantize=False, device='cpu'):\n",
        "        super().__init__(model, model.alphas_cumprod, quantize=quantize)\n",
        "\n",
        "    def get_eps(self, *args, **kwargs):\n",
        "        return self.inner_model.apply_model(*args, **kwargs)\n",
        "\n",
        "def get_cond_simp(d,cond):\n",
        "  return cond\n",
        "\n",
        "def get_cond_list(d,cond):\n",
        "  return cond[d]\n",
        "\n",
        "get_cond=get_cond_simp\n",
        "\n",
        "class CFGDenoiser(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.inner_model = model\n",
        "\n",
        "    def forward(self, d, x, sigma, uncond, cond, cond_scale):\n",
        "        x_in = torch.cat([x] * 2)\n",
        "        sigma_in = torch.cat([sigma] * 2)\n",
        "        cond_in = torch.cat([uncond, get_cond(d,cond)])\n",
        "        uncond, cond = self.inner_model(x_in, sigma_in, cond=cond_in).chunk(2)\n",
        "        return uncond + (cond - uncond) * cond_scale\n",
        "\n",
        "\n",
        "class SRDenoiser(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.inner_model = model\n",
        "\n",
        "    def forward(self, x, sigma, cond ):\n",
        "        cond = self.inner_model(x, sigma, cond=cond)\n",
        "        return cond\n",
        "\n",
        "\n",
        "class CompVisJIT():\n",
        "  def __init__(self):\n",
        "    self.alphas_cumprod=torch.tensor(alphas_cumprod,device=cudev)\n",
        "    self.apply_model=apply_model\n",
        "\n",
        "class ifeeder():\n",
        "  def __init__(self):\n",
        "    self.getn=self.get_simp\n",
        "  def get_simp(self,n):\n",
        "    return self.bs\n",
        "  def setbs(self,in_bs):\n",
        "    self.bs=in_bs\n",
        "  \n",
        "  def get_npbins(self,n):\n",
        "    return torch.tensor(np.fromfile(self.pattern%(n+1),dtype=np.float32).reshape(self.shape),device=cudev)+self.noiseadd\n",
        "\n",
        "Karras=False\n",
        "model_wrap=None"
      ],
      "metadata": {
        "id": "YzmPpU9reaMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if not os.path.isfile('MultiPromptExample1.txt'):\n",
        "  t3 = Thread(target = dlpromptexample)\n",
        "  a3 = t3.start()"
      ],
      "metadata": {
        "id": "yh5Ms_sLOk_f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "872ef901-c3e8-4ba8-853b-6a9c340ab397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/MultiPromptExample1.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Super Resolution 4x<br>\n",
        "Select one of these task: Super Resolution, txt2img, (old ldm)infilling"
      ],
      "metadata": {
        "id": "115Y_TyxYkbk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4vrU_GL_6f3",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "jit=False #@param {type:'boolean'}\n",
        "\n",
        "import os\n",
        "\n",
        "if not os.path.isfile('fsd_pnnx.pt'):\n",
        "  !wget https://huggingface.co/Larvik/LDMjit/resolve/main/alphas_cumprod.npy\n",
        "  !wget https://huggingface.co/Larvik/LDMjit/resolve/main/dm_pnnx.pt\n",
        "  !wget https://huggingface.co/Larvik/LDMjit/resolve/main/fsd_pnnx.pt\n",
        "\n",
        "\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import functools\n",
        "import torch\n",
        "\n",
        "cudev=torch.device('cuda')\n",
        "\n",
        "alphas_cumprod = np.load('alphas_cumprod.npy')\n",
        "\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "torch.set_num_threads(os.cpu_count())\n",
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True\n",
        "\n",
        "\n",
        "# ======================\n",
        "# Arguemnt Parser Config\n",
        "# ======================\n",
        "\n",
        "def imread(filename, flags=cv2.IMREAD_COLOR):\n",
        "    if not os.path.isfile(filename):\n",
        "        print(f\"File does not exist: {filename}\")\n",
        "        sys.exit()\n",
        "    data = np.fromfile(filename, np.int8)\n",
        "    img = cv2.imdecode(data, flags)\n",
        "    return img\n",
        "\n",
        "def preprocessing_img(img):\n",
        "    if len(img.shape) < 3:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGRA)\n",
        "    elif img.shape[2] == 3:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2BGRA)\n",
        "    elif img.shape[2] == 1:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGRA)\n",
        "    return img\n",
        "\n",
        "\n",
        "def load_image(image_path):\n",
        "    if os.path.isfile(image_path):\n",
        "        img = imread(image_path, cv2.IMREAD_UNCHANGED)\n",
        "    else:\n",
        "        print(f'{image_path} not found.')\n",
        "    return preprocessing_img(img)\n",
        "\n",
        "\n",
        "\n",
        "def meshgrid(h, w):\n",
        "    y = torch.arange(0, h).view(h, 1, 1).repeat(1, w, 1)\n",
        "    x = torch.arange(0, w).view(1, w, 1).repeat(h, 1, 1)\n",
        "\n",
        "    arr = torch.cat([y, x], dim=-1)\n",
        "    return arr\n",
        "\n",
        "\n",
        "def delta_border(h, w):\n",
        "    \"\"\"\n",
        "    :param h: height\n",
        "    :param w: width\n",
        "    :return: normalized distance to image border,\n",
        "      wtith min distance = 0 at border and max dist = 0.5 at image center\n",
        "    \"\"\"\n",
        "    lower_right_corner = torch.tensor([h - 1, w - 1]).view(1, 1, 2)\n",
        "    arr = meshgrid(h, w) / lower_right_corner\n",
        "    dist_left_up = torch.min(arr, dim=-1, keepdims=True)[0]\n",
        "    dist_right_down = torch.min(1 - arr, dim=-1, keepdims=True)[0]\n",
        "    edge_dist = torch.min(torch.cat([dist_left_up, dist_right_down], dim=-1), dim=-1)[0]\n",
        "    return edge_dist\n",
        "\n",
        "\n",
        "\n",
        "def get_weighting(h, w, Ly, Lx, device):\n",
        "  clip_min_weight = 0.01\n",
        "  clip_max_weight = 0.5\n",
        "  weighting = delta_border(h, w)\n",
        "  weighting = torch.clip(weighting, clip_min_weight, clip_max_weight, )\n",
        "  weighting = weighting.view(1, h * w, 1).repeat(1, 1, Ly * Lx).to(device)\n",
        "\n",
        "\n",
        "  return weighting\n",
        "\n",
        "def get_fold_unfold(x, kernel_size, stride, uf=1, df=1):  # todo load once not every time, shorten code\n",
        "    \"\"\"\n",
        "    :param x: img of size (bs, c, h, w)\n",
        "    :return: n img crops of size (n, bs, c, kernel_size[0], kernel_size[1])\n",
        "    \"\"\"\n",
        "    bs, nc, h, w = x.shape\n",
        "\n",
        "    # number of crops in image\n",
        "    Ly = (h - kernel_size[0]) // stride[0] + 1\n",
        "    Lx = (w - kernel_size[1]) // stride[1] + 1\n",
        "\n",
        "    if uf == 1 and df == 1:\n",
        "        fold_params = dict(kernel_size=kernel_size, dilation=1, padding=0, stride=stride)\n",
        "        unfold = torch.nn.Unfold(**fold_params)\n",
        "\n",
        "        fold = torch.nn.Fold(output_size=x.shape[2:], **fold_params)\n",
        "\n",
        "        weighting = get_weighting(kernel_size[0], kernel_size[1], Ly, Lx, x.device).to(x.dtype)\n",
        "        normalization = fold(weighting).view(1, 1, h, w)  # normalizes the overlap\n",
        "        weighting = weighting.view((1, 1, kernel_size[0], kernel_size[1], Ly * Lx))\n",
        "\n",
        "    elif uf > 1 and df == 1:\n",
        "        fold_params = dict(kernel_size=kernel_size, dilation=1, padding=0, stride=stride)\n",
        "        unfold = torch.nn.Unfold(**fold_params)\n",
        "\n",
        "        fold_params2 = dict(kernel_size=(kernel_size[0] * uf, kernel_size[0] * uf),\n",
        "                            dilation=1, padding=0,\n",
        "                            stride=(stride[0] * uf, stride[1] * uf))\n",
        "        fold = torch.nn.Fold(output_size=(x.shape[2] * uf, x.shape[3] * uf), **fold_params2)\n",
        "\n",
        "        weighting = get_weighting(kernel_size[0] * uf, kernel_size[1] * uf, Ly, Lx, x.device).to(x.dtype)\n",
        "        normalization = fold(weighting).view(1, 1, h * uf, w * uf)  # normalizes the overlap\n",
        "        weighting = weighting.view((1, 1, kernel_size[0] * uf, kernel_size[1] * uf, Ly * Lx))\n",
        "\n",
        "    elif df > 1 and uf == 1:\n",
        "        fold_params = dict(kernel_size=kernel_size, dilation=1, padding=0, stride=stride)\n",
        "        unfold = torch.nn.Unfold(**fold_params)\n",
        "\n",
        "        fold_params2 = dict(kernel_size=(kernel_size[0] // df, kernel_size[0] // df),\n",
        "                            dilation=1, padding=0,\n",
        "                            stride=(stride[0] // df, stride[1] // df))\n",
        "        fold = torch.nn.Fold(output_size=(x.shape[2] // df, x.shape[3] // df), **fold_params2)\n",
        "\n",
        "        weighting = get_weighting(kernel_size[0] // df, kernel_size[1] // df, Ly, Lx, x.device).to(x.dtype)\n",
        "        normalization = fold(weighting).view(1, 1, h // df, w // df)  # normalizes the overlap\n",
        "        weighting = weighting.view((1, 1, kernel_size[0] // df, kernel_size[1] // df, Ly * Lx))\n",
        "\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    return fold, unfold, normalization, weighting\n",
        "\n",
        "\n",
        "\n",
        "def normalize_image(image, normalize_type='255'):\n",
        "    \"\"\"\n",
        "    Normalize image\n",
        "    Parameters\n",
        "    ----------\n",
        "    image: numpy array\n",
        "        The image you want to normalize\n",
        "    normalize_type: string\n",
        "        Normalize type should be chosen from the type below.\n",
        "        - '255': simply dividing by 255.0\n",
        "        - '127.5': output range : -1 and 1\n",
        "        - 'ImageNet': normalize by mean and std of ImageNet\n",
        "        - 'None': no normalization\n",
        "    Returns\n",
        "    -------\n",
        "    normalized_image: numpy array\n",
        "    \"\"\"\n",
        "    if normalize_type == 'None':\n",
        "        return image\n",
        "    elif normalize_type == '255':\n",
        "        return image / 255.0\n",
        "    elif normalize_type == '127.5':\n",
        "        return image / 127.5 - 1.0\n",
        "    elif normalize_type == 'ImageNet':\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        image = image / 255.0\n",
        "        for i in range(3):\n",
        "            image[:, :, i] = (image[:, :, i] - mean[i]) / std[i]\n",
        "        return image\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "\n",
        "\n",
        "def preprocess(img):\n",
        "    im_h, im_w, _ = img.shape\n",
        "\n",
        "    up_f = 4\n",
        "    oh, ow = up_f * im_h, up_f * im_w\n",
        "\n",
        "    img = normalize_image(img, normalize_type='255')\n",
        "\n",
        "    c = img * 2 - 1\n",
        "    c = c.transpose(2, 0, 1)  # HWC -> CHW\n",
        "    c = np.expand_dims(c, axis=0)\n",
        "    c = c.astype(np.float32)\n",
        "\n",
        "    return None, c\n",
        "\n",
        "\n",
        "def postprocess(sample):\n",
        "    sample = np.clip(sample, -1., 1.)\n",
        "    sample = (sample + 1.) / 2. * 255\n",
        "    sample = np.transpose(sample, (1, 2, 0))\n",
        "    sample = sample[:, :, ::-1]  # RGB -> BGR\n",
        "    sample = sample.astype(np.uint8)\n",
        "\n",
        "    return sample\n",
        "\n",
        "\n",
        "def decode_first_stage(z):\n",
        "    ks = (128, 128)\n",
        "    stride = (64, 64)\n",
        "    uf = 4\n",
        "\n",
        "    bs, nc, h, w = z.shape\n",
        "\n",
        "    fold, unfold, normalization, weighting = get_fold_unfold(z, ks, stride, uf=uf)\n",
        "\n",
        "    z = unfold(z)  # (bn, nc * prod(**ks), L)\n",
        "\n",
        "    # Reshape to img shape\n",
        "    z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n",
        "\n",
        "\n",
        "    print('first_stage_decode...')\n",
        "\n",
        "    outputs = []\n",
        "    for i in range(z.shape[-1]):\n",
        "        x = z[:, :, :, :, i]\n",
        "        output = first_stage_decode(x)\n",
        "        outputs.append(output[0])\n",
        "\n",
        "    o = torch.stack(outputs, axis=-1)  # # (bn, nc, ks[0], ks[1], L)\n",
        "    o = o * weighting\n",
        "    # Reverse 1. reshape to img shape\n",
        "    o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n",
        "    # stitch crops together\n",
        "    decoded = fold(o)\n",
        "    decoded = decoded / normalization  # norm is shape (1, 1, h, w)\n",
        "    return decoded\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ddpm\n",
        "def apply_model(x, t, cond):\n",
        "    x_noisy=x\n",
        "    ks = (128, 128)\n",
        "    stride = (64, 64)\n",
        "\n",
        "    h, w = x_noisy.shape[-2:]\n",
        "\n",
        "    fold, unfold, normalization, weighting = get_fold_unfold(x_noisy, ks, stride)\n",
        "\n",
        "\n",
        "    z = unfold(x_noisy)  # (bn, nc * prod(**ks), L)\n",
        "    # Reshape to img shape\n",
        "    z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n",
        "    z_list = [z[:, :, :, :, i] for i in range(z.shape[-1])]\n",
        "\n",
        "    c = unfold(cond)\n",
        "    c = c.view((c.shape[0], -1, ks[0], ks[1], c.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n",
        "    cond_list = [c[:, :, :, :, i] for i in range(c.shape[-1])]\n",
        "\n",
        "    # apply model by loop over crops\n",
        "    \n",
        "    outputs = []\n",
        "    for i in range(z.shape[-1]):\n",
        "        x = z_list[i]\n",
        "        cond = cond_list[i]\n",
        "        xc = torch.cat([x, cond], dim=1)\n",
        "        \n",
        "        \n",
        "        output = diffusion_model(xc, t)\n",
        "            \n",
        "       \n",
        "        outputs.append(output[0])\n",
        "\n",
        "    o = torch.stack(outputs, axis=-1)\n",
        "    o = o * weighting\n",
        "    # Reverse reshape to img shape\n",
        "    o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n",
        "    # stitch crops together\n",
        "    x_recon = fold(o) / normalization\n",
        "\n",
        "\n",
        "    return x_recon\n",
        "\n",
        "def warmup():\n",
        "  v_0 = torch.rand(1,6,128,128, dtype=torch.float).half().cuda()\n",
        "  v_1 = torch.randint(10, (1, ), dtype=torch.long).cuda()\n",
        "\n",
        "  for d in range(2):\n",
        "    with torch.cuda.amp.autocast(dtype=torch.float16):\n",
        "      uaa = diffusion_model(v_0,v_1)\n",
        "  v_0 = torch.rand(1,3,128,128, dtype=torch.float).cuda()\n",
        "  for d in range(2):\n",
        "    uaa = first_stage_decode(v_0)\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "UseSamplr=sample_euler_ancestral\n",
        "def predict(c):\n",
        "    \n",
        "    c=torch.tensor(c,device=cudev)\n",
        "\n",
        "\n",
        "    sigmas = f_sigmas()\n",
        "\n",
        "    noise = torch.randn(c.shape, dtype=torch.float,device=cudev)\n",
        "    \n",
        "    extra_args = {'cond': c}\n",
        "    df=detail_strength/(detail_strength-1+float(sigmas[0]))\n",
        "    print(df)\n",
        "    with torch.cuda.amp.autocast(dtype=torch.float16):\n",
        "        samples = UseSamplr(model_wrap_cfg, noise * sigmas[0] * df , sigmas, extra_args=extra_args, disable=False)\n",
        "   \n",
        "    x_sample = decode_first_stage(samples)\n",
        "\n",
        "    img = postprocess(x_sample[0].cpu().numpy())\n",
        "\n",
        "    return img\n",
        "\n",
        "if model_wrap is None:\n",
        "  first_stage_decode=torch.jit.load('/content/fsd_pnnx.pt').eval().cuda()\n",
        "  diffusion_model=torch.jit.load('/content/dm_pnnx.pt').eval().half().cuda()\n",
        "  warmup()\n",
        "  torch.cuda.empty_cache()\n",
        "  model_wrap = CompVisDenoiser(CompVisJIT())\n",
        "  model_wrap_cfg = SRDenoiser(model_wrap)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "image_path='/content/sample_data/10_0x0v1.png' #@param {type:'string'}\n",
        "\n",
        "\"\"\"\n",
        "ddim_timesteps\n",
        "\"\"\"\n",
        "ddim_eta = 0.75  #@param {type:'number'}\n",
        "ddim_num_steps = 100  #@param {type:'number'}\n",
        "ddpm_num_timesteps = 1000 #@param {type:'number'}\n",
        "\n",
        "detail_strength=20000  #@param {type:'number'}\n",
        "\n",
        "ddim_timesteps = make_ddim_timesteps(ddim_num_steps, ddpm_num_timesteps)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "ddim sampling parameters\n",
        "\"\"\"\n",
        "\n",
        "ddim_sigmas, ddim_alphas, ddim_alphas_prev = \\\n",
        "    make_ddim_sampling_parameters(\n",
        "        alphacums=alphas_cumprod,\n",
        "        ddim_timesteps=ddim_timesteps,\n",
        "        eta=ddim_eta)\n",
        "\n",
        "#ddim_sigmas=torch.tensor(ddim_sigmas.astype(np.float32),device=cudev)\n",
        "\n",
        "ddim_sqrt_one_minus_alphas = np.sqrt(1. - ddim_alphas)\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# inference\n",
        "print('Start inference...')\n",
        "if image_path.endswith('.npy'):\n",
        "  c=latdec(image_path).detach()\n",
        "else:\n",
        "  img = load_image(image_path)\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
        "  img = img[:, :, ::-1]  # BGR -> RGB\n",
        "  _, c = preprocess(img)\n",
        "  \n",
        "\n",
        "img = predict(c)\n",
        "\n",
        "# plot result\n",
        "savepath = image_path[:-4]+'_4x.png'\n",
        "print(f'saved at : {savepath}')\n",
        "cv2.imwrite(savepath, img)\n",
        "\n"
      ],
      "metadata": {
        "id": "xrSkT1_5AJSr",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Sampler='euler_a' #@param ['euler', 'euler_a', 'heun','dpm_2','dpm_2_a','lms']\n",
        "f_sampler()\n",
        "\n",
        "Karras=False #@param {type:'boolean'}\n",
        "KarrasRho = 7.0 #@param {type:'number'}"
      ],
      "metadata": {
        "id": "2UYWGCLIKSFS",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional: SD lat decoder"
      ],
      "metadata": {
        "id": "_jVyaZSmEfPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latent='4x6_1x1v1.npy' #@param {type:'string'}\n",
        "\n",
        "ymg=Image.fromarray( (( ( latdec(latent)[0] +1)*127.5 ).cpu().numpy()).transpose(1,2,0).clip(0,255).astype(np.uint8) )\n",
        "ymg.save(latent[:-4]+'.png')\n",
        "ymg"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TUrKQWmQFo7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional: GFPgan-jit"
      ],
      "metadata": {
        "id": "c79C9jECmn-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import glob\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "import math\n",
        "\n",
        "\n",
        "from torchvision.transforms.functional import normalize\n",
        "from itertools import product\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def imwrite(img, file_path, params=None, auto_mkdir=True):\n",
        "\n",
        "    if auto_mkdir:\n",
        "        dir_name = os.path.abspath(os.path.dirname(file_path))\n",
        "        os.makedirs(dir_name, exist_ok=True)\n",
        "    ok = cv2.imwrite(file_path, img, params)\n",
        "    if not ok:\n",
        "        raise IOError('Failed in writing images.')\n",
        "\n",
        "\n",
        "\n",
        "def bb_intersection_over_union(boxA, boxB):\n",
        "    # determine the (x, y)-coordinates of the intersection rectangle\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[2], boxB[2])\n",
        "    yB = min(boxA[3], boxB[3])\n",
        "    # compute the area of intersection rectangle\n",
        "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
        "    # compute the area of both the prediction and ground-truth\n",
        "    # rectangles\n",
        "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
        "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
        "    # compute the intersection over union by taking the intersection\n",
        "    # area and dividing it by the sum of prediction + ground-truth\n",
        "    # areas - the interesection area\n",
        "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
        "    # return the intersection over union value\n",
        "    return iou\n",
        "\n",
        "\n",
        "def nms_boxes(boxes, scores, iou_thres):\n",
        "    # Performs non-maximum suppression (NMS) on the boxes according to their intersection-over-union (IoU).\n",
        "\n",
        "    keep = []\n",
        "    for i, box_a in enumerate(boxes):\n",
        "        is_keep = True\n",
        "        for j in range(i):\n",
        "            if not keep[j]:\n",
        "                continue\n",
        "            box_b = boxes[j]\n",
        "            iou = bb_intersection_over_union(box_a, box_b)\n",
        "            if iou >= iou_thres:\n",
        "                if scores[i] > scores[j]:\n",
        "                    keep[j] = False\n",
        "                else:\n",
        "                    is_keep = False\n",
        "                    break\n",
        "\n",
        "        keep.append(is_keep)\n",
        "\n",
        "    return np.array(keep).nonzero()[0]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_anchor(image_size):\n",
        "    \n",
        "    min_sizes = [[16, 32], [64, 128], [256, 512]]\n",
        "    steps = [8, 16, 32]\n",
        "    feature_maps = [[math.ceil(image_size[0] / step), math.ceil(image_size[1] / step)] for step in steps]\n",
        "\n",
        "    anchors = []\n",
        "    for k, f in enumerate(feature_maps):\n",
        "        m_sizes = min_sizes[k]\n",
        "        for i, j in product(range(f[0]), range(f[1])):\n",
        "            for min_size in m_sizes:\n",
        "                s_kx = min_size / image_size[1]\n",
        "                s_ky = min_size / image_size[0]\n",
        "                dense_cx = [x * steps[k] / image_size[1] for x in [j + 0.5]]\n",
        "                dense_cy = [y * steps[k] / image_size[0] for y in [i + 0.5]]\n",
        "                for cy, cx in product(dense_cy, dense_cx):\n",
        "                    anchors.extend([cx, cy, s_kx, s_ky])\n",
        "\n",
        "    output = np.array(anchors).reshape(-1, 4)\n",
        "    return output\n",
        "\n",
        "\n",
        "# Adapted from https://github.com/Hakuyume/chainer-ssd\n",
        "def decode(loc, priors, variances):\n",
        "    \"\"\"Decode locations from predictions using priors to undo\n",
        "    the encoding we did for offset regression at train time.\n",
        "    Args:\n",
        "        loc (tensor): location predictions for loc layers,\n",
        "            Shape: [num_priors,4]\n",
        "        priors (tensor): Prior boxes in center-offset form.\n",
        "            Shape: [num_priors,4].\n",
        "        variances: (list[float]) Variances of priorboxes\n",
        "    Return:\n",
        "        decoded bounding box predictions\n",
        "    \"\"\"\n",

        "    boxes = np.concatenate(\n",
        "        (priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\n",
        "         priors[:, 2:] * np.exp(loc[:, 2:] * variances[1])), 1)\n",
        "    boxes[:, :2] -= boxes[:, 2:] / 2\n",
        "    boxes[:, 2:] += boxes[:, :2]\n",
        "\n",
        "    return boxes\n",
        "\n",
        "\n",
        "def decode_landm(pre, priors, variances):\n",
        "    \"\"\"Decode landm from predictions using priors to undo\n",
        "    the encoding we did for offset regression at train time.\n",
        "    Args:\n",
        "        pre (tensor): landm predictions for loc layers,\n",
        "            Shape: [num_priors,10]\n",
        "        priors (tensor): Prior boxes in center-offset form.\n",
        "            Shape: [num_priors,4].\n",
        "        variances: (list[float]) Variances of priorboxes\n",
        "    Return:\n",
        "        decoded landm predictions\n",
        "    \"\"\"\n",
        "    tmp = (\n",
        "        priors[:, :2] + pre[:, :2] * variances[0] * priors[:, 2:],\n",
        "        priors[:, :2] + pre[:, 2:4] * variances[0] * priors[:, 2:],\n",
        "        priors[:, :2] + pre[:, 4:6] * variances[0] * priors[:, 2:],\n",
        "        priors[:, :2] + pre[:, 6:8] * variances[0] * priors[:, 2:],\n",
        "        priors[:, :2] + pre[:, 8:10] * variances[0] * priors[:, 2:],\n",
        "    )\n",
        "    landms = np.concatenate(tmp, axis=1)\n",
        "\n",
        "    return landms\n",
        "\n",
        "\n",
        "\n",
        "def detect_faces(\n",
        "        image,\n",
        "        conf_threshold=0.8,\n",
        "        nms_threshold=0.4,\n",
        "        use_origin_size=True,\n",
        "    ):\n",
        "        \n",
        "        height, width = image.shape[:2]\n",
        "        image = image.transpose(2, 0, 1).astype(np.float32)\n",
        "        image = torch.from_numpy(image).to(cudevg).unsqueeze(0)\n",
        "\n",
        "        image = image - torch.tensor([[[[104.]], [[117.]], [[123.]]]])\n",
        "\n",
        "        loc, conf, landmarks = RetinaFace(image)\n",

        "        priors = get_anchor((height, width))\n",
        "\n",
        "        variance = [0.1, 0.2]\n",
        "        scale = np.array([width, height, width, height])\n",
        "        scale1 = np.array([\n",
        "            width, height, width, height, width, height, width, height, width, height\n",
        "        ])\n",
        "\n",
        "        boxes = decode(loc[0].cpu().numpy(), priors, variance)\n",
        "        boxes = boxes * scale\n",
        "        \n",
        "\n",
        "        scores = conf[0][:, 1].cpu().numpy()\n",
        "\n",
        "        landmarks = decode_landm(landmarks[0].cpu().numpy(), priors, variance)\n",
        "        landmarks = landmarks * scale1\n",
        "        \n",
        "\n",
        "        # ignore low scores\n",
        "        inds = np.where(scores > conf_threshold)[0]\n",
        "        boxes, landmarks, scores = boxes[inds], landmarks[inds], scores[inds]\n",
        "\n",
        "        # sort\n",
        "        order = scores.argsort()[::-1]\n",
        "        boxes, landmarks, scores = boxes[order], landmarks[order], scores[order]\n",
        "\n",
        "        # do NMS\n",
        "        bounding_boxes = np.hstack((boxes, scores[:, np.newaxis])).astype(np.float32, copy=False)\n",
        "        keep = nms_boxes(bounding_boxes[:, :4], bounding_boxes[:, 4], nms_threshold)\n",
        "        bounding_boxes, landmarks = bounding_boxes[keep, :], landmarks[keep]\n",

        "        return np.concatenate((bounding_boxes, landmarks), axis=1)\n",
        "\n",
        "def get_largest_face(det_faces, h, w):\n",
        "\n",
        "    def get_location(val, length):\n",
        "        if val < 0:\n",
        "            return 0\n",
        "        elif val > length:\n",
        "            return length\n",
        "        else:\n",
        "            return val\n",
        "\n",
        "    face_areas = []\n",
        "    for det_face in det_faces:\n",
        "        left = get_location(det_face[0], w)\n",
        "        right = get_location(det_face[2], w)\n",
        "        top = get_location(det_face[1], h)\n",
        "        bottom = get_location(det_face[3], h)\n",
        "        face_area = (right - left) * (bottom - top)\n",
        "        face_areas.append(face_area)\n",
        "    largest_idx = face_areas.index(max(face_areas))\n",
        "    return det_faces[largest_idx], largest_idx\n",
        "\n",
        "\n",
        "def get_center_face(det_faces, h=0, w=0, center=None):\n",
        "    if center is not None:\n",
        "        center = np.array(center)\n",
        "    else:\n",
        "        center = np.array([w / 2, h / 2])\n",
        "    center_dist = []\n",
        "    for det_face in det_faces:\n",
        "        face_center = np.array([(det_face[0] + det_face[2]) / 2, (det_face[1] + det_face[3]) / 2])\n",
        "        dist = np.linalg.norm(face_center - center)\n",
        "        center_dist.append(dist)\n",
        "    center_idx = center_dist.index(min(center_dist))\n",
        "    return det_faces[center_idx], center_idx\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def img2tensor(imgs, bgr2rgb=True, float32=True):\n",
        "\n",
        "\n",
        "    def _totensor(img, bgr2rgb, float32):\n",
        "        if img.shape[2] == 3 and bgr2rgb:\n",
        "            if img.dtype == 'float64':\n",
        "                img = img.astype('float32')\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = torch.from_numpy(img.transpose(2, 0, 1))\n",
        "        if float32:\n",
        "            img = img.float()\n",
        "        return img\n",
        "\n",
        "    if isinstance(imgs, list):\n",
        "        return [_totensor(img, bgr2rgb, float32) for img in imgs]\n",
        "    else:\n",
        "        return _totensor(imgs, bgr2rgb, float32)\n",
        "\n",
        "\n",
        "\n",
        "def read_image(img):\n",
        "    \"\"\"img can be image path or cv2 loaded image.\"\"\"\n",
        "    # self.input_img is Numpy array, (h, w, c), BGR, uint8, [0, 255]\n",
        "\n",
        "\n",
        "    if np.max(img) > 256:  # 16-bit image\n",
        "        img = (img / 65535) * 255\n",
        "    if len(img.shape) == 2:  # gray image\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
        "    elif img.shape[2] == 4:  # RGBA image with alpha channel\n",
        "        img = img[:, :, 0:3]\n",
        "\n",
        "    return img\n",
        "'''\n",
        "def srproc(img,fac):\n",
        "  return cv2.resize(img, None,fx=fac,fy=fac, interpolation=cv2.INTER_LINEAR)\n",
        "'''\n",
        "def srproc(img,fac):\n",
        "  _,c=preprocess(img[:, :, ::-1])\n",
        "  return predict(c)\n",
        "\n",
        "class faceimg:\n",
        "  def __init__(self, image,\n",
        "                 face_size=512,\n",
        "                 crop_ratio=(1, 1),\n",
        "                 save_ext='png',\n",
        "                 template_3points=False,\n",
        "                 pad_blur=False,\n",
        "                 use_parse=False,\n",
        "                 device=None):\n",
        "    self.nXimage=read_image(image)\n",
        "    downscale=1/upscale\n",
        "    self.input_img=cv2.resize(self.nXimage,None,fx=downscale,fy=downscale,interpolation=cv2.INTER_AREA)\n",
        "    self.template_3points = template_3points  # improve robustness\n",
        "    self.upscale_factor = upscale\n",
        "    # the cropped face ratio based on the square face\n",
        "    self.crop_ratio = crop_ratio  # (h, w)\n",
        "    assert (self.crop_ratio[0] >= 1 and self.crop_ratio[1] >= 1), 'crop ration only supports >=1'\n",
        "    self.face_size = (int(face_size * self.crop_ratio[1]), int(face_size * self.crop_ratio[0]))\n",
        "\n",
        "    if self.template_3points:\n",
        "        self.face_template = np.array([[192, 240], [319, 240], [257, 371]])\n",
        "    else:\n",
        "        # standard 5 landmarks for FFHQ faces with 512 x 512\n",
        "        self.face_template = np.array([[192.98138, 239.94708], [318.90277, 240.1936], [256.63416, 314.01935],\n",
        "                                        [201.26117, 371.41043], [313.08905, 371.15118]])\n",
        "    self.face_template = self.face_template * (face_size / 512.0)\n",
        "    if self.crop_ratio[0] > 1:\n",
        "        self.face_template[:, 1] += face_size * (self.crop_ratio[0] - 1) / 2\n",
        "    if self.crop_ratio[1] > 1:\n",
        "        self.face_template[:, 0] += face_size * (self.crop_ratio[1] - 1) / 2\n",
        "    self.save_ext = save_ext\n",
        "    self.pad_blur = pad_blur\n",
        "    if self.pad_blur is True:\n",
        "        self.template_3points = False\n",
        "\n",
        "    self.all_landmarks_5 = []\n",
        "    self.det_faces = []\n",
        "    self.affine_matrices = []\n",
        "    self.inverse_affine_matrices = []\n",
        "    self.cropped_faces = []\n",
        "    self.pad_input_imgs = []\n",
        "    self.restored_faces=[]\n",
        "\n",
        "\n",
        "    # init face parsing model\n",
        "    self.use_parse = use_parse\n",
        "  def get_face_landmarks_5(self,\n",
        "              only_keep_largest=False,\n",
        "              only_center_face=False,\n",
        "              resize=None,\n",
        "              blur_ratio=0.01,\n",
        "              eye_dist_threshold=None):\n",
        "    if resize is None:\n",
        "        scale = 1\n",
        "        input_img = self.input_img\n",
        "    else:\n",
        "        h, w = self.input_img.shape[0:2]\n",
        "        scale = min(h, w) / resize\n",
        "        h, w = int(h / scale), int(w / scale)\n",
        "        input_img = cv2.resize(self.input_img, (w, h), interpolation=cv2.INTER_LANCZOS4)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        bboxes = detect_faces( input_img ) * scale #0.97\n",
        "    for bbox in bboxes:\n",
        "        # remove faces with too small eye distance: side faces or too small faces\n",
        "        eye_dist = np.linalg.norm([bbox[6] - bbox[8], bbox[7] - bbox[9]])\n",
        "        if eye_dist_threshold is not None and (eye_dist < eye_dist_threshold):\n",
        "            continue\n",
        "\n",
        "        if self.template_3points:\n",
        "            landmark = np.array([[bbox[i], bbox[i + 1]] for i in range(5, 11, 2)])\n",
        "        else:\n",
        "            landmark = np.array([[bbox[i], bbox[i + 1]] for i in range(5, 15, 2)])\n",
        "        self.all_landmarks_5.append(landmark)\n",
        "        self.det_faces.append(bbox[0:5])\n",
        "    if len(self.det_faces) == 0:\n",
        "        return 0\n",
        "    if only_keep_largest:\n",
        "        h, w, _ = self.input_img.shape\n",
        "        self.det_faces, largest_idx = get_largest_face(self.det_faces, h, w)\n",
        "        self.all_landmarks_5 = [self.all_landmarks_5[largest_idx]]\n",
        "    elif only_center_face:\n",
        "        h, w, _ = self.input_img.shape\n",
        "        self.det_faces, center_idx = get_center_face(self.det_faces, h, w)\n",
        "        self.all_landmarks_5 = [self.all_landmarks_5[center_idx]]\n",
        "\n",
        "    # pad blurry images\n",
        "    if self.pad_blur:\n",
        "        self.pad_input_imgs = []\n",
        "        for landmarks in self.all_landmarks_5:\n",
        "            # get landmarks\n",
        "            eye_left = landmarks[0, :]\n",
        "            eye_right = landmarks[1, :]\n",
        "            eye_avg = (eye_left + eye_right) * 0.5\n",
        "            mouth_avg = (landmarks[3, :] + landmarks[4, :]) * 0.5\n",
        "            eye_to_eye = eye_right - eye_left\n",
        "            eye_to_mouth = mouth_avg - eye_avg\n",
        "\n",
        "            # Get the oriented crop rectangle\n",
        "            # x: half width of the oriented crop rectangle\n",
        "            x = eye_to_eye - np.flipud(eye_to_mouth) * [-1, 1]\n",
        "            #  - np.flipud(eye_to_mouth) * [-1, 1]: rotate 90 clockwise\n",
        "            # norm with the hypotenuse: get the direction\n",
        "            x /= np.hypot(*x)  # get the hypotenuse of a right triangle\n",
        "            rect_scale = 1.5\n",
        "            x *= max(np.hypot(*eye_to_eye) * 2.0 * rect_scale, np.hypot(*eye_to_mouth) * 1.8 * rect_scale)\n",
        "            # y: half height of the oriented crop rectangle\n",
        "            y = np.flipud(x) * [-1, 1]\n",
        "\n",
        "            # c: center\n",
        "            c = eye_avg + eye_to_mouth * 0.1\n",
        "            # quad: (left_top, left_bottom, right_bottom, right_top)\n",
        "            quad = np.stack([c - x - y, c - x + y, c + x + y, c + x - y])\n",
        "            # qsize: side length of the square\n",
        "            qsize = np.hypot(*x) * 2\n",
        "            border = max(int(np.rint(qsize * 0.1)), 3)\n",
        "\n",
        "            # get pad\n",
        "            # pad: (width_left, height_top, width_right, height_bottom)\n",
        "            pad = (int(np.floor(min(quad[:, 0]))), int(np.floor(min(quad[:, 1]))), int(np.ceil(max(quad[:, 0]))),\n",
        "                    int(np.ceil(max(quad[:, 1]))))\n",
        "            pad = [\n",
        "                max(-pad[0] + border, 1),\n",
        "                max(-pad[1] + border, 1),\n",
        "                max(pad[2] - self.input_img.shape[0] + border, 1),\n",
        "                max(pad[3] - self.input_img.shape[1] + border, 1)\n",
        "            ]\n",
        "\n",
        "            if max(pad) > 1:\n",
        "                # pad image\n",
        "                pad_img = np.pad(self.input_img, ((pad[1], pad[3]), (pad[0], pad[2]), (0, 0)), 'reflect')\n",
        "                # modify landmark coords\n",
        "                landmarks[:, 0] += pad[0]\n",
        "                landmarks[:, 1] += pad[1]\n",
        "                # blur pad images\n",
        "                h, w, _ = pad_img.shape\n",
        "                y, x, _ = np.ogrid[:h, :w, :1]\n",
        "                mask = np.maximum(1.0 - np.minimum(np.float32(x) / pad[0],\n",
        "                                                    np.float32(w - 1 - x) / pad[2]),\n",
        "                                  1.0 - np.minimum(np.float32(y) / pad[1],\n",
        "                                                    np.float32(h - 1 - y) / pad[3]))\n",
        "                blur = int(qsize * blur_ratio)\n",
        "                if blur % 2 == 0:\n",
        "                    blur += 1\n",
        "                blur_img = cv2.boxFilter(pad_img, 0, ksize=(blur, blur))\n",
        "                # blur_img = cv2.GaussianBlur(pad_img, (blur, blur), 0)\n",
        "\n",
        "                pad_img = pad_img.astype('float32')\n",
        "                pad_img += (blur_img - pad_img) * np.clip(mask * 3.0 + 1.0, 0.0, 1.0)\n",
        "                pad_img += (np.median(pad_img, axis=(0, 1)) - pad_img) * np.clip(mask, 0.0, 1.0)\n",
        "                pad_img = np.clip(pad_img, 0, 255)  # float32, [0, 255]\n",
        "                self.pad_input_imgs.append(pad_img)\n",
        "            else:\n",
        "                self.pad_input_imgs.append(np.copy(self.input_img))\n",
        "\n",
        "    return len(self.all_landmarks_5)\n",
        "  def align_warp_face(self, save_cropped_path=None, border_mode='constant'):\n",
        "    \"\"\"Align and warp faces with face template.\n",
        "    \"\"\"\n",
        "    if self.pad_blur:\n",
        "        assert len(self.pad_input_imgs) == len(\n",
        "            self.all_landmarks_5), f'Mismatched samples: {len(self.pad_input_imgs)} and {len(self.all_landmarks_5)}'\n",
        "    for idx, landmark in enumerate(self.all_landmarks_5):\n",
        "        # use 5 landmarks to get affine matrix\n",
        "        # use cv2.LMEDS method for the equivalence to skimage transform\n",
        "        # ref: https://blog.csdn.net/yichxi/article/details/115827338\n",
        "        affine_matrix = cv2.estimateAffinePartial2D(landmark, self.face_template, method=cv2.LMEDS)[0]\n",
        "        self.affine_matrices.append(affine_matrix)\n",
        "        # warp and crop faces\n",
        "        if border_mode == 'constant':\n",
        "            border_mode = cv2.BORDER_CONSTANT\n",
        "        elif border_mode == 'reflect101':\n",
        "            border_mode = cv2.BORDER_REFLECT101\n",
        "        elif border_mode == 'reflect':\n",
        "            border_mode = cv2.BORDER_REFLECT\n",
        "        if self.pad_blur:\n",
        "            input_img = self.pad_input_imgs[idx]\n",
        "        else:\n",
        "            input_img = self.input_img\n",
        "        cropped_face = cv2.warpAffine(\n",
        "            input_img, affine_matrix, self.face_size, borderMode=border_mode, borderValue=(135, 133, 132))  # gray\n",
        "        self.cropped_faces.append(cropped_face)\n",
        "        # save the cropped face\n",
        "        if save_cropped_path is not None:\n",
        "            path = os.path.splitext(save_cropped_path)[0]\n",
        "            save_path = f'{path}_{idx:02d}.{self.save_ext}'\n",
        "            imwrite(cropped_face, save_path)\n",
        "  def add_restored_face(self, face):\n",
        "    self.restored_faces.append(face)\n",
        "  def get_inverse_affine(self, save_inverse_affine_path=None):\n",
        "    \"\"\"Get inverse affine matrix.\"\"\"\n",
        "    for idx, affine_matrix in enumerate(self.affine_matrices):\n",
        "        inverse_affine = cv2.invertAffineTransform(affine_matrix)\n",
        "        inverse_affine[:, 2]*= self.upscale_factor\n",
        "        #inverse_affine *= self.upscale_factor\n",
        "        self.inverse_affine_matrices.append(inverse_affine)\n",
        "        # save inverse affine matrices\n",
        "        if save_inverse_affine_path is not None:\n",
        "            path, _ = os.path.splitext(save_inverse_affine_path)\n",
        "            save_path = f'{path}_{idx:02d}.pth'\n",
        "            torch.save(inverse_affine, save_path)\n",
        "  def paste_faces_to_input_image(self, save_path=None):\n",
        "    h, w, _ = self.input_img.shape\n",
        "    h_up, w_up = int(h * self.upscale_factor), int(w * self.upscale_factor)\n",
        "\n",
        "    upsample_img = self.nXimage\n",
        "\n",
        "    assert len(self.restored_faces) == len(\n",
        "        self.inverse_affine_matrices), ('length of restored_faces and affine_matrices are different.')\n",
        "    maskpool=None\n",
        "    restorepool=None\n",
        "    for restored_face, inverse_affine in zip(self.restored_faces, self.inverse_affine_matrices):\n",
        "        \n",
        "        if (inverse_affine[0][0]*self.upscale_factor) < 1.5:\n",
        "          inverse_affine[:, 2]/= self.upscale_factor\n",
        "          inverse_affine*=self.upscale_factor\n",
        "          restored_face=restored_face.astype('uint8')\n",
        "        else:\n",
        "          restored_face=srproc(restored_face,self.upscale_factor).astype('uint8')\n",
        "          \n",
        "\n",
        "        if self.upscale_factor > 1:\n",
        "            extra_offset = 0.5 * self.upscale_factor\n",
        "        else:\n",
        "            extra_offset = 0\n",
        "        inverse_affine[:, 2] += extra_offset\n",
        "        \n",
        "        inv_restored = cv2.warpAffine(restored_face, inverse_affine, (w_up, h_up))\n",
        "\n",
        "        \n",
        "        # inference\n",
        "        face_input = cv2.resize(restored_face, (512, 512), interpolation=cv2.INTER_LINEAR)\n",
        "        face_input = img2tensor(face_input.astype('float32') / 255., bgr2rgb=True, float32=True)\n",
        "        normalize(face_input, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5), inplace=True)\n",
        "        face_input = torch.unsqueeze(face_input, 0).to(cudevg)\n",
        "        with torch.no_grad():\n",
        "            out = face_parse(face_input)[0]\n",
        "        out = out.argmax(dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "        mask = np.zeros(out.shape)\n",
        "        MASK_COLORMAP = [0, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 0, 255, 0, 0, 0]\n",
        "        for idx, color in enumerate(MASK_COLORMAP):\n",
        "            mask[out == idx] = color\n",
        "        #  blur the mask\n",
        "        mask = cv2.GaussianBlur(mask, (101, 101), 11)\n",
        "        mask = cv2.GaussianBlur(mask, (101, 101), 11)\n",
        "        # remove the black borders\n",
        "        thres = 10\n",
        "        mask[:thres, :] = 0\n",
        "        mask[-thres:, :] = 0\n",
        "        mask[:, :thres] = 0\n",
        "        mask[:, -thres:] = 0\n",
        "        mask = mask / 255.\n",
        "\n",
        "        mask = cv2.resize(mask, restored_face.shape[:2])\n",
        "        mask = cv2.warpAffine(mask, inverse_affine, (w_up, h_up), flags=3)\n",
        "        inv_soft_mask = mask[:, :, None]\n",
        "        pasted_face = inv_restored\n",
        "\n",
        "        if maskpool is None:\n",
        "          maskpool=inv_soft_mask\n",
        "          restorepool=np.zeros(inv_soft_mask.shape)\n",
        "          blanc=np.ones(inv_soft_mask.shape)\n",
        "        else:\n",
        "          maskpool = inv_soft_mask*blanc+(1 - inv_soft_mask)*maskpool\n",
        "\n",
        "        inv_hard_mask=np.array(inv_soft_mask, copy=True)\n",
        "        inv_hard_mask[np.where(inv_hard_mask!=0)]=1.0\n",
        "        restorepool = inv_hard_mask * pasted_face + (1 - inv_hard_mask) * restorepool\n",
        "\n",
        "    if np.max(upsample_img) > 256:  # 16-bit image\n",
        "        upsample_img = np.concatenate((restorepool, maskpool*65535), axis=2).astype(np.uint16)\n",
        "    else:\n",
        "        upsample_img = np.concatenate((restorepool, maskpool*255), axis=2).astype(np.uint8)\n",
        "    if save_path is not None:\n",
        "        path = os.path.splitext(save_path)[0]\n",
        "        save_path = f'{path}.{self.save_ext}'\n",
        "        imwrite(upsample_img, save_path)\n",
        "    return upsample_img\n",
        "\n",
        "\n",
        "def tensor2img(tensor, rgb2bgr=True, out_type=np.uint8, min_max=(0, 1)):\n",
        "\n",
        "    if not (torch.is_tensor(tensor) or (isinstance(tensor, list) and all(torch.is_tensor(t) for t in tensor))):\n",
        "        raise TypeError(f'tensor or list of tensors expected, got {type(tensor)}')\n",
        "\n",
        "    if torch.is_tensor(tensor):\n",
        "        tensor = [tensor]\n",
        "    result = []\n",
        "    for _tensor in tensor:\n",
        "        _tensor = _tensor.squeeze(0).float().detach().cpu().clamp_(*min_max)\n",
        "        _tensor = (_tensor - min_max[0]) / (min_max[1] - min_max[0])\n",
        "\n",
        "        n_dim = _tensor.dim()\n",
        "        if n_dim == 4:\n",
        "            img_np = make_grid(_tensor, nrow=int(math.sqrt(_tensor.size(0))), normalize=False).numpy()\n",
        "            img_np = img_np.transpose(1, 2, 0)\n",
        "            if rgb2bgr:\n",
        "                img_np = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
        "        elif n_dim == 3:\n",
        "            img_np = _tensor.numpy()\n",
        "            img_np = img_np.transpose(1, 2, 0)\n",
        "            if img_np.shape[2] == 1:  # gray image\n",
        "                img_np = np.squeeze(img_np, axis=2)\n",
        "            else:\n",
        "                if rgb2bgr:\n",
        "                    img_np = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
        "        elif n_dim == 2:\n",
        "            img_np = _tensor.numpy()\n",
        "        else:\n",
        "            raise TypeError(f'Only support 4D, 3D or 2D tensor. But received with dimension: {n_dim}')\n",
        "        if out_type == np.uint8:\n",
        "            # Unlike MATLAB, numpy.unit8() WILL NOT round by default.\n",
        "            img_np = (img_np * 255.0).round()\n",
        "        img_np = img_np.astype(out_type)\n",
        "        result.append(img_np)\n",
        "    if len(result) == 1:\n",
        "        result = result[0]\n",
        "    return result\n\n",
        "def doenh_gfp(cropped_face_t):\n",
        "  global gfpgan_enc\n",
        "  global gfpgan_dec\n",
        "  if gfpgan_enc is None:\n",
        "    gfpgan_enc =torch.jit.load('gfpgan_enc_pnnx.pt').eval().to(cudevg)\n",
        "    gfpgan_dec =torch.jit.load('gfpgan_dec_pnnx.pt').eval().to(cudevg)\n",
        "  latent, conditions = gfpgan_enc(cropped_face_t)\n",
        "  output = gfpgan_dec(latent,*conditions)\n",
        "  return output\n",
        "\n",
        "doenh=doenh_gfp\n",
        "\n",
        "@torch.no_grad()\n",
        "def enhance(img, has_aligned=False, only_center_face=False, paste_back=True):\n",
        "  \n",
        "  faces=faceimg(img)\n",
        "\n",
        "  if has_aligned:  # the inputs are already aligned\n",
        "      img = cv2.resize(img, (512, 512))\n",
        "      faces.cropped_faces = [img]\n",
        "  else:\n",
        "      faces.get_face_landmarks_5(only_center_face=only_center_face, eye_dist_threshold=5)\n",
        "      # eye_dist_threshold=5: skip faces whose eye distance is smaller than 5 pixels\n",
        "      # TODO: even with eye_dist_threshold, it will still introduce wrong detections and restorations.\n",
        "      # align and warp each face\n",
        "      faces.align_warp_face()\n",
        "\n",
        "  # face restoration\n",
        "  for cropped_face in faces.cropped_faces:\n",
        "      # prepare data\n",
        "      cropped_face_t = img2tensor(cropped_face / 255., bgr2rgb=True, float32=True)\n",
        "      normalize(cropped_face_t, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5), inplace=True)\n",
        "      cropped_face_t = cropped_face_t.unsqueeze(0).to(cudevg)\n\n",
        "      output = doenh(cropped_face_t)\n",
        "      restored_face = tensor2img(output[0].cpu(), rgb2bgr=True, min_max=(-1, 1))\n",
        "  \n",
        "\n",
        "      restored_face = restored_face\n",
        "      faces.add_restored_face(restored_face)\n",
        "\n",
        "  if not has_aligned and paste_back:\n",
        "      # upsample the background\n",
        "      \n",
        "\n",
        "      faces.get_inverse_affine(None)\n",
        "      # paste each restored face to the input image\n",
        "      restored_img = faces.paste_faces_to_input_image()\n",
        "      return faces, restored_img\n",
        "  else:\n",
        "      return faces, None\n",
        "\n",
        "\n",
        "\n",
        "if not os.path.isfile('retinaface_pnnx.pt'):\n",
        "  !wget https://huggingface.co/Larvik/GFPGANjit/resolve/main/face_parse_pnnx.pt\n",
        "  !wget https://huggingface.co/Larvik/GFPGANjit/resolve/main/gfpgan_dec_pnnx.pt\n",
        "  !wget https://huggingface.co/Larvik/GFPGANjit/resolve/main/gfpgan_enc_pnnx.pt\n",
        "  !wget https://huggingface.co/Larvik/GFPGANjit/resolve/main/retinaface_pnnx.pt\n",
        "\n",
        "GFPgan_device='cpu' #@param ['cpu', 'cuda']\n",
        "cudevg=torch.device(GFPgan_device)\n",
        "\n",
        "\n",
        "gfpgan_enc=None\n",
        "RetinaFace =torch.jit.load('retinaface_pnnx.pt').eval().to(cudevg)\n",
        "face_parse =torch.jit.load('face_parse_pnnx.pt').eval().to(cudevg)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "yKhdSfpTmsiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input='/content/aaa2_4x.png' #@param {type:'string'}\n",
        "output='results'\n",
        "\n",
        "upscale=4\n",
        "suffix=None\n",
        "only_center_face=False\n",
        "aligned=False\n",
        "ext='auto'\n",
        "\n",
        "\n",
        "# ------------------------ input & output ------------------------\n",
        "if input.endswith('/'):\n",
        "    input = input[:-1]\n",
        "if os.path.isfile(input):\n",
        "    img_list = [input]\n",
        "else:\n",
        "    img_list = sorted(glob.glob(os.path.join(input, '*')))\n",
        "\n",
        "os.makedirs(output, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------ restore ------------------------\n",
        "for img_path in img_list:\n",
        "    # read image\n",
        "    img_name = os.path.basename(img_path)\n",
        "    print(f'Processing {img_name} ...')\n",
        "    basename, ext = os.path.splitext(img_name)\n",
        "    input_img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "\n",
        "    # restore faces and background if necessary\n",
        "    faces, restored_img = enhance(input_img, has_aligned=aligned, only_center_face=only_center_face, paste_back=True)\n",
        "\n",
        "    # save faces\n",
        "    for idx, (cropped_face, restored_face) in enumerate(zip(faces.cropped_faces, faces.restored_faces)):\n",
        "        # save cropped face\n",
        "        save_crop_path = os.path.join(output, 'cropped_faces', f'{basename}_{idx:02d}.png')\n",
        "        imwrite(cropped_face, save_crop_path)\n",
        "        # save restored face\n",
        "        if suffix is not None:\n",
        "            save_face_name = f'{basename}_{idx:02d}_{suffix}.png'\n",
        "        else:\n",
        "            save_face_name = f'{basename}_{idx:02d}.png'\n",
        "        save_restore_path = os.path.join(output, 'restored_faces', save_face_name)\n",
        "        imwrite(restored_face, save_restore_path)\n",
        "        # save comparison image\n",
        "        cmp_img = np.concatenate((cropped_face, restored_face), axis=1)\n",
        "        imwrite(cmp_img, os.path.join(output, 'cmp', f'{basename}_{idx:02d}.png'))\n",
        "\n",
        "    # save restored img\n",
        "    if restored_img is not None:\n",
        "        if ext == 'auto':\n",
        "            extension = ext[1:]\n",
        "        else:\n",
        "            extension = ext\n",
        "\n",
        "        if suffix is not None:\n",
        "            save_restore_path = os.path.join(output, 'restored_imgs', f'{basename}_{suffix}.png')\n",
        "        else:\n",
        "            save_restore_path = os.path.join(output, 'restored_imgs', f'{basename}.png')\n",
        "        imwrite(restored_img, save_restore_path)\n",
        "\n",
        "print(f'Results are in the [{output}] folder.')\n"
      ],
      "metadata": {
        "id": "OOqZ0K2Rm_G4",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# txt2img"
      ],
      "metadata": {
        "id": "PWCzxzNkYpEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SDver='470k' #@param ['440k', '470k']\n",
        "Dfm='Orig' #@param ['Orig', '_imgemb','_a19561','_a17750','_a17750_e5750']\n",
        "if Dfm=='Orig':\n",
        "  Dfm=''\n",
        "\n",
        "import os\n",
        "\n",
        "f_dljit(SDver,Dfm)\n",
        "\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import PIL\n",
        "from transformers import CLIPTokenizer\n",
        "\n",
        "import torch\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "torch.set_num_threads(os.cpu_count())\n",
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True\n",
        "\n",
        "alphas_cumprod = np.load('alphas_cumprod.npz')['a']\n",
        "\n",
        "cudev=torch.device('cuda')\n",
        "\n",
        "\n",
        "\n",
        "inzdict=[\n",
        "'*',265,\n",
        "'»',7599,\n",
        "'¿',17133,\n",
        "'¥',20199,\n",
        "'®',8436]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "preimg=None\n",
        "revpreimg=None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ddpm\n",
        "def apply_model(x, t, cond):\n",
        "\n",
        "    h, emb, hs = diffusion_emb(x, t, cond)\n",
        "    \n",
        "    h = diffusion_mid(h, emb, cond, *hs[6:])\n",
        "\n",
        "    output = diffusion_out(h, emb, cond, *hs[:6])\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "# decoder\n",
        "def decode_first_stage(z):\n",
        "\n",
        "    output = autoencoder(z/0.18215)\n",
        "       \n",
        "    return output\n",
        "\n",
        "\n",
        "    \n",
        "def load_img(path):\n",
        "    image = Image.open(path).convert(\"RGB\")\n",
        "    w, h = image.size\n",
        "    print(f\"loaded input image of size ({w}, {h}) from {path}\")\n",
        "    w2, h2 = map(lambda x: x - x % 32, (w, h))\n",
        "    if w!=w2 or h!=h2:\n",
        "      image = image.resize((w2, h2), resample=PIL.Image.LANCZOS)\n",
        "    image = np.array(image).astype(np.float32) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    return 2.*image - 1.\n",
        "\n",
        "depthLimit=10\n",
        "\n",
        "def txtErr(prmt0,msg):\n",
        "  print(msg)\n",
        "  prmt=prmt0.split('/')[-1][:-4]\n",
        "  print('err prompt: '+prmt)\n",
        "  return [cond_stage_model.encode(prmt, n_samples)]\n",
        "\n",
        "def intptxtemb(stz):\n",
        "  prmpl=len(stz)>>1\n",
        "  ptxt=[]\n",
        "  pstp=[]\n",
        "  for i in range(prmpl):\n",
        "    ptxt.append(  makeCs(stz[2*i],1)[0]  )\n",
        "    pstp.append(  int(stz[2*i+1])+1  )\n",
        "  prmpl-=1\n",
        "  intpos=[]\n",
        "  for vv in range(prmpl):\n",
        "    c1=ptxt[vv]\n",
        "    c2=ptxt[vv+1]\n",
        "    stp=pstp[vv]\n",
        "    for i in range(stp):\n",
        "      intpos.append((c2*i+c1*(stp-i))/stp)\n",
        "\n",
        "  lztbk=pstp[-1]\n",
        "  if lztbk > 1:\n",
        "    c1=ptxt[prmpl]\n",
        "    c2=ptxt[0]\n",
        "    for i in range(lztbk):\n",
        "      intpos.append((c2*i+c1*(lztbk-i))/lztbk)\n",
        "  else:\n",
        "    intpos.append(ptxt[-1])\n",
        "  return intpos\n",
        "\n",
        "def mkcondlist(stz):\n",
        "  prmpl=len(stz)>>1\n",
        "  ptxt=[]\n",
        "  pstp=[]\n",
        "  stpsum=0\n",
        "  for i in range(prmpl):\n",
        "    ptxt.append(  makeCs(stz[2*i],1)[0]  )\n",
        "    soi=int(stz[2*i+1])\n",
        "    stpsum+=soi\n",
        "    pstp.append(  soi  )\n",
        "  retar=[0]*stpsum\n",
        "  stpsum=0\n",
        "  k=0\n",
        "  for stp in pstp:\n",
        "    pmt=ptxt[k]\n",
        "    for aaa in range(stp):\n",
        "      retar[stpsum]=pmt\n",
        "      stpsum+=1\n",
        "    k+=1\n",
        "  return [retar]\n",
        "\n",
        "\n",
        "\n",
        "def encodepatt():\n",
        "  ozi=output_pattern.split('/')[-1]\n",
        "  pdir=output_pattern[:-len(ozi)-1]\n",
        "  flist=os.listdir(pdir)\n",
        "  flist.sort()\n",
        "  pdir+='/'\n",
        "  rpt=load_img(pdir+flist[0])\n",
        "  vB=1\n",
        "  vH=rpt.size(2)\n",
        "  vW=rpt.size(3)\n",
        "  thsize=torch.Size([vB,4,vH>>3,vW>>3])\n",
        "  noyaz=torch.randn(thsize)\n",
        "  zadd=0\n",
        "  for f in flist:\n",
        "    if f.endswith('.png'):\n",
        "      vlat=imgenc(  load_img( pdir+f) ,  noyaz )*0.18215\n",
        "      vlat.numpy().tofile(pdir+f[:-3]+'bin')\n",
        "      zadd+=1\n",
        "  with open(output_pattern[:-3].replace('%','!@!')+'txt','wt') as f:\n",
        "    f.write(str(list(thsize))[1:-1]+'\\n'+str(zadd))\n",
        "  !rm {pdir}*.png\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def makeCs(prmt,depth):\n",
        "  global get_cond\n",
        "  global ddim_num_steps\n",
        "  if prmt.endswith('.txt'):\n",
        "    if depth > depthLimit:\n",
        "      return txtErr(prmt,'Too many ref, probably circular reference.')\n",
        "    depth+=1\n",
        "    if not os.path.isfile(prmt):\n",
        "      return txtErr(prmt,'ref not found.')\n",
        "    with open(prmt,'rt') as f:\n",
        "      stz=f.read().splitlines()\n",
        "    cmd=stz[0].replace(' ','').replace('\\t','').split('/')\n",
        "    cmd0=cmd[0]\n",
        "    if cmd0.startswith('intp'):\n",
        "      if depth > 1:\n",
        "        return txtErr(stz[1],'do not intp in ref')\n",
        "      return intptxtemb(stz[1:])\n",
        "    elif cmd0.startswith('dymc'):\n",
        "      if depth > 1:\n",
        "        return txtErr(stz[1],'do not dymc in ref')\n",
        "      get_cond=get_cond_list\n",
        "      ddim_num_steps=-1\n",
        "      return mkcondlist(stz[1:])\n",
        "\n",
        "\n",
        "    prmpl=(len(stz)-1)>>1\n",
        "    stz=stz[1:]\n",
        "    ptxt=[]\n",
        "    pwgt=[]\n",
        "    wgtsum=0\n",
        "    for i in range(prmpl):\n",
        "      ptxt.append(  makeCs(stz[2*i],depth)[0]  )\n",
        "      wgt=float(stz[2*i+1])\n",
        "      wgtsum+=wgt\n",
        "      pwgt.append(  wgt  )\n",
        "    if cmd0.startswith('avg'):\n",
        "      for i in range(prmpl):\n",
        "        pwgt[i]=pwgt[i]/wgtsum\n",
        "    \n",
        "    cout=ptxt[0]*pwgt[0]\n",
        "    for i in range(1,prmpl):\n",
        "      cout+=(ptxt[i]*pwgt[i])\n",
        "    return [cout]\n",
        "  elif prmt.endswith('.bin'):\n",
        "    return [torch.tensor(np.fromfile(prmt,dtype=np.float32)).reshape((-1,768)).broadcast_to(n_samples,77,768).cuda()]\n",
        "  else:\n",
        "    return [cond_stage_model.encode(prmt, n_samples)]\n",
        "\n",
        "\n",
        "def warmup():\n",
        "  v_0 = torch.rand(2, 4, 32, 32, dtype=torch.float).half().cuda()\n",
        "  v_1 = torch.randint(10, (2, ), dtype=torch.long).cuda()\n",
        "  v_2 = torch.rand(2, 77, 768, dtype=torch.float).half().cuda()\n",
        "  for d in range(2):\n",
        "    with torch.cuda.amp.autocast(dtype=torch.float16):\n",
        "      uaa = apply_model(v_0,v_1,v_2)\n",
        "  v_0 = torch.rand(1, 4, 32, 32, dtype=torch.float).cuda()\n",
        "  for d in range(2):\n",
        "    uaa = autoencoder(v_0)\n",
        "  torch.cuda.empty_cache()\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "fext='_%dx%dv%d.png'\n",
        "def saver():\n",
        "  global x_samples\n",
        "  i=iita\n",
        "  np.save( (outputp+fext%(i,1,ktta))[:-4] + '.npy', samples)\n",
        "  x_samples = np.clip((x_samples.numpy() + 1.0) / 2.0, a_min=0.0, a_max=1.0)\n",
        "  k=0\n",
        "  for x_sample in x_samples:\n",
        "      x_sample = x_sample.transpose(1, 2, 0)  # CHW -> HWC\n",
        "      x_sample = x_sample * 255\n",
        "      img = x_sample.astype(np.uint8)\n",
        "      img = img[:, :, ::-1]  # RGB -> BGR\n",
        "      cv2.imwrite(outputp+fext%(i,k,ktta), img)\n",
        "      k+=1\n",
        "  \n",
        "\n",
        "UseSamplr=sample_lms\n",
        "\n",
        "def predict(prompt, uc):\n",
        "    global x_samples\n",
        "    global samples\n",
        "    global ktta\n",
        "    global tmpfeeder\n",
        "    global noise\n",
        "    global ddim_num_steps\n",
        "    global get_cond\n",
        "    get_cond=get_cond_simp\n",
        "\n",
        "    c_list = makeCs(prompt,0)\n",
        "\n",
        "    if ddim_num_steps < 0:\n",
        "      ddim_num_steps=len(c_list[0])\n",
        "\n",
        "    feeder=ifeeder()\n",
        "    \n",
        "    sigmas = f_sigmas()\n",
        "\n",
        "    noise = torch.randn(shape, dtype=torch.float,device=cudev)\n",
        "    if preimg is not None:\n",
        "      t_enc= int(strength * ddim_num_steps)\n",
        "      sigma_sched = sigmas[ddim_num_steps - t_enc - 1:]\n",
        "      if preimg.dim()==1:\n",
        "        cmd0=int(preimg[0])\n",
        "        if cmd0 == 2:\n",
        "          feeder.pattern=tmpfeeder.pattern\n",
        "          feeder.shape=tmpfeeder.shape\n",
        "          feeder.getn=feeder.get_npbins\n",
        "          feeder.noiseadd=noise * sigmas[ddim_num_steps - t_enc - 1]\n",
        "          c_list=[c_list[0]]*tmpfeeder.xpenlen\n",
        "          feeder.xpenlen=tmpfeeder.xpenlen\n",
        "          tmpfeeder=feeder\n",
        "      else:\n",
        "        img = preimg.cuda() + noise * sigmas[ddim_num_steps - t_enc - 1]\n",
        "        feeder.setbs(img)\n",
        "    else:\n",
        "      img = noise*sigmas[0]\n",
        "      sigma_sched=sigmas\n",
        "      feeder.setbs(img)\n",
        "\n",
        "\n",
        "    ktta=0\n",
        "    for c in c_list:\n",
        "      extra_args = {'cond': c, 'uncond': uc, 'cond_scale': cfg_scale}\n",
        "      with torch.cuda.amp.autocast(dtype=torch.float16):\n",
        "        samples = UseSamplr(model_wrap_cfg, feeder.getn(ktta), sigma_sched, extra_args=extra_args, disable=False)\n",
        "      ktta+=1\n",
        "      \n",
        "      \n",
        "      x_samples = decode_first_stage(  samples ).cpu()\n",
        "      samples=samples.cpu()\n",
        "      t3 = Thread(target = saver)\n",
        "      a3 = t3.start()\n",
        "\n",
        "    return\n",
        "\n",
        "predict_orig=predict\n",
        "\n",
        "def init_img_type():\n",
        "  global init_img\n",
        "  global tmpfeeder\n",
        "  if init_img.endswith('.npy'):\n",
        "    return 0\n",
        "  elif init_img.endswith('.jpg') or init_img.endswith('.png'):\n",
        "    if os.path.isfile(init_img+'.npy'):\n",
        "      init_img+='.npy'\n",
        "      return 0\n",
        "    else:\n",
        "      return 1\n",
        "  elif init_img.endswith('.txt'):\n",
        "    return 2\n",
        "  else:\n",
        "    return 99\n",
        "\n",
        "\n",
        "if model_wrap is None:\n",
        "  cond_stage_model = BERTEmbedder(torch.jit.load('transformer_pnnx.pt').eval())\n",
        "  diffusion_emb = torch.jit.load('diffusion_emb_pnnx.pt').eval().half().cuda()\n",
        "  diffusion_mid = torch.jit.load('diffusion_mid_pnnx.pt').eval().half().cuda()\n",
        "  diffusion_out = torch.jit.load('diffusion_out_pnnx.pt').eval().half().cuda()\n",
        "  autoencoder = torch.jit.load('autoencoder_pnnx.pt').eval().cuda()\n",
        "  SDlatDEC=autoencoder\n",
        "  imgenc = torch.jit.load('imgencoder_pnnx.pt').eval()\n",
        "  warmup()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  model_wrap = CompVisDenoiser(CompVisJIT())\n",
        "  model_wrap_cfg = CFGDenoiser(model_wrap)"
      ],
      "metadata": {
        "id": "fmYgI8PLYudK",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "👇Optional👇"
      ],
      "metadata": {
        "id": "E4dG4ZxdW4y-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Sampler='euler_a' #@param ['euler', 'euler_a', 'heun','dpm_2','dpm_2_a','lms']\n",
        "f_sampler()\n",
        "\n",
        "Karras=False #@param {type:'boolean'}\n",
        "KarrasRho = 7.0 #@param {type:'number'}"
      ],
      "metadata": {
        "id": "fQlZQgl7uxkx",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cond_stage_model.insert('<>')\n",
        "cond_stage_model.insert('<majipuri>')\n",
        "cond_stage_model.insert('<pekora>')"
      ],
      "metadata": {
        "id": "x0KSuj8yEZmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_img='xxx' #@param {type:'string'}\n",
        "strength=0.5 #@param {type:'number'}\n",
        "initymgtyp=init_img_type()\n",
        "if initymgtyp == 0:\n",
        "  preimg=torch.tensor(np.load(init_img), device='cpu')\n",
        "  n_samples=preimg.size(0)\n",
        "  H=preimg.size(2)<<3\n",
        "  W=preimg.size(3)<<3\n",
        "elif initymgtyp == 1:\n",
        "  n_samples=1\n",
        "  rpt=load_img(init_img)\n",
        "  H=rpt.size(2)\n",
        "  W=rpt.size(3)\n",
        "  preimg=imgenc(  rpt, torch.randn(torch.Size([n_samples,4,H>>3,W>>3]))  )*0.18215\n",
        "  np.save(init_img+'.npy',preimg.numpy())\n",
        "elif initymgtyp == 2:\n",
        "  tmpfeeder=ifeeder()\n",
        "  tmpfeeder.pattern=init_img[:-3].replace('!@!','%')+'bin'\n",
        "  with open(init_img,'rt') as f:\n",
        "    stz=f.read().replace(' ','').replace('\\t','').splitlines()\n",
        "  tmpfeeder.xpenlen=int(stz[1])\n",
        "  stz=stz[0].split(',')\n",
        "  tmpfeeder.shape=[ int(stz[0]), int(stz[1]), int(stz[2]), int(stz[3]) ]\n",
        "  n_samples=tmpfeeder.shape[0]\n",
        "  H=tmpfeeder.shape[2]<<3\n",
        "  W=tmpfeeder.shape[3]<<3\n",
        "  preimg=torch.tensor([2])\n",
        "else:\n",
        "  revpre=revpre0\n",
        "  preimg=None\n",
        "revpreimg=None"
      ],
      "metadata": {
        "id": "I1FtIJmB2Ojd",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "infilling"
      ],
      "metadata": {
        "id": "G2ANdLCfXTj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FillFromNoise=False #@param {type:'boolean'}\n",
        "masknpy='bench2_mask.npy' #@param {type:'string'}\n",
        "zamask=torch.tensor(np.load(masknpy)).cuda()\n",
        "revpreimg=preimg.cuda()\n",
        "if FillFromNoise:\n",
        "  preimg=None\n",
        "revpre=revpre1"
      ],
      "metadata": {
        "id": "Rkrl24HWW31Q",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt interpolation with latent re-feeding"
      ],
      "metadata": {
        "id": "B6VrT6bw0tUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Revert2Orig=False #@param {type:'boolean'}\n",
        "\n",
        "\n",
        "def predict(prompt, uc):\n",
        "    global x_samples\n",
        "    global samples\n",
        "    global ktta\n",
        "    global tmpfeeder\n",
        "    global noise\n",
        "    preimg=None\n",
        "    \n",
        "\n",
        "    c_list = makeCs(prompt,0)\n",
        "    feeder=ifeeder()\n",
        "\n",
        "    \n",
        "    \n",
        "    sigmas = f_sigmas()\n",
        "\n",
        "    noise = torch.randn(shape, dtype=torch.float,device=cudev)\n",
        "\n",
        "    ktta=0\n",
        "    for c in c_list:\n",
        "      \n",
        "      if preimg is not None:\n",
        "        noise=torch.permute(noise, (0,3,1,2)).reshape(noise.shape)\n",
        "        t_enc= int(strength * ddim_num_steps)\n",
        "        sigma_sched = sigmas[ddim_num_steps - t_enc - 1:]\n",
        "        img = preimg.cuda() +  noise* sigmas[ddim_num_steps - t_enc - 1]\n",
        "        feeder.setbs(img)        \n",
        "      else:\n",
        "        img = noise*sigmas[0]\n",
        "        sigma_sched=sigmas\n",
        "        feeder.setbs(img)\n",
        "\n",
        "\n",
        "\n",
        "      \n",
        "      \n",
        "      extra_args = {'cond': c, 'uncond': uc, 'cond_scale': cfg_scale}\n",
        "      with torch.cuda.amp.autocast(dtype=torch.float16):\n",
        "        samples = UseSamplr(model_wrap_cfg, feeder.getn(ktta), sigma_sched, extra_args=extra_args, disable=False)\n",
        "    \n",
        "      ktta+=1\n",
        "      preimg=samples\n",
        "      x_samples = decode_first_stage( samples ).cpu()\n",
        "      samples=samples.cpu()\n",
        "      \n",
        "      t3 = Thread(target = saver)\n",
        "      a3 = t3.start()\n",
        "    \n",
        "    return\n",
        "if Revert2Orig:\n",
        "  predict=predict_orig\n",
        "else:\n",
        "  strength=0.75 #@param {type:'number'}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Pgy-4fQn0sT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NoiseMap interpolation<br>re-feed previous when strength > 0"
      ],
      "metadata": {
        "id": "Ht69eCNYt5VW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Revert2Orig=False #@param {type:'boolean'}\n",
        "\n",
        "\n",
        "def mknoises():\n",
        "  sil=len(Seed_Interval_list)>>1\n",
        "  nolist=[]\n",
        "  for n in range(sil):\n",
        "    zeed=Seed_Interval_list[n*2]\n",
        "    if zeed < 1:\n",
        "      zeed=random.randint(0, 2**32)\n",
        "      print('seed%d='%n)\n",
        "      print(zeed)\n",
        "    torch.manual_seed(zeed)\n",
        "    nolist.append( torch.randn(shape, dtype=torch.float,device=cudev) )\n",
        "  nolist.append(nolist[0])\n",
        "  interpos=[]\n",
        "  DOT_THRESHOLD=0.9995\n",
        "  for n in range(sil):\n",
        "    stp=Seed_Interval_list[n*2+1]+1\n",
        "    v0=nolist[n]\n",
        "    v1=nolist[n+1]\n",
        "    dot = torch.sum(v0 * v1 / (torch.linalg.norm(v0) * torch.linalg.norm(v1)))\n",
        "    if torch.abs(dot) > DOT_THRESHOLD:\n",
        "      for j in range(stp):\n",
        "        t=j/stp\n",
        "        interpos.append( (1 - t) * v0 + t * v1 )\n",
        "    else:\n",
        "      theta_0 = torch.acos(dot)\n",
        "      sin_theta_0 = torch.sin(theta_0)\n",
        "      for j in range(stp):\n",
        "        t=j/stp\n",
        "        theta_t = theta_0 * t\n",
        "        sin_theta_t = torch.sin(theta_t)\n",
        "        s0 = torch.sin(theta_0 - theta_t) / sin_theta_0\n",
        "        s1 = sin_theta_t / sin_theta_0\n",
        "        interpos.append( s0 * v0 + s1 * v1 )\n",
        "\n",
        "  return interpos\n",
        "  \n",
        "\n",
        "\n",
        "def predict(prompt, uc):\n",
        "    global x_samples\n",
        "    global samples\n",
        "    global ktta\n",
        "    global tmpfeeder\n",
        "    global noise\n",
        "    preimg=None\n",
        "    \n",
        "\n",
        "    c_list = makeCs(prompt,0)\n",
        "    feeder=ifeeder()\n",
        "\n",
        "    \n",
        "    \n",
        "    sigmas = f_sigmas()\n",
        "\n",
        "    noise = mknoises()\n",
        "    c_list=c_list*len(noise)\n",
        "\n",
        "    ktta=0\n",
        "    for c in c_list:\n",
        "      \n",
        "      if preimg is not None:\n",
        "        t_enc= int(strength * ddim_num_steps)\n",
        "        sigma_sched = sigmas[ddim_num_steps - t_enc - 1:]\n",
        "        img = preimg.cuda() +  noise[ktta]* sigmas[ddim_num_steps - t_enc - 1]\n",
        "        feeder.setbs(img)        \n",
        "      else:\n",
        "        img = noise[ktta]*sigmas[0]\n",
        "        sigma_sched=sigmas\n",
        "        feeder.setbs(img)\n",
        "\n",
        "\n",
        "\n",
        "      \n",
        "      \n",
        "      extra_args = {'cond': c, 'uncond': uc, 'cond_scale': cfg_scale}\n",
        "      with torch.cuda.amp.autocast(dtype=torch.float16):\n",
        "        samples = UseSamplr(model_wrap_cfg, feeder.getn(ktta), sigma_sched, extra_args=extra_args, disable=False)\n",
        "    \n",
        "      ktta+=1\n",
        "      if strength > 0:\n",
        "        preimg=samples*(1-strength)\n",
        "      x_samples = decode_first_stage( samples ).cpu()\n",
        "      samples=samples.cpu()\n",
        "      \n",
        "      t3 = Thread(target = saver)\n",
        "      a3 = t3.start()\n",
        "    \n",
        "    return\n",
        "if Revert2Orig:\n",
        "  predict=predict_orig\n",
        "else:\n",
        "  strength=0 #@param {type:'number'}\n",
        "  Seed_Interval_list=[    775577,10,    881188,10,    996699,10    ] #@param {type:'raw'}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "JNB7IAIMt_YU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "☝️Optional☝️"
      ],
      "metadata": {
        "id": "QunwpFSGXKz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "InThread=False #@param {type:'boolean'}\n",
        "\n",
        "prompt = 'a photograph of an astronaut riding a horse' #@param {type:'string'}\n",
        "neg_prompt = '' #@param {type:'string'}\n",
        "\n",
        "n_iter = 1 #@param {type:'integer'}\n",
        "if preimg is None and revpreimg is None:\n",
        "  n_samples = 1 #@param {type:'integer'}\n",
        "  H=704 #@param {type:'integer'}\n",
        "  W=768 #@param {type:'integer'}\n",
        "\n",
        "\n",
        "\n",
        "ddim_num_steps = 50  #@param {type:'integer'}\n",
        "ddpm_num_timesteps = 1000\n",
        "\n",
        "seed=0 #@param {type:'integer'}\n",
        "\n",
        "outputp='/content/sample_data' #@param {type:'string'}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "cfg_scale = 7.5 #@param {type:'number'}\n",
        "ddim_eta = 1.0  #@param {type:'number'}\n",
        "\n",
        "\n",
        "outputp=outputp+'/'+str(len(os.listdir(outputp)))\n",
        "\"\"\"\n",
        "ddim_timesteps\n",
        "\"\"\"\n",
        "\n",
        "ddim_timesteps = make_ddim_timesteps(\n",
        "    ddim_num_steps, ddpm_num_timesteps)\n",
        "\n",
        "\"\"\"\n",
        "ddim sampling parameters\n",
        "\"\"\"\n",
        "\n",
        "ddim_sigmas, ddim_alphas, ddim_alphas_prev = \\\n",
        "    make_ddim_sampling_parameters(\n",
        "        alphacums=alphas_cumprod,\n",
        "        ddim_timesteps=ddim_timesteps,\n",
        "        eta=ddim_eta)\n",
        "\n",
        "ddim_sqrt_one_minus_alphas = np.sqrt(1. - ddim_alphas)\n",
        "\n",
        "shape = [n_samples, 4, H>>3 , W>>3 ]\n",
        "\n",
        "\n",
        "makerng()\n",
        "\n",
        "\n",
        "print(\"prompt: %s\" % prompt)\n",
        "\n",
        "\n",
        "\n",
        "print('Start inference...')\n",
        "uc = None\n",
        "if cfg_scale != 1.0:\n",
        "  uc = cond_stage_model.encode(neg_prompt, n_samples)\n",
        "\n",
        "\n",
        "  \n",
        "def wpa():\n",
        "  global iita\n",
        "  torch.set_grad_enabled(False)\n",
        "  \n",
        "  for iita in range(n_iter):\n",
        "      print(\"iteration: %s\" % (iita + 1))\n",
        "      predict(prompt, uc)\n",
        "      \n",
        "  print('Script finished successfully.')\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "if InThread:\n",
        "  t1 = Thread(target = wpa)\n",
        "  a1 = t1.start()\n",
        "else:\n",
        "  wpa()\n"
      ],
      "metadata": {
        "id": "V5xaKk_uYwBK",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "pH0ExQ__p8Cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ffmpeg -framerate 3 -i /content/sample_data/48_0x3v%d.png intp03.mp4"
      ],
      "metadata": {
        "id": "Yby8cQ9HqkuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tools\n",
        "designed for the gen proc running with `InThread` or gradio app<br>\n",
        "so imgenc (image->latent encoder) is on cpu"
      ],
      "metadata": {
        "id": "vFqWsqyVNB2A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gif/Video to latent pack"
      ],
      "metadata": {
        "id": "_9d7uJg9NEPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_anim  = '/content/senpai.gif' #@param {type:'string'}\n",
        "output_pattern = '/content/ijj/senpai_%04d.png' #@param {type:'string'}\n",
        "!ffmpeg -i {input_anim} {output_pattern}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "MmRwF523NIZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resize the output to `(64*n)x(64*m)` first"
      ],
      "metadata": {
        "id": "GLrbLP9cN8oh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encodepatt()"
      ],
      "metadata": {
        "id": "ckwYthEQOJSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ImageEmbSetup  = False #@param {type:'boolean'}\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "def load_im(im_path):\n",
        "    if im_path.startswith(\"http\"):\n",
        "        response = requests.get(im_path)\n",
        "        response.raise_for_status()\n",
        "        im = Image.open(BytesIO(response.content))\n",
        "    else:\n",
        "        im = Image.open(im_path).convert(\"RGB\")\n",
        "    tforms = transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.CenterCrop((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    inp = tforms(im).unsqueeze(0)\n",
        "    return inp*2-1\n",
        "if not os.path.isfile('imgemb.pt'):\n",
        "  !wget https://huggingface.co/Larvik/imgemb/resolve/main/imgemb.pt\n",
        "imgemb=torch.jit.load('imgemb.pt').float()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GxQjN-yyggf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgemb(load_im('/content/chaz512.jpg')).numpy().tofile('chaz.bin')"
      ],
      "metadata": {
        "id": "qk9SKnUkgifF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio Gui\n",
        "tho I don't really understand why you want a webui inside another webui"
      ],
      "metadata": {
        "id": "bJsLMq9pplwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gradio=False #@param {type:'boolean'}\n",
        "\n",
        "!pip install gradio\n",
        "from google.colab import output\n",
        "import gradio as gr\n",
        "\n",
        "def dream():\n",
        "  return\n",
        "\n",
        "\n",
        "dream_interface = gr.Interface(\n",
        "    dream,\n",
        "    inputs=[\n",
        "        gr.Textbox(placeholder=\"A corgi wearing a top hat as an oil painting.\", lines=1),\n",
        "        gr.Slider(minimum=1, maximum=150, step=1, label=\"Sampling Steps\", value=50),\n",
        "        gr.Checkbox(label='Enable PLMS sampling', value=False),\n",
        "        gr.Checkbox(label='Enable Fixed Code sampling', value=False),\n",
        "        gr.Slider(minimum=0.0, maximum=1.0, step=0.01, label=\"DDIM ETA\", value=0.0, visible=False),\n",
        "        gr.Slider(minimum=1, maximum=50, step=1, label='Sampling iterations', value=8),\n",
        "        gr.Slider(minimum=1, maximum=8, step=1, label='Samples per iteration', value=1),\n",
        "        gr.Slider(minimum=1.0, maximum=20.0, step=0.5, label='Classifier Free Guidance Scale', value=7.0),\n",
        "        gr.Number(label='Seed', value=-1),\n",
        "        gr.Slider(minimum=64, maximum=2048, step=64, label=\"Height\", value=704),\n",
        "        gr.Slider(minimum=64, maximum=2048, step=64, label=\"Width\", value=768),\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Gallery(),\n",
        "        gr.Number(label='Seed')\n",
        "    ],\n",
        "    title=\"Stable Diffusion Text-to-Image\",\n",
        "    description=\"Generate images from text with Stable Diffusion\",\n",
        ")\n",
        "\n",
        "\n",
        "gdemo = gr.TabbedInterface(interface_list=[dream_interface], tab_names=[\"Dream\"])\n",
        "\n",
        "\n",
        "output.serve_kernel_port_as_window(8233, path='/dl.htm')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mh-_HQ1jquy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy the link above to `GoogleLocal`"
      ],
      "metadata": {
        "id": "ePW4zpPPsVTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GoogleLocal = 'aaaaa' #@param {type:'string'}\n",
        "if '.googleusercontent.com' in GoogleLocal:\n",
        "  gdemo.launch()\n",
        "else:\n",
        "  print('set a valid GoogleLocal')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ndCPoghArlfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# glid-3-xl-stable"
      ],
      "metadata": {
        "id": "O6t9w7xdHv6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SDver='470k' #@param ['440k', '470k']\n",
        "Dfm='Orig' #@param ['Orig', '_imgemb','_a19561','_a17750','_a17750_e5750']\n",
        "if Dfm=='Orig':\n",
        "  Dfm=''\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "torch.set_num_threads(os.cpu_count())\n",
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True\n",
        "\n",
        "\n",
        "def get_keys_to_submodule(model):\n",
        "    keys_to_submodule = {}\n",
        "    # iterate all submodules\n",
        "    for submodule_name, submodule in model.named_modules():\n",
        "        # iterate all paramters in each submobule\n",
        "        for param_name, param in submodule.named_parameters():\n",
        "            # param_name is organized as <name>.<subname>.<subsubname> ...\n",
        "            splitted_param_name = param_name.split('.')\n",
        "            # we cannot go inside it anymore. This is the actual parameter\n",
        "            is_leaf_param = len(splitted_param_name) == 1\n",
        "            if is_leaf_param:\n",
        "                # we recreate the correct key\n",
        "                key = f\"{submodule_name}.{param_name}\"\n",
        "                # we associate this key with this submodule\n",
        "                keys_to_submodule[key] = submodule\n",
        "                \n",
        "    return keys_to_submodule\n",
        "\n",
        "def load_state_dict_with_low_memory(model, state_dict):\n",
        "    print('======hacky load======')\n",
        "    keys_to_submodule = get_keys_to_submodule(model)\n",
        "    mste=model.state_dict()\n",
        "    for key, submodule in keys_to_submodule.items():\n",
        "        # get the valye from the state_dict\n",
        "        if key in state_dict:\n",
        "          val = state_dict[key]\n",
        "        else:\n",
        "          print(key)\n",
        "          val = torch.ones(mste[key].shape, dtype= torch.float32)\n",
        "\n",
        "        param_name = key.split('.')[-1]\n",
        "        new_val = torch.nn.Parameter(val,requires_grad=False)\n",
        "        setattr(submodule, param_name, new_val)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class imgencdec:\n",
        "  def encode(self,im):\n",
        "    nzmp=im.size(0)\n",
        "    H=im.size(2)\n",
        "    W=im.size(3)\n",
        "    return imgenc(  im, torch.randn(torch.Size([nzmp,4,H>>3,W>>3]))  )\n",
        "  def decode(self,im):\n",
        "    return autoencoder(im)\n",
        "\n",
        "\n",
        "f_dljit(SDver,Dfm)\n",
        "\n",
        "if not os.path.isfile('/content/guided_diffusion/unet.py'):\n",
        "  !wget https://raw.githubusercontent.com/TabuaTambalam/DalleWebms/main/docs/sd/jkt.py\n",
        "  !git clone https://github.com/Jack000/glid-3-xl-stable.git\n",
        "  !mv /content/glid-3-xl-stable/guided_diffusion /content/guided_diffusion \n",
        "\n",
        "from transformers import CLIPTokenizer\n",
        "cond_stage_model = BERTEmbedder(torch.jit.load('transformer_pnnx.pt').eval())\n",
        "diffusion_emb = torch.jit.load('diffusion_emb_pnnx.pt').eval().cuda()\n",
        "diffusion_mid = torch.jit.load('diffusion_mid_pnnx.pt').eval().cuda()\n",
        "diffusion_out = torch.jit.load('diffusion_out_pnnx.pt').eval().cuda()\n",
        "autoencoder = torch.jit.load('autoencoder_pnnx.pt').eval().cuda()\n",
        "SDlatDEC=autoencoder\n",
        "imgenc = torch.jit.load('imgencoder_pnnx.pt').eval()"
      ],
      "metadata": {
        "id": "2cz08wFzH-8L",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import io\n",
        "import math\n",
        "import sys\n",
        "\n",
        "from PIL import Image, ImageOps\n",
        "import requests\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as TF\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from guided_diffusion.script_util import create_model_and_diffusion, model_and_diffusion_defaults\n",
        "\n",
        "\n",
        "from accelerate import init_empty_weights\n",
        "from einops import rearrange\n",
        "from math import log2, sqrt\n",
        "\n",
        "\n",
        "!mkdir output_npy\n",
        "!mkdir output\n",
        "\n",
        "def save_sample(i, sample, clip_score=False):\n",
        "    for k, image in enumerate(sample['pred_xstart'][:1]):\n",
        "        image /= 0.18215\n",
        "        im = image.unsqueeze(0)\n",
        "        out = ldm.decode(im)\n",
        "\n",
        "        npy_filename = f'output_npy/{i * batchsz + k:05}.npy'\n",
        "        with open(npy_filename, 'wb') as outfile:\n",
        "            np.save(outfile, image.detach().cpu().numpy())\n",
        "\n",
        "        out = TF.to_pil_image(out.squeeze(0).add(1).div(2).clamp(0, 1))\n",
        "\n",
        "        filename = f'output/{i * batchsz + k:05}.png'\n",
        "        out.save(filename)\n",
        "\n",
        "\n",
        "# Create a classifier-free guidance sampling function\n",
        "def model_fn(x_t, ts, **kwargs):\n",
        "    half = x_t[: len(x_t) // 2]\n",
        "    combined = torch.cat([half, half], dim=0)\n",
        "    model_out = model(combined, ts, **kwargs)\n",
        "    eps, rest = model_out[:, :3], model_out[:, 3:]\n",
        "    cond_eps, uncond_eps = torch.split(eps, len(eps) // 2, dim=0)\n",
        "    half_eps = uncond_eps + guidance_scale * (cond_eps - uncond_eps)\n",
        "    eps = torch.cat([half_eps, half_eps], dim=0)\n",
        "    return torch.cat([eps, rest], dim=1)\n",
        "\n",
        "device = torch.device('cuda:0')\n",
        "print('Using device:', device)\n",
        "\n",
        "\n",
        "\n",
        "model_params = {\n",
        "    'attention_resolutions': '32,16,8',\n",
        "    'class_cond': False,\n",
        "    'diffusion_steps': 1000,\n",
        "    'rescale_timesteps': True,\n",
        "    'timestep_respacing': '50',  # Modify this value to decrease the number of\n",
        "                                 # timesteps.\n",
        "    'image_size': 32,\n",
        "    'learn_sigma': False,\n",
        "    'noise_schedule': 'linear',\n",
        "    'num_channels': 320,\n",
        "    'num_heads': 8,\n",
        "    'num_res_blocks': 2,\n",
        "    'resblock_updown': False,\n",
        "    'use_fp16': False,\n",
        "    'use_scale_shift_norm': False,\n",
        "    'clip_embed_dim': None, #768,\n",
        "    'image_condition': False,\n",
        "    #'image_condition': True if model_state_dict['input_blocks.0.0.weight'].shape[1] == 8 else False,\n",
        "    'super_res_condition': False,\n",
        "}\n",
        "\n",
        "model_params['timestep_respacing'] = '100'\n",
        "\n",
        "model_config = model_and_diffusion_defaults()\n",
        "model_config.update(model_params)\n",
        "\n",
        "\n",
        "model_config['use_fp16'] = True\n",
        "\n",
        "# Load models\n",
        "with init_empty_weights():\n",
        "  model, diffusion = create_model_and_diffusion(**model_config)\n",
        "\n",
        "load_state_dict_with_low_memory(model,mkmodel_state_dict())\n",
        "\n",
        "if model_config['use_fp16']:\n",
        "  model.convert_to_fp16()"
      ],
      "metadata": {
        "id": "Qpp_f7ZWIIWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.requires_grad_(False).eval().to(device)\n",
        "\n",
        "\n",
        "torch.manual_seed(114514)\n",
        "\n",
        "\n",
        "# vae\n",
        "\n",
        "ldm=imgencdec()\n",
        "\n",
        "\n",
        "guidance_scale=7\n",
        "height=832\n",
        "width=896\n",
        "batchsz=1\n",
        "\n",
        "\n",
        "args_text='thicc farm girl, long blonde hair, japanimation, by Alfons Maria Mucha, cinematic lightning, cinematic wallpaper'\n",
        "args_negative=''\n",
        "# clip context\n",
        "\n",
        "\n",
        "text_emb = cond_stage_model.encode(args_text,batchsz)\n",
        "text_emb_blank = cond_stage_model.encode(args_negative,batchsz)\n",
        "\n",
        "image_embed = None\n",
        "\n",
        "\n",
        "\n",
        "input_image = torch.zeros(batchsz, 4, height//8, width//8, device=device)\n",
        "'''\n",
        "lat=torch.tensor(np.load('96_4x1v1.npy'))\n",
        "\n",
        "\n",
        "input_image[0][:,:,:32]=lat[0][:,:,:32]\n",
        "'''\n",
        "\n",
        "      \n",
        "image_embed = None #torch.cat(batchsz*2*[input_image], dim=0).float()\n",
        "\n",
        "\n",
        "\n",
        "kwargs = {\n",
        "    \"context\": torch.cat([text_emb, text_emb_blank], dim=0).half().cuda(),\n",
        "    \"clip_embed\": None,\n",
        "    \"image_embed\": image_embed\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "cur_t = None\n",
        "\n",
        "sample_fn = diffusion.plms_sample_loop_progressive\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "init = Image.open('xipooh.jpg').convert('RGB')\n",
        "\n",
        "init = TF.to_tensor(init).to(device).unsqueeze(0).clamp(0,1)\n",
        "h = ldm.encode(init * 2 - 1) *  0.18215\n",
        "init = torch.cat(1*2*[h], dim=0)\n",
        "'''\n",
        "init=None\n",
        "\n",
        "for i in range(1):\n",
        "    cur_t = diffusion.num_timesteps - 1\n",
        "    with torch.cuda.amp.autocast(dtype=torch.float16):\n",
        "      samples = sample_fn(\n",
        "          model_fn,\n",
        "          (batchsz*2, 4, height>>3, width>>3),\n",
        "          clip_denoised=False,\n",
        "          model_kwargs=kwargs,\n",
        "          cond_fn=None,\n",
        "          device=device,\n",
        "          progress=True,\n",
        "          init_image=init,\n",
        "          skip_timesteps=0,\n",
        "      )\n",
        "\n",
        "    for j, sample in enumerate(samples):\n",
        "        cur_t -= 1\n",
        "\n",
        "    save_sample(i, sample)\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "62dodU08IN9i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}