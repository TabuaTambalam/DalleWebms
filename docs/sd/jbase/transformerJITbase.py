import torch
import torch.nn as nn
import torch.nn.functional as F
import jitbase

torch.set_grad_enabled(False)


class transformer(nn.Module):
    def __init__(self):
        super().__init__()

        self.cut=0
        self.text_model_embeddings_token_embedding = nn.Embedding(embedding_dim=768, num_embeddings=49408, sparse=False)
        self.text_model_encoder_layers_0_layer_norm1 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(768,))
        self.text_model_encoder_layers_0_self_attn_q_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_0_self_attn_k_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_0_self_attn_v_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_0_self_attn_out_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_0_layer_norm2 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(768,))
        self.text_model_encoder_layers_0_mlp_fc1 = nn.Linear(bias=True, in_features=768, out_features=3072)
        self.text_model_encoder_layers_0_mlp_fc2 = nn.Linear(bias=True, in_features=3072, out_features=768)
        self.text_model_encoder_layers_1_layer_norm1 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(768,))
        self.text_model_encoder_layers_1_self_attn_q_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_1_self_attn_k_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_1_self_attn_v_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_1_self_attn_out_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_1_layer_norm2 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(768,))
        self.text_model_encoder_layers_1_mlp_fc1 = nn.Linear(bias=True, in_features=768, out_features=3072)
        self.text_model_encoder_layers_1_mlp_fc2 = nn.Linear(bias=True, in_features=3072, out_features=768)
        self.text_model_encoder_layers_2_layer_norm1 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(768,))
        self.text_model_encoder_layers_2_self_attn_q_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_2_self_attn_k_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_2_self_attn_v_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_2_self_attn_out_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_2_layer_norm2 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(768,))
        self.text_model_encoder_layers_2_mlp_fc1 = nn.Linear(bias=True, in_features=768, out_features=3072)
        self.text_model_encoder_layers_2_mlp_fc2 = nn.Linear(bias=True, in_features=3072, out_features=768)
        self.text_model_encoder_layers_3_layer_norm1 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(768,))
        self.text_model_encoder_layers_3_self_attn_q_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_3_self_attn_k_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_3_self_attn_v_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_3_self_attn_out_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_3_layer_norm2 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(768,))
        self.text_model_encoder_layers_3_mlp_fc1 = nn.Linear(bias=True, in_features=768, out_features=3072)
        self.text_model_encoder_layers_3_mlp_fc2 = nn.Linear(bias=True, in_features=3072, out_features=768)
        self.text_model_encoder_layers_4_layer_norm1 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(768,))
        self.text_model_encoder_layers_4_self_attn_q_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_4_self_attn_k_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_4_self_attn_v_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_4_self_attn_out_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_4_layer_norm2 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(768,))
        self.text_model_encoder_layers_4_mlp_fc1 = nn.Linear(bias=True, in_features=768, out_features=3072)
        self.text_model_encoder_layers_4_mlp_fc2 = nn.Linear(bias=True, in_features=3072, out_features=768)
        self.text_model_encoder_layers_5_layer_norm1 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(768,))
        self.text_model_encoder_layers_5_self_attn_q_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_5_self_attn_k_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_5_self_attn_v_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_5_self_attn_out_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_5_layer_norm2 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(768,))
        self.text_model_encoder_layers_5_mlp_fc1 = nn.Linear(bias=True, in_features=768, out_features=3072)
        self.text_model_encoder_layers_5_mlp_fc2 = nn.Linear(bias=True, in_features=3072, out_features=768)
        self.text_model_encoder_layers_6_layer_norm1 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(768,))
        self.text_model_encoder_layers_6_self_attn_q_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_6_self_attn_k_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_6_self_attn_v_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_6_self_attn_out_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_6_layer_norm2 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(768,))
        self.text_model_encoder_layers_6_mlp_fc1 = nn.Linear(bias=True, in_features=768, out_features=3072)
        self.text_model_encoder_layers_6_mlp_fc2 = nn.Linear(bias=True, in_features=3072, out_features=768)
        self.text_model_encoder_layers_7_layer_norm1 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(768,))
        self.text_model_encoder_layers_7_self_attn_q_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_7_self_attn_k_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_7_self_attn_v_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_7_self_attn_out_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_7_layer_norm2 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(768,))
        self.text_model_encoder_layers_7_mlp_fc1 = nn.Linear(bias=True, in_features=768, out_features=3072)
        self.text_model_encoder_layers_7_mlp_fc2 = nn.Linear(bias=True, in_features=3072, out_features=768)
        self.text_model_encoder_layers_8_layer_norm1 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(768,))
        self.text_model_encoder_layers_8_self_attn_q_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_8_self_attn_k_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_8_self_attn_v_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_8_self_attn_out_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_8_layer_norm2 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(768,))
        self.text_model_encoder_layers_8_mlp_fc1 = nn.Linear(bias=True, in_features=768, out_features=3072)
        self.text_model_encoder_layers_8_mlp_fc2 = nn.Linear(bias=True, in_features=3072, out_features=768)
        self.text_model_encoder_layers_9_layer_norm1 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(768,))
        self.text_model_encoder_layers_9_self_attn_q_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_9_self_attn_k_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_9_self_attn_v_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_9_self_attn_out_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_9_layer_norm2 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(768,))
        self.text_model_encoder_layers_9_mlp_fc1 = nn.Linear(bias=True, in_features=768, out_features=3072)
        self.text_model_encoder_layers_9_mlp_fc2 = nn.Linear(bias=True, in_features=3072, out_features=768)
        self.text_model_encoder_layers_10_layer_norm1 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(768,))
        self.text_model_encoder_layers_10_self_attn_q_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_10_self_attn_k_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_10_self_attn_v_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_10_self_attn_out_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_10_layer_norm2 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(768,))
        self.text_model_encoder_layers_10_mlp_fc1 = nn.Linear(bias=True, in_features=768, out_features=3072)
        self.text_model_encoder_layers_10_mlp_fc2 = nn.Linear(bias=True, in_features=3072, out_features=768)
        self.text_model_encoder_layers_11_layer_norm1 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(768,))
        self.text_model_encoder_layers_11_self_attn_q_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_11_self_attn_k_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_11_self_attn_v_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_11_self_attn_out_proj = nn.Linear(bias=True, in_features=768, out_features=768)
        self.text_model_encoder_layers_11_layer_norm2 = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(768,))
        self.text_model_encoder_layers_11_mlp_fc1 = nn.Linear(bias=True, in_features=768, out_features=3072)
        self.text_model_encoder_layers_11_mlp_fc2 = nn.Linear(bias=True, in_features=3072, out_features=768)
        self.text_model_final_layer_norm = nn.LayerNorm(elementwise_affine=True, eps=0.000010, normalized_shape=(768,))
        self.pnnx_fold_49_pnnx_fold_49=nn.Parameter(torch.ones(1), requires_grad=False)
        self.pnnx_fold_causal_attention_mask_1_pnnx_fold_causal_attention_mask_1=nn.Parameter(torch.ones(1), requires_grad=False)
#self.position_embedding
#self.causal_attention_mask
#np.load()[:77,:77]

    def embedding(self, v_0):
        return self.text_model_embeddings_token_embedding(v_0)

    def FinalNorm(self, v):
        return self.text_model_final_layer_norm(v)

    
    def forward(self, x_in):
      x=self.forward2(x_in)
      return self.FinalNorm(x)

    def forward2(self, v_0):

        k70 = v_0.size(0)
        k77 = v_0.size(1)
        v_3 = v_0 + self.pnnx_fold_49_pnnx_fold_49

        v_4 = self.pnnx_fold_causal_attention_mask_1_pnnx_fold_causal_attention_mask_1.expand([k70,-1,-1,-1])


        v_5 = self.text_model_encoder_layers_0_layer_norm1(v_3)
        v_6 = self.text_model_encoder_layers_0_self_attn_q_proj(v_5)
        v_7 = (v_6 /8)
        v_8 = self.text_model_encoder_layers_0_self_attn_k_proj(v_5)
        v_9 = v_8.view(k70, -1, 12, 64)
        v_10 = self.text_model_encoder_layers_0_self_attn_v_proj(v_5)
        v_11 = v_10.view(k70, -1, 12, 64)
        v_12 = v_7.view(k70, k77, 12, 64)
        v_13 = torch.transpose(input=v_12, dim0=1, dim1=2)
        v_14 = torch.transpose(input=v_9, dim0=1, dim1=2)
        v_15 = torch.transpose(input=v_11, dim0=1, dim1=2)
        k70x12=k70*12
        v_16 = v_14.reshape(k70x12, -1, 64)
        v_17 = torch.transpose(input=v_16, dim0=1, dim1=2)
        v_18 = v_13.reshape(k70x12, -1, 64)
        v_19 = torch.bmm(input=v_18, mat2=v_17)
        v_20 = v_19.view(k70, 12, k77, k77)
        v_21 = (v_20 + v_4)
        v_22 = v_21.view(k70x12, k77, k77)
        v_23 = torch.softmax(input=v_22, dim=-1)
        v_24 = v_15.reshape(k70x12, -1, 64)
        v_25 = torch.bmm(input=v_23, mat2=v_24)
        v_26 = v_25.view(k70, 12, k77, 64)
        v_27 = torch.transpose(input=v_26, dim0=1, dim1=2)
        v_28 = v_27.reshape(k70, k77, 768)
        v_29 = self.text_model_encoder_layers_0_self_attn_out_proj(v_28)
        v_30 = (v_3 + v_29)
        v_31 = self.text_model_encoder_layers_0_layer_norm2(v_30)
        v_32 = self.text_model_encoder_layers_0_mlp_fc1(v_31)
        v_33 = (v_32 * 1.702)
        v_34 = torch.sigmoid(input=v_33)
        v_35 = (v_32 * v_34)
        v_36 = self.text_model_encoder_layers_0_mlp_fc2(v_35)
        v_37 = (v_30 + v_36)


        v_38 = self.text_model_encoder_layers_1_layer_norm1(v_37)
        v_39 = self.text_model_encoder_layers_1_self_attn_q_proj(v_38)
        v_40 = (v_39 /8)
        v_41 = self.text_model_encoder_layers_1_self_attn_k_proj(v_38)
        v_42 = v_41.view(k70, -1, 12, 64)
        v_43 = self.text_model_encoder_layers_1_self_attn_v_proj(v_38)
        v_44 = v_43.view(k70, -1, 12, 64)
        v_45 = v_40.view(k70, k77, 12, 64)
        v_46 = torch.transpose(input=v_45, dim0=1, dim1=2)
        v_47 = torch.transpose(input=v_42, dim0=1, dim1=2)
        v_48 = torch.transpose(input=v_44, dim0=1, dim1=2)
        v_49 = v_47.reshape(k70x12, -1, 64)
        v_50 = torch.transpose(input=v_49, dim0=1, dim1=2)
        v_51 = v_46.reshape(k70x12, -1, 64)
        v_52 = torch.bmm(input=v_51, mat2=v_50)
        v_53 = v_52.view(k70, 12, k77, k77)
        v_54 = (v_53 + v_4)
        v_55 = v_54.view(k70x12, k77, k77)
        v_56 = torch.softmax(input=v_55, dim=-1)
        v_57 = v_48.reshape(k70x12, -1, 64)
        v_58 = torch.bmm(input=v_56, mat2=v_57)
        v_59 = v_58.view(k70, 12, k77, 64)
        v_60 = torch.transpose(input=v_59, dim0=1, dim1=2)
        v_61 = v_60.reshape(k70, k77, 768)
        v_62 = self.text_model_encoder_layers_1_self_attn_out_proj(v_61)
        v_63 = (v_37 + v_62)
        v_64 = self.text_model_encoder_layers_1_layer_norm2(v_63)
        v_65 = self.text_model_encoder_layers_1_mlp_fc1(v_64)
        v_66 = (v_65 * 1.702)
        v_67 = torch.sigmoid(input=v_66)
        v_68 = (v_65 * v_67)
        v_69 = self.text_model_encoder_layers_1_mlp_fc2(v_68)
        v_70 = (v_63 + v_69)


        v_71 = self.text_model_encoder_layers_2_layer_norm1(v_70)
        v_72 = self.text_model_encoder_layers_2_self_attn_q_proj(v_71)
        v_73 = (v_72 /8)
        v_74 = self.text_model_encoder_layers_2_self_attn_k_proj(v_71)
        v_75 = v_74.view(k70, -1, 12, 64)
        v_76 = self.text_model_encoder_layers_2_self_attn_v_proj(v_71)
        v_77 = v_76.view(k70, -1, 12, 64)
        v_78 = v_73.view(k70, k77, 12, 64)
        v_79 = torch.transpose(input=v_78, dim0=1, dim1=2)
        v_80 = torch.transpose(input=v_75, dim0=1, dim1=2)
        v_81 = torch.transpose(input=v_77, dim0=1, dim1=2)
        v_82 = v_80.reshape(k70x12, -1, 64)
        v_83 = torch.transpose(input=v_82, dim0=1, dim1=2)
        v_84 = v_79.reshape(k70x12, -1, 64)
        v_85 = torch.bmm(input=v_84, mat2=v_83)
        v_86 = v_85.view(k70, 12, k77, k77)
        v_87 = (v_86 + v_4)
        v_88 = v_87.view(k70x12, k77, k77)
        v_89 = torch.softmax(input=v_88, dim=-1)
        v_90 = v_81.reshape(k70x12, -1, 64)
        v_91 = torch.bmm(input=v_89, mat2=v_90)
        v_92 = v_91.view(k70, 12, k77, 64)
        v_93 = torch.transpose(input=v_92, dim0=1, dim1=2)
        v_94 = v_93.reshape(k70, k77, 768)
        v_95 = self.text_model_encoder_layers_2_self_attn_out_proj(v_94)
        v_96 = (v_70 + v_95)
        v_97 = self.text_model_encoder_layers_2_layer_norm2(v_96)
        v_98 = self.text_model_encoder_layers_2_mlp_fc1(v_97)
        v_99 = (v_98 * 1.702)
        v_100 = torch.sigmoid(input=v_99)
        v_101 = (v_98 * v_100)
        v_102 = self.text_model_encoder_layers_2_mlp_fc2(v_101)
        v_103 = (v_96 + v_102)


        v_104 = self.text_model_encoder_layers_3_layer_norm1(v_103)
        v_105 = self.text_model_encoder_layers_3_self_attn_q_proj(v_104)
        v_106 = (v_105 /8)
        v_107 = self.text_model_encoder_layers_3_self_attn_k_proj(v_104)
        v_108 = v_107.view(k70, -1, 12, 64)
        v_109 = self.text_model_encoder_layers_3_self_attn_v_proj(v_104)
        v_110 = v_109.view(k70, -1, 12, 64)
        v_111 = v_106.view(k70, k77, 12, 64)
        v_112 = torch.transpose(input=v_111, dim0=1, dim1=2)
        v_113 = torch.transpose(input=v_108, dim0=1, dim1=2)
        v_114 = torch.transpose(input=v_110, dim0=1, dim1=2)
        v_115 = v_113.reshape(k70x12, -1, 64)
        v_116 = torch.transpose(input=v_115, dim0=1, dim1=2)
        v_117 = v_112.reshape(k70x12, -1, 64)
        v_118 = torch.bmm(input=v_117, mat2=v_116)
        v_119 = v_118.view(k70, 12, k77, k77)
        v_120 = (v_119 + v_4)
        v_121 = v_120.view(k70x12, k77, k77)
        v_122 = torch.softmax(input=v_121, dim=-1)
        v_123 = v_114.reshape(k70x12, -1, 64)
        v_124 = torch.bmm(input=v_122, mat2=v_123)
        v_125 = v_124.view(k70, 12, k77, 64)
        v_126 = torch.transpose(input=v_125, dim0=1, dim1=2)
        v_127 = v_126.reshape(k70, k77, 768)
        v_128 = self.text_model_encoder_layers_3_self_attn_out_proj(v_127)
        v_129 = (v_103 + v_128)
        v_130 = self.text_model_encoder_layers_3_layer_norm2(v_129)
        v_131 = self.text_model_encoder_layers_3_mlp_fc1(v_130)
        v_132 = (v_131 * 1.702)
        v_133 = torch.sigmoid(input=v_132)
        v_134 = (v_131 * v_133)
        v_135 = self.text_model_encoder_layers_3_mlp_fc2(v_134)
        v_136 = (v_129 + v_135)


        v_137 = self.text_model_encoder_layers_4_layer_norm1(v_136)
        v_138 = self.text_model_encoder_layers_4_self_attn_q_proj(v_137)
        v_139 = (v_138 /8)
        v_140 = self.text_model_encoder_layers_4_self_attn_k_proj(v_137)
        v_141 = v_140.view(k70, -1, 12, 64)
        v_142 = self.text_model_encoder_layers_4_self_attn_v_proj(v_137)
        v_143 = v_142.view(k70, -1, 12, 64)
        v_144 = v_139.view(k70, k77, 12, 64)
        v_145 = torch.transpose(input=v_144, dim0=1, dim1=2)
        v_146 = torch.transpose(input=v_141, dim0=1, dim1=2)
        v_147 = torch.transpose(input=v_143, dim0=1, dim1=2)
        v_148 = v_146.reshape(k70x12, -1, 64)
        v_149 = torch.transpose(input=v_148, dim0=1, dim1=2)
        v_150 = v_145.reshape(k70x12, -1, 64)
        v_151 = torch.bmm(input=v_150, mat2=v_149)
        v_152 = v_151.view(k70, 12, k77, k77)
        v_153 = (v_152 + v_4)
        v_154 = v_153.view(k70x12, k77, k77)
        v_155 = torch.softmax(input=v_154, dim=-1)
        v_156 = v_147.reshape(k70x12, -1, 64)
        v_157 = torch.bmm(input=v_155, mat2=v_156)
        v_158 = v_157.view(k70, 12, k77, 64)
        v_159 = torch.transpose(input=v_158, dim0=1, dim1=2)
        v_160 = v_159.reshape(k70, k77, 768)
        v_161 = self.text_model_encoder_layers_4_self_attn_out_proj(v_160)
        v_162 = (v_136 + v_161)
        v_163 = self.text_model_encoder_layers_4_layer_norm2(v_162)
        v_164 = self.text_model_encoder_layers_4_mlp_fc1(v_163)
        v_165 = (v_164 * 1.702)
        v_166 = torch.sigmoid(input=v_165)
        v_167 = (v_164 * v_166)
        v_168 = self.text_model_encoder_layers_4_mlp_fc2(v_167)
        v_169 = (v_162 + v_168)


        v_170 = self.text_model_encoder_layers_5_layer_norm1(v_169)
        v_171 = self.text_model_encoder_layers_5_self_attn_q_proj(v_170)
        v_172 = (v_171 /8)
        v_173 = self.text_model_encoder_layers_5_self_attn_k_proj(v_170)
        v_174 = v_173.view(k70, -1, 12, 64)
        v_175 = self.text_model_encoder_layers_5_self_attn_v_proj(v_170)
        v_176 = v_175.view(k70, -1, 12, 64)
        v_177 = v_172.view(k70, k77, 12, 64)
        v_178 = torch.transpose(input=v_177, dim0=1, dim1=2)
        v_179 = torch.transpose(input=v_174, dim0=1, dim1=2)
        v_180 = torch.transpose(input=v_176, dim0=1, dim1=2)
        v_181 = v_179.reshape(k70x12, -1, 64)
        v_182 = torch.transpose(input=v_181, dim0=1, dim1=2)
        v_183 = v_178.reshape(k70x12, -1, 64)
        v_184 = torch.bmm(input=v_183, mat2=v_182)
        v_185 = v_184.view(k70, 12, k77, k77)
        v_186 = (v_185 + v_4)
        v_187 = v_186.view(k70x12, k77, k77)
        v_188 = torch.softmax(input=v_187, dim=-1)
        v_189 = v_180.reshape(k70x12, -1, 64)
        v_190 = torch.bmm(input=v_188, mat2=v_189)
        v_191 = v_190.view(k70, 12, k77, 64)
        v_192 = torch.transpose(input=v_191, dim0=1, dim1=2)
        v_193 = v_192.reshape(k70, k77, 768)
        v_194 = self.text_model_encoder_layers_5_self_attn_out_proj(v_193)
        v_195 = (v_169 + v_194)
        v_196 = self.text_model_encoder_layers_5_layer_norm2(v_195)
        v_197 = self.text_model_encoder_layers_5_mlp_fc1(v_196)
        v_198 = (v_197 * 1.702)
        v_199 = torch.sigmoid(input=v_198)
        v_200 = (v_197 * v_199)
        v_201 = self.text_model_encoder_layers_5_mlp_fc2(v_200)
        v_202 = (v_195 + v_201)


        v_203 = self.text_model_encoder_layers_6_layer_norm1(v_202)
        v_204 = self.text_model_encoder_layers_6_self_attn_q_proj(v_203)
        v_205 = (v_204 /8)
        v_206 = self.text_model_encoder_layers_6_self_attn_k_proj(v_203)
        v_207 = v_206.view(k70, -1, 12, 64)
        v_208 = self.text_model_encoder_layers_6_self_attn_v_proj(v_203)
        v_209 = v_208.view(k70, -1, 12, 64)
        v_210 = v_205.view(k70, k77, 12, 64)
        v_211 = torch.transpose(input=v_210, dim0=1, dim1=2)
        v_212 = torch.transpose(input=v_207, dim0=1, dim1=2)
        v_213 = torch.transpose(input=v_209, dim0=1, dim1=2)
        v_214 = v_212.reshape(k70x12, -1, 64)
        v_215 = torch.transpose(input=v_214, dim0=1, dim1=2)
        v_216 = v_211.reshape(k70x12, -1, 64)
        v_217 = torch.bmm(input=v_216, mat2=v_215)
        v_218 = v_217.view(k70, 12, k77, k77)
        v_219 = (v_218 + v_4)
        v_220 = v_219.view(k70x12, k77, k77)
        v_221 = torch.softmax(input=v_220, dim=-1)
        v_222 = v_213.reshape(k70x12, -1, 64)
        v_223 = torch.bmm(input=v_221, mat2=v_222)
        v_224 = v_223.view(k70, 12, k77, 64)
        v_225 = torch.transpose(input=v_224, dim0=1, dim1=2)
        v_226 = v_225.reshape(k70, k77, 768)
        v_227 = self.text_model_encoder_layers_6_self_attn_out_proj(v_226)
        v_228 = (v_202 + v_227)
        v_229 = self.text_model_encoder_layers_6_layer_norm2(v_228)
        v_230 = self.text_model_encoder_layers_6_mlp_fc1(v_229)
        v_231 = (v_230 * 1.702)
        v_232 = torch.sigmoid(input=v_231)
        v_233 = (v_230 * v_232)
        v_234 = self.text_model_encoder_layers_6_mlp_fc2(v_233)
        v_235 = (v_228 + v_234)


        v_236 = self.text_model_encoder_layers_7_layer_norm1(v_235)
        v_237 = self.text_model_encoder_layers_7_self_attn_q_proj(v_236)
        v_238 = (v_237 /8)
        v_239 = self.text_model_encoder_layers_7_self_attn_k_proj(v_236)
        v_240 = v_239.view(k70, -1, 12, 64)
        v_241 = self.text_model_encoder_layers_7_self_attn_v_proj(v_236)
        v_242 = v_241.view(k70, -1, 12, 64)
        v_243 = v_238.view(k70, k77, 12, 64)
        v_244 = torch.transpose(input=v_243, dim0=1, dim1=2)
        v_245 = torch.transpose(input=v_240, dim0=1, dim1=2)
        v_246 = torch.transpose(input=v_242, dim0=1, dim1=2)
        v_247 = v_245.reshape(k70x12, -1, 64)
        v_248 = torch.transpose(input=v_247, dim0=1, dim1=2)
        v_249 = v_244.reshape(k70x12, -1, 64)
        v_250 = torch.bmm(input=v_249, mat2=v_248)
        v_251 = v_250.view(k70, 12, k77, k77)
        v_252 = (v_251 + v_4)
        v_253 = v_252.view(k70x12, k77, k77)
        v_254 = torch.softmax(input=v_253, dim=-1)
        v_255 = v_246.reshape(k70x12, -1, 64)
        v_256 = torch.bmm(input=v_254, mat2=v_255)
        v_257 = v_256.view(k70, 12, k77, 64)
        v_258 = torch.transpose(input=v_257, dim0=1, dim1=2)
        v_259 = v_258.reshape(k70, k77, 768)
        v_260 = self.text_model_encoder_layers_7_self_attn_out_proj(v_259)
        v_261 = (v_235 + v_260)
        v_262 = self.text_model_encoder_layers_7_layer_norm2(v_261)
        v_263 = self.text_model_encoder_layers_7_mlp_fc1(v_262)
        v_264 = (v_263 * 1.702)
        v_265 = torch.sigmoid(input=v_264)
        v_266 = (v_263 * v_265)
        v_267 = self.text_model_encoder_layers_7_mlp_fc2(v_266)
        v_268 = (v_261 + v_267)
        if self.cut == -4:
          return v_268

        v_269 = self.text_model_encoder_layers_8_layer_norm1(v_268)
        v_270 = self.text_model_encoder_layers_8_self_attn_q_proj(v_269)
        v_271 = (v_270 /8)
        v_272 = self.text_model_encoder_layers_8_self_attn_k_proj(v_269)
        v_273 = v_272.view(k70, -1, 12, 64)
        v_274 = self.text_model_encoder_layers_8_self_attn_v_proj(v_269)
        v_275 = v_274.view(k70, -1, 12, 64)
        v_276 = v_271.view(k70, k77, 12, 64)
        v_277 = torch.transpose(input=v_276, dim0=1, dim1=2)
        v_278 = torch.transpose(input=v_273, dim0=1, dim1=2)
        v_279 = torch.transpose(input=v_275, dim0=1, dim1=2)
        v_280 = v_278.reshape(k70x12, -1, 64)
        v_281 = torch.transpose(input=v_280, dim0=1, dim1=2)
        v_282 = v_277.reshape(k70x12, -1, 64)
        v_283 = torch.bmm(input=v_282, mat2=v_281)
        v_284 = v_283.view(k70, 12, k77, k77)
        v_285 = (v_284 + v_4)
        v_286 = v_285.view(k70x12, k77, k77)
        v_287 = torch.softmax(input=v_286, dim=-1)
        v_288 = v_279.reshape(k70x12, -1, 64)
        v_289 = torch.bmm(input=v_287, mat2=v_288)
        v_290 = v_289.view(k70, 12, k77, 64)
        v_291 = torch.transpose(input=v_290, dim0=1, dim1=2)
        v_292 = v_291.reshape(k70, k77, 768)
        v_293 = self.text_model_encoder_layers_8_self_attn_out_proj(v_292)
        v_294 = (v_268 + v_293)
        v_295 = self.text_model_encoder_layers_8_layer_norm2(v_294)
        v_296 = self.text_model_encoder_layers_8_mlp_fc1(v_295)
        v_297 = (v_296 * 1.702)
        v_298 = torch.sigmoid(input=v_297)
        v_299 = (v_296 * v_298)
        v_300 = self.text_model_encoder_layers_8_mlp_fc2(v_299)
        v_301 = (v_294 + v_300)
        if self.cut == -3:
          return v_301

        v_302 = self.text_model_encoder_layers_9_layer_norm1(v_301)
        v_303 = self.text_model_encoder_layers_9_self_attn_q_proj(v_302)
        v_304 = (v_303 /8)
        v_305 = self.text_model_encoder_layers_9_self_attn_k_proj(v_302)
        v_306 = v_305.view(k70, -1, 12, 64)
        v_307 = self.text_model_encoder_layers_9_self_attn_v_proj(v_302)
        v_308 = v_307.view(k70, -1, 12, 64)
        v_309 = v_304.view(k70, k77, 12, 64)
        v_310 = torch.transpose(input=v_309, dim0=1, dim1=2)
        v_311 = torch.transpose(input=v_306, dim0=1, dim1=2)
        v_312 = torch.transpose(input=v_308, dim0=1, dim1=2)
        v_313 = v_311.reshape(k70x12, -1, 64)
        v_314 = torch.transpose(input=v_313, dim0=1, dim1=2)
        v_315 = v_310.reshape(k70x12, -1, 64)
        v_316 = torch.bmm(input=v_315, mat2=v_314)
        v_317 = v_316.view(k70, 12, k77, k77)
        v_318 = (v_317 + v_4)
        v_319 = v_318.view(k70x12, k77, k77)
        v_320 = torch.softmax(input=v_319, dim=-1)
        v_321 = v_312.reshape(k70x12, -1, 64)
        v_322 = torch.bmm(input=v_320, mat2=v_321)
        v_323 = v_322.view(k70, 12, k77, 64)
        v_324 = torch.transpose(input=v_323, dim0=1, dim1=2)
        v_325 = v_324.reshape(k70, k77, 768)
        v_326 = self.text_model_encoder_layers_9_self_attn_out_proj(v_325)
        v_327 = (v_301 + v_326)
        v_328 = self.text_model_encoder_layers_9_layer_norm2(v_327)
        v_329 = self.text_model_encoder_layers_9_mlp_fc1(v_328)
        v_330 = (v_329 * 1.702)
        v_331 = torch.sigmoid(input=v_330)
        v_332 = (v_329 * v_331)
        v_333 = self.text_model_encoder_layers_9_mlp_fc2(v_332)
        v_334 = (v_327 + v_333)
        if self.cut == -2:
          return v_334

        v_335 = self.text_model_encoder_layers_10_layer_norm1(v_334)
        v_336 = self.text_model_encoder_layers_10_self_attn_q_proj(v_335)
        v_337 = (v_336 /8)
        v_338 = self.text_model_encoder_layers_10_self_attn_k_proj(v_335)
        v_339 = v_338.view(k70, -1, 12, 64)
        v_340 = self.text_model_encoder_layers_10_self_attn_v_proj(v_335)
        v_341 = v_340.view(k70, -1, 12, 64)
        v_342 = v_337.view(k70, k77, 12, 64)
        v_343 = torch.transpose(input=v_342, dim0=1, dim1=2)
        v_344 = torch.transpose(input=v_339, dim0=1, dim1=2)
        v_345 = torch.transpose(input=v_341, dim0=1, dim1=2)
        v_346 = v_344.reshape(k70x12, -1, 64)
        v_347 = torch.transpose(input=v_346, dim0=1, dim1=2)
        v_348 = v_343.reshape(k70x12, -1, 64)
        v_349 = torch.bmm(input=v_348, mat2=v_347)
        v_350 = v_349.view(k70, 12, k77, k77)
        v_351 = (v_350 + v_4)
        v_352 = v_351.view(k70x12, k77, k77)
        v_353 = torch.softmax(input=v_352, dim=-1)
        v_354 = v_345.reshape(k70x12, -1, 64)
        v_355 = torch.bmm(input=v_353, mat2=v_354)
        v_356 = v_355.view(k70, 12, k77, 64)
        v_357 = torch.transpose(input=v_356, dim0=1, dim1=2)
        v_358 = v_357.reshape(k70, k77, 768)
        v_359 = self.text_model_encoder_layers_10_self_attn_out_proj(v_358)
        v_360 = (v_334 + v_359)
        v_361 = self.text_model_encoder_layers_10_layer_norm2(v_360)
        v_362 = self.text_model_encoder_layers_10_mlp_fc1(v_361)
        v_363 = (v_362 * 1.702)
        v_364 = torch.sigmoid(input=v_363)
        v_365 = (v_362 * v_364)
        v_366 = self.text_model_encoder_layers_10_mlp_fc2(v_365)
        v_367 = (v_360 + v_366)
        if self.cut == -1:
          return v_367

        v_368 = self.text_model_encoder_layers_11_layer_norm1(v_367)
        v_369 = self.text_model_encoder_layers_11_self_attn_q_proj(v_368)
        v_370 = (v_369 /8)
        v_371 = self.text_model_encoder_layers_11_self_attn_k_proj(v_368)
        v_372 = v_371.view(k70, -1, 12, 64)
        v_373 = self.text_model_encoder_layers_11_self_attn_v_proj(v_368)
        v_374 = v_373.view(k70, -1, 12, 64)
        v_375 = v_370.view(k70, k77, 12, 64)
        v_376 = torch.transpose(input=v_375, dim0=1, dim1=2)
        v_377 = torch.transpose(input=v_372, dim0=1, dim1=2)
        v_378 = torch.transpose(input=v_374, dim0=1, dim1=2)
        v_379 = v_377.reshape(k70x12, -1, 64)
        v_380 = torch.transpose(input=v_379, dim0=1, dim1=2)
        v_381 = v_376.reshape(k70x12, -1, 64)
        v_382 = torch.bmm(input=v_381, mat2=v_380)
        v_383 = v_382.view(k70, 12, k77, k77)
        v_384 = (v_383 + v_4)
        v_385 = v_384.view(k70x12, k77, k77)
        v_386 = torch.softmax(input=v_385, dim=-1)
        v_387 = v_378.reshape(k70x12, -1, 64)
        v_388 = torch.bmm(input=v_386, mat2=v_387)
        v_389 = v_388.view(k70, 12, k77, 64)
        v_390 = torch.transpose(input=v_389, dim0=1, dim1=2)
        v_391 = v_390.reshape(k70, k77, 768)
        v_392 = self.text_model_encoder_layers_11_self_attn_out_proj(v_391)
        v_393 = (v_367 + v_392)
        v_394 = self.text_model_encoder_layers_11_layer_norm2(v_393)
        v_395 = self.text_model_encoder_layers_11_mlp_fc1(v_394)
        v_396 = (v_395 * 1.702)
        v_397 = torch.sigmoid(input=v_396)
        v_398 = (v_395 * v_397)
        v_399 = self.text_model_encoder_layers_11_mlp_fc2(v_398)
        v_400 = (v_393 + v_399)


        return v_400

from accelerate import init_empty_weights
with init_empty_weights():
    jitbase.transformerJIT= torch.jit.script(transformer().requires_grad_(False).eval())