{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "min-dalle-interactive.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "mU7OUXRWJdNl",
        "yn8qjo29URQE"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Min-dalle classes"
      ],
      "metadata": {
        "id": "mU7OUXRWJdNl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IyGLh1yWJEdu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if not os.path.isfile('/content/once.txt'):\n",
        "  !pip install accelerate\n",
        "  !pip install ncnn\n",
        "  !wget -O /tmp/vq.param https://raw.githubusercontent.com/TabuaTambalam/vqqncnn/main/vq.param\n",
        "  !wget -O /tmp/vq.bin https://github.com/TabuaTambalam/vqqncnn/releases/download/0.0/vq.bin\n",
        "  !wget -O /tmp/vq_vert.param https://raw.githubusercontent.com/TabuaTambalam/vqqncnn/main/vq_vert.param\n",
        "  !git clone https://github.com/kuprel/min-dalle.git\n",
        "  !mv 'min-dalle/min_dalle' min_dalle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/once.txt\n",
        "uwa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmwpJTT4JaTt",
        "outputId": "616d185d-d546-43af-d743-10cc8cf77c5c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/once.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torch import LongTensor, FloatTensor\n",
        "from math import sqrt\n",
        "import torch\n",
        "import torch.backends.cudnn, torch.backends.cuda\n",
        "import json\n",
        "import requests\n",
        "from typing import Iterator\n",
        "from min_dalle.text_tokenizer import TextTokenizer\n",
        "from min_dalle.models import DalleBartEncoder, DalleBartDecoder\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "torch.set_num_threads(os.cpu_count())\n",
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True\n",
        "\n",
        "MIN_DALLE_REPO = 'https://huggingface.co/kuprel/min-dalle/resolve/main/'\n",
        "IMAGE_TOKEN_COUNT = 256\n",
        "\n",
        "\n",
        "class MinDalle:\n",
        "    def __init__(\n",
        "        self,\n",
        "        models_root: str = 'pretrained',\n",
        "        dtype: torch.dtype = torch.float16,\n",
        "        device: str = None,\n",
        "        is_mega: bool = True, \n",
        "        is_reusable: bool = True,\n",
        "        is_verbose = True\n",
        "    ):\n",
        "        if torch.cuda.is_available():\n",
        "          if device == None:\n",
        "              device = 'cuda'\n",
        "        else:\n",
        "          device = 'cpu'\n",
        "          dtype=torch.float32\n",
        "\n",
        "        if is_verbose: print(\"using device\", device)\n",
        "        self.device = device\n",
        "        self.is_mega = is_mega\n",
        "        self.is_reusable = is_reusable\n",
        "        self.dtype = dtype\n",
        "        self.is_verbose = is_verbose\n",
        "        self.text_token_count = 64\n",
        "        self.layer_count = 24 if is_mega else 12\n",
        "        self.attention_head_count = 32 if is_mega else 16\n",
        "        self.embed_count = 2048 if is_mega else 1024\n",
        "        self.glu_embed_count = 4096 if is_mega else 2730\n",
        "        self.text_vocab_count = 50272 if is_mega else 50264\n",
        "        self.image_vocab_count = 16415 if is_mega else 16384\n",
        "\n",
        "        model_name = 'dalle_bart_{}'.format('mega' if is_mega else 'mini')\n",
        "        dalle_path = os.path.join(models_root, model_name)\n",
        "        vqgan_path = os.path.join(models_root, 'vqgan')\n",
        "        if not os.path.exists(dalle_path): os.makedirs(dalle_path)\n",
        "        if not os.path.exists(vqgan_path): os.makedirs(vqgan_path)\n",
        "        self.vocab_path = os.path.join(dalle_path, 'vocab.json')\n",
        "        self.merges_path = os.path.join(dalle_path, 'merges.txt')\n",
        "        self.encoder_params_path = os.path.join(dalle_path, 'encoder.pt')\n",
        "        self.decoder_params_path = os.path.join(dalle_path, 'decoder.pt')\n",
        "        self.detoker_params_path = os.path.join(vqgan_path, 'detoker.pt')\n",
        "\n",
        "        self.init_tokenizer()\n",
        "        if is_reusable:\n",
        "            self.init_encoder()\n",
        "            self.init_decoder()\n",
        "\n",
        "\n",
        "    def download_tokenizer(self):\n",
        "        if self.is_verbose: print(\"downloading tokenizer params\")\n",
        "        suffix = '' if self.is_mega else '_mini'\n",
        "        vocab = requests.get(MIN_DALLE_REPO + 'vocab{}.json'.format(suffix))\n",
        "        merges = requests.get(MIN_DALLE_REPO + 'merges{}.txt'.format(suffix))\n",
        "        with open(self.vocab_path, 'wb') as f: f.write(vocab.content)\n",
        "        with open(self.merges_path, 'wb') as f: f.write(merges.content)\n",
        "\n",
        "\n",
        "    def download_encoder(self):\n",
        "        if self.is_verbose: print(\"downloading encoder params\")\n",
        "        suffix = '' if self.is_mega else '_mini'\n",
        "        urli=MIN_DALLE_REPO + 'encoder{}.pt'.format(suffix)\n",
        "        !wget -O {self.encoder_params_path} {urli}\n",
        "\n",
        "\n",
        "    def download_decoder(self):\n",
        "        if self.is_verbose: print(\"downloading decoder params\")\n",
        "        suffix = '' if self.is_mega else '_mini'\n",
        "        urli=MIN_DALLE_REPO + 'decoder{}.pt'.format(suffix)\n",
        "        !wget -O {self.decoder_params_path} {urli}\n",
        "    \n",
        "\n",
        "    def init_tokenizer(self):\n",
        "        is_downloaded = os.path.exists(self.vocab_path)\n",
        "        is_downloaded &= os.path.exists(self.merges_path)\n",
        "        if not is_downloaded: self.download_tokenizer()\n",
        "        if self.is_verbose: print(\"intializing TextTokenizer\")\n",
        "        with open(self.vocab_path, 'r', encoding='utf8') as f:\n",
        "            vocab = json.load(f)\n",
        "        with open(self.merges_path, 'r', encoding='utf8') as f:\n",
        "            merges = f.read().split(\"\\n\")[1:-1]\n",
        "        self.tokenizer = TextTokenizer(vocab, merges)\n",
        "\n",
        "\n",
        "    def init_encoder(self):\n",
        "        is_downloaded = os.path.exists(self.encoder_params_path)\n",
        "        if not is_downloaded: self.download_encoder()\n",
        "        if self.is_verbose: print(\"initializing DalleBartEncoder\")\n",
        "        self.encoder = DalleBartEncoder(\n",
        "            attention_head_count = self.attention_head_count,\n",
        "            embed_count = self.embed_count,\n",
        "            glu_embed_count = self.glu_embed_count,\n",
        "            text_token_count = self.text_token_count,\n",
        "            text_vocab_count = self.text_vocab_count,\n",
        "            layer_count = self.layer_count,\n",
        "            device=self.device\n",
        "        ).to(self.dtype).eval()\n",
        "        params = torch.load(self.encoder_params_path)\n",
        "        self.encoder.load_state_dict(params, strict=False)\n",
        "        del params\n",
        "        self.encoder = self.encoder.to(device=self.device)\n",
        "\n",
        "\n",
        "    def init_decoder(self):\n",
        "        is_downloaded = os.path.exists(self.decoder_params_path)\n",
        "        if not is_downloaded: self.download_decoder()\n",
        "        if self.is_verbose: print(\"initializing DalleBartDecoder\")\n",
        "        self.decoder = DalleBartDecoder(\n",
        "            image_vocab_count = self.image_vocab_count,\n",
        "            attention_head_count = self.attention_head_count,\n",
        "            embed_count = self.embed_count,\n",
        "            glu_embed_count = self.glu_embed_count,\n",
        "            layer_count = self.layer_count,\n",
        "            device=self.device\n",
        "        ).to(self.dtype).eval()\n",
        "        params = torch.load(self.decoder_params_path)\n",
        "        self.decoder.load_state_dict(params, strict=False)\n",
        "        del params\n",
        "        self.decoder = self.decoder.to(device=self.device)\n",
        "\n"
      ],
      "metadata": {
        "id": "l14d8tRdJ8ak"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mkCBemb(seq):\n",
        "  with net.create_extractor() as ex:\n",
        "    ex.input(\"in0\", ncnn.Mat(seq).clone())\n",
        "    hrr, out0 = ex.extract(\"2\")\n",
        "  del ex\n",
        "  return out0\n",
        "\n",
        "def emb2img(emb):\n",
        "  with net.create_extractor() as ex:\n",
        "    ex.input(\"2\", emb)\n",
        "    hrr, out0 = ex.extract(\"out0\")\n",
        "  del ex\n",
        "  return Image.fromarray(np.array(out0).astype(np.uint8))\n",
        "\n",
        "def npmkCBemb(seq):\n",
        "  with net.create_extractor() as ex:\n",
        "    ex.input(\"in0\", ncnn.Mat(seq).clone())\n",
        "    hrr, out0 = ex.extract(\"2\")\n",
        "  del ex\n",
        "  return np.array(out0)\n",
        "\n",
        "\n",
        "def npemb2img(emb):\n",
        "  with net.create_extractor() as ex:\n",
        "    ex.input(\"2\", ncnn.Mat(emb).clone())\n",
        "    hrr, out0 = ex.extract(\"out0\")\n",
        "  del ex\n",
        "  return Image.fromarray(np.array(out0).astype(np.uint8))\n",
        "\n",
        "def pbla(step,scale):\n",
        "  ret=[]\n",
        "  mga=4-(4/scale)\n",
        "  k=step-1\n",
        "  for i in range(step):\n",
        "    ret.append(k+mga*( ((i**2)/k) - i ))\n",
        "  return ret\n",
        "\n",
        "\n",
        "def hstack(sta,n=16,crop=[]):\n",
        "  haf=[]\n",
        "  for k in range(n):\n",
        "    haf.append(dumped_seqs[sta+k].reshape((16,16))[:,:8])\n",
        "  if crop:\n",
        "    sta=crop[0]>>4\n",
        "    endo=(crop[0]+crop[1])>>4\n",
        "    return np.hstack(haf)[:,sta:endo].reshape(16*(endo-sta))\n",
        "  else:\n",
        "    return np.hstack(haf).reshape(128*n)\n",
        "    \n",
        "\n",
        "\n",
        "def mk3x3(idx):\n",
        "  rowz=[]\n",
        "  for y in range(3):\n",
        "    colz=[]\n",
        "    for x in range(3):\n",
        "      colz.append(dumped_seqs[idx[y*3+x]].reshape((16,16)))\n",
        "    rowz.append(np.hstack(colz).reshape(768))\n",
        "  return np.concatenate(rowz)\n",
        "\n",
        "def interpo(seq1,seq2,step=30,scale=1.21,outfmt='/content/avif/%02d.png'):\n",
        "  stp=step-1\n",
        "  divi=pbla(step,scale)\n",
        "  em1=npmkCBemb(seq1)\n",
        "  em2=npmkCBemb(seq2)\n",
        "  for i in range(step):\n",
        "    npemb2img((em1*i+em2*(stp-i))/divi[i]).save(outfmt%i)\n",
        "\n",
        "def showp(n):\n",
        "  with net.create_extractor() as ex:\n",
        "    ex.input(\"in0\", ncnn.Mat(dumped_seqs[n].astype(np.uint32)).clone())\n",
        "    hrr, out0 = ex.extract(\"out0\")\n",
        "  del ex\n",
        "  uz=Image.fromarray(np.array(out0).astype(np.uint8))\n",
        "  uz.save('/content/sample_data/%d.png'%n)\n",
        "  return uz\n",
        "\n",
        "def showp2(seq):\n",
        "  with net.create_extractor() as ex:\n",
        "    ex.input(\"in0\", ncnn.Mat(seq.astype(np.uint32)).clone())\n",
        "    hrr, out0 = ex.extract(\"out0\")\n",
        "  del ex\n",
        "  uz=Image.fromarray(np.array(out0).astype(np.uint8))\n",
        "  uz.save('/content/sample_data/000.png')\n",
        "  return uz\n",
        "\n",
        "def hcopy(tk,left_sta=8):\n",
        "  len=16-left_sta\n",
        "  for y in range(16):\n",
        "    mae=1+y*16\n",
        "    tk[mae:mae+len]=tk[mae+left_sta:mae+16]\n",
        "\n",
        "def hcopy_dup(tk,sele,left_sta=8):\n",
        "  src=sele.expand(candidate_count,-1).T\n",
        "  len=16-left_sta\n",
        "  for y in range(16):\n",
        "    mae=1+y*16\n",
        "    tk[mae:mae+len]=src[mae+left_sta:mae+16]\n",
        "\n",
        "def hcopy_dst(src,tk,len=8):\n",
        "  for y in range(16):\n",
        "    mae=y*16\n",
        "    tk[1+mae:1+mae+len]=torch.from_numpy(src[mae:mae+len])\n",
        "\n",
        "def rumpla():\n",
        "  global attention_state\n",
        "  for row_index in range(   ROW_START   ,16):\n",
        "    print('%x:'%row_index, end='')\n",
        "    kt=16 * row_index\n",
        "    for col_index in range(COL_START,16):\n",
        "      i =  kt + col_index       \n",
        "      with torch.cuda.amp.autocast(dtype=mindd.dtype):\n",
        "          image_tokens[i + 1], attention_state = mindd.decoder.forward(\n",
        "              settings=settings,\n",
        "              attention_mask=attention_mask,\n",
        "              encoder_state=encoder_state,\n",
        "              attention_state=attention_state,\n",
        "              prev_tokens=image_tokens[i],\n",
        "              token_index=token_indices[[i]]\n",
        "          )\n",
        "  \n",
        "!rm /content/sample_data/*.png\n",
        "newstart=True"
      ],
      "metadata": {
        "id": "Ujkb7IY3NSDb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PlayGround"
      ],
      "metadata": {
        "id": "mDKKp_RiJ3Hs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if newstart:\n",
        "  UseMega=False #@param {type:\"boolean\"}\n",
        "  mindd = MinDalle(is_mega=UseMega, is_reusable=True)\n",
        "  import ncnn\n",
        "  import gc\n",
        "  net = ncnn.Net()\n",
        "  net.opt.use_vulkan_compute = True\n",
        "  net.load_param(  \"/tmp/vq.param\"   )  #   \"/content/vq3x3.txt\"\n",
        "  net.load_model(\"/tmp/vq.bin\")\n",
        "  newstart=False\n",
        "  !nvidia-smi"
      ],
      "metadata": {
        "cellView": "form",
        "id": "NGoqCk3nLQRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make a prompt"
      ],
      "metadata": {
        "id": "bvizR5gUOACG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text =\"desert oasis merchant market high fantasy book cover painting\" #@param {type:\"string\"}\n",
        "candidate_count =  4#@param {type:\"integer\"}\n",
        "seed= 776677  #@param {type:\"integer\"}\n",
        "\n",
        "'''\n",
        "555\n",
        "'''\n",
        "\n",
        "is_verbose=False\n",
        "\n",
        "if is_verbose: print(\"tokenizing text\")\n",
        "tokens = mindd.tokenizer.tokenize(text, is_verbose=is_verbose)\n",
        "if len(tokens) > mindd.text_token_count: \n",
        "    tokens = tokens[:mindd.text_token_count]\n",
        "if is_verbose: print(\"{} text tokens\".format(len(tokens)), tokens)\n",
        "text_tokens = np.ones((2, 64), dtype=np.int32)\n",
        "text_tokens[0, :2] = [tokens[0], tokens[-1]]\n",
        "text_tokens[1, :len(tokens)] = tokens\n",
        "text_tokens = torch.tensor(\n",
        "    text_tokens, \n",
        "    dtype=torch.long, \n",
        "    device=mindd.device\n",
        ")\n",
        "\n",
        "if not mindd.is_reusable: mindd.init_encoder()\n",
        "if is_verbose: print(\"encoding text tokens\")\n",
        "with torch.cuda.amp.autocast(dtype=mindd.dtype):\n",
        "    encoder_state = mindd.encoder.forward(text_tokens)\n",
        "if not mindd.is_reusable: del mindd.encoder\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "if not mindd.is_reusable: mindd.init_decoder()\n",
        "\n",
        "with torch.cuda.amp.autocast(dtype=mindd.dtype):\n",
        "    expanded_indices = [0] * candidate_count + [1] * candidate_count\n",
        "    text_tokens = text_tokens[expanded_indices]\n",
        "    encoder_state = encoder_state[expanded_indices]\n",
        "    attention_mask = text_tokens.not_equal(1)\n",
        "    attention_state = torch.zeros(\n",
        "        size=(\n",
        "            mindd.layer_count,\n",
        "            candidate_count * 4,\n",
        "            IMAGE_TOKEN_COUNT,\n",
        "            mindd.embed_count\n",
        "        ), \n",
        "        device=mindd.device\n",
        "    )\n",
        "    image_tokens = torch.full(\n",
        "        (IMAGE_TOKEN_COUNT + 1, candidate_count), \n",
        "        mindd.image_vocab_count,\n",
        "        dtype=torch.long,\n",
        "        device=mindd.device\n",
        "    )\n",
        "    \n",
        "    if seed > 0: torch.manual_seed(seed)\n",
        "\n",
        "token_indices = torch.arange(IMAGE_TOKEN_COUNT, device=mindd.device)\n",
        "\n",
        "userselect=[]\n",
        "userselectN=[0]*128\n",
        "\n",
        "!rm -rf /content/steps\n",
        "!mkdir /content/steps\n",
        "!rm /content/sample_data/*.png\n",
        "\n",
        "step=0\n",
        "newprompt=True"
      ],
      "metadata": {
        "cellView": "form",
        "id": "zkSWZoJINcE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate loop<br>\n",
        "(Run this cell multiple times,<br>changing candidate_select & other settings)"
      ],
      "metadata": {
        "id": "Ooc4r-kLR0Ay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_select=0 #@param {type:\"integer\"}\n",
        "ROW_START=0 \n",
        "COL_START=7 #@param {type:\"integer\"}\n",
        "top_k= 2048 #@param {type:\"integer\"}\n",
        "temperature= 3  #@param {type:\"integer\"}\n",
        "supercondition_factor= 64 #@param {type:\"integer\"}\n",
        "\n",
        "PreviewLimit=2\n",
        "\n",
        "settings = torch.tensor(\n",
        "    [temperature, top_k, supercondition_factor], \n",
        "    dtype=torch.float32,\n",
        "    device=mindd.device\n",
        ")\n",
        "\n",
        "\n",
        "if newprompt:\n",
        "  ROW_START=0\n",
        "  COL_START=0\n",
        "elif candidate_select < candidate_count:\n",
        "  userselectN[step]+=candidate_select\n",
        "  userselect.append(dumped_seqs[candidate_select].reshape((16,16))[:,:8])\n",
        "  hcopy_dup(image_tokens,image_tokens.T[candidate_select])\n",
        "  step+=1\n",
        "else:\n",
        "  userselectN[step]+=candidate_count\n",
        "\n",
        "rumpla()\n",
        "\n",
        "dumped_seqs=image_tokens[1:].T.to('cpu').numpy().astype(np.uint16)\n",
        "with open('/content/steps/s%d.bin'%step,mode='ba+') as f:\n",
        "  dumped_seqs.tofile(f)\n",
        "\n",
        "if newprompt:\n",
        "  for n in range(candidate_count):\n",
        "    print('Init%d= '%n,end='')\n",
        "    display(showp(n))\n",
        "    print('=====================')\n",
        "else:\n",
        "  syz=len(userselect)\n",
        "  if syz > PreviewLimit:\n",
        "    tview=userselect[syz-PreviewLimit:]\n",
        "    syz=PreviewLimit\n",
        "  else:\n",
        "    tview=userselect\n",
        "  for n in range(candidate_count):\n",
        "    print('Next%d= '%n,end='')\n",
        "    display(showp2( np.hstack(tview+[dumped_seqs[n].reshape((16,16))]).reshape(256+128*syz) ))\n",
        "    dfna='-'.join(str(x) for x in userselectN[:step])\n",
        "    os.rename('/content/sample_data/000.png','/content/sample_data/'+dfna+'-'+str(n)+'.png')\n",
        "    print('=====================')\n",
        "\n",
        "\n",
        "newprompt=False"
      ],
      "metadata": {
        "cellView": "form",
        "id": "OKlJ-HJtQAuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finial<br>(last select)"
      ],
      "metadata": {
        "id": "o8i_Hg157FxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_select=2 #@param {type:\"integer\"}\n",
        "userselect.append(dumped_seqs[candidate_select].reshape((16,16)))\n",
        "userselectN[step]+=candidate_select\n",
        "step+=1\n",
        "\n",
        "\n",
        "userselectN=userselectN[:step]\n",
        "curfull=np.hstack(userselect).astype(np.uint32)\n",
        "print(userselectN)\n",
        "showp2(curfull.reshape(128+128*len(userselect)))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "5Ph5wQ827K1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "================<br>Crop & Re-decode"
      ],
      "metadata": {
        "id": "TD5z-6HnZU8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Left=219 #@param {type:\"integer\"}\n",
        "Width=355 #@param {type:\"integer\"}\n",
        "\n",
        "sta=Left>>4\n",
        "endo=(Left+Width)>>4\n",
        "showp2(curfull[:,sta:endo].reshape(16*(endo-sta)))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "8eZrnMlTZJDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tools"
      ],
      "metadata": {
        "id": "yn8qjo29URQE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pack steps"
      ],
      "metadata": {
        "id": "uu2uesW9B__A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!7z a /content/pk.7z /content/steps\n",
        "dfna='-'.join(str(x) for x in userselectN)\n",
        "os.rename('/content/pk.7z','/content/'+dfna+'.7z')"
      ],
      "metadata": {
        "id": "0oyn0L18CD_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reload ncnndec<br>\n",
        "(when memory leaks)"
      ],
      "metadata": {
        "id": "KwNojuI9NA5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "UseV=False #@param {type:\"boolean\"}\n",
        "del net\n",
        "gc.collect()\n",
        "\n",
        "net = ncnn.Net()\n",
        "net.opt.use_vulkan_compute = True\n",
        "if UseV:\n",
        "  net.load_param(\"/tmp/vq_vert.param\")\n",
        "else:\n",
        "  net.load_param(  \"/tmp/vq.param\"   )  #   \"/content/vq3x3.txt\"\n",
        "net.load_model(\"/tmp/vq.bin\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4gROBCYAM-8l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}