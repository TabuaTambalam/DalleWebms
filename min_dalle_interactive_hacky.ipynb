{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "min-dalle-interactive_hacky.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "mU7OUXRWJdNl",
        "I_F-NdAXmM0s",
        "OV3gL7ybZah1"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Min-dalle classes"
      ],
      "metadata": {
        "id": "mU7OUXRWJdNl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyGLh1yWJEdu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if not os.path.isfile('/content/once.txt'):\n",
        "  !pip install accelerate\n",
        "  !apt-get install -y libvulkan-dev libomp5\n",
        "  !pip install ncnn-vulkan\n",
        "  !mv /usr/local/lib/python3.7/dist-packages/ncnn_vulkan/*.so /usr/local/lib/python3.7/dist-packages/ncnn/\n",
        "  !pip install emoji\n",
        "  !wget -O /tmp/vq.param https://raw.githubusercontent.com/TabuaTambalam/vqqncnn/main/vq.param\n",
        "  !wget -O /tmp/vq.bin https://github.com/TabuaTambalam/vqqncnn/releases/download/0.0/vq.bin\n",
        "  !wget -O /tmp/vq_vert.param https://raw.githubusercontent.com/TabuaTambalam/vqqncnn/main/vq_vert.param\n",
        "  !git clone https://github.com/kuprel/min-dalle.git\n",
        "  !mv 'min-dalle/min_dalle' min_dalle\n",
        "  !rm -rf '/content/min-dalle'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/once.txt\n",
        "\n",
        "2@0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmwpJTT4JaTt",
        "outputId": "c75ca89f-f94c-466d-b003-38726e92090e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/once.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from threading import Thread\n",
        "import sys\n",
        "import numpy as np\n",
        "from torch import LongTensor, FloatTensor, BoolTensor,nn\n",
        "from math import sqrt\n",
        "import torch\n",
        "import torch.backends.cudnn, torch.backends.cuda\n",
        "import json\n",
        "import requests\n",
        "from typing import Iterator, List, Tuple, Dict\n",
        "from min_dalle.text_tokenizer import TextTokenizer\n",
        "from min_dalle.models import DalleBartEncoder, DalleBartDecoder\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "torch.set_num_threads(os.cpu_count())\n",
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True\n",
        "\n",
        "MIN_DALLE_REPO = 'https://huggingface.co/kuprel/min-dalle/resolve/main/'\n",
        "IMAGE_TOKEN_COUNT = 256\n",
        "\n",
        "\n",
        "from accelerate import init_empty_weights\n",
        "\n",
        "def get_keys_to_submodule(model: nn.Module) -> Dict[str, nn.Module]:\n",
        "    keys_to_submodule = {}\n",
        "    # iterate all submodules\n",
        "    for submodule_name, submodule in model.named_modules():\n",
        "        # iterate all paramters in each submobule\n",
        "        for param_name, param in submodule.named_parameters():\n",
        "            # param_name is organized as <name>.<subname>.<subsubname> ...\n",
        "            # the more we go deep in the model, the less \"subname\"s we have\n",
        "            splitted_param_name = param_name.split('.')\n",
        "            # if we have only one subname, then it means that we reach a \"leaf\" submodule, \n",
        "            # we cannot go inside it anymore. This is the actual parameter\n",
        "            is_leaf_param = len(splitted_param_name) == 1\n",
        "            if is_leaf_param:\n",
        "                # we recreate the correct key\n",
        "                key = f\"{submodule_name}.{param_name}\"\n",
        "                # we associate this key with this submodule\n",
        "                keys_to_submodule[key] = submodule\n",
        "                \n",
        "    return keys_to_submodule\n",
        "\n",
        "def load_state_dict_with_low_memory(model: nn.Module, state_dict):\n",
        "    print('======hacky load======')\n",
        "    # free up memory by placing the model in the `meta` device\n",
        "    keys_to_submodule = get_keys_to_submodule(model)\n",
        "    mste=model.state_dict()\n",
        "    for key, submodule in keys_to_submodule.items():\n",
        "        # get the valye from the state_dict\n",
        "        if key in state_dict:\n",
        "          val = state_dict[key]\n",
        "        else:\n",
        "          val = torch.ones(mste[key].shape, dtype= torch.float16)\n",
        "        # we need to substitute the parameter inside submodule, \n",
        "        # remember key is composed of <name>.<subname>.<subsubname>\n",
        "        # the actual submodule's parameter is stored inside the \n",
        "        # last subname. If key is `in_proj.weight`, the correct field if `weight`\n",
        "        param_name = key.split('.')[-1]\n",
        "        #param_dtype = getattr(submodule, param_name).dtype\n",
        "        #val = val.to(param_dtype)\n",
        "        # create a new parameter\n",
        "        new_val = torch.nn.Parameter(val)\n",
        "        setattr(submodule, param_name, new_val)\n",
        "\n",
        "\n",
        "def init2(mdl,path):\n",
        "  load_state_dict_with_low_memory(mdl, torch.load(path,map_location='cpu'))\n",
        "  mdl.requires_grad_(False).eval()\n",
        "  \n",
        "\n",
        "\n",
        "class MinDalle:\n",
        "    def __init__(\n",
        "        self,\n",
        "        models_root: str = 'pretrained',\n",
        "        dtype: torch.dtype = torch.float16,\n",
        "        device: str = None,\n",
        "        is_mega: bool = True, \n",
        "        is_reusable: bool = True,\n",
        "        is_verbose = True\n",
        "    ):\n",
        "        if torch.cuda.is_available():\n",
        "          if device == None:\n",
        "              device = 'cuda'\n",
        "        else:\n",
        "          device = 'cpu'\n",
        "          dtype=torch.float32\n",
        "\n",
        "        if is_verbose: print(\"using device\", device)\n",
        "        self.device = device\n",
        "        self.is_mega = is_mega\n",
        "        self.is_reusable = is_reusable\n",
        "        self.dtype = dtype\n",
        "        self.is_verbose = is_verbose\n",
        "        self.text_token_count = 64\n",
        "        self.layer_count = 24 if is_mega else 12\n",
        "        self.attention_head_count = 32 if is_mega else 16\n",
        "        self.embed_count = 2048 if is_mega else 1024\n",
        "        self.glu_embed_count = 4096 if is_mega else 2730\n",
        "        self.text_vocab_count = 50272 if is_mega else 50264\n",
        "        self.image_vocab_count = 16415 if is_mega else 16384\n",
        "\n",
        "        model_name = 'dalle_bart_{}'.format('mega' if is_mega else 'mini')\n",
        "        dalle_path = os.path.join(models_root, model_name)\n",
        "        vqgan_path = os.path.join(models_root, 'vqgan')\n",
        "        if not os.path.exists(dalle_path): os.makedirs(dalle_path)\n",
        "        if not os.path.exists(vqgan_path): os.makedirs(vqgan_path)\n",
        "        self.vocab_path = os.path.join(dalle_path, 'vocab.json')\n",
        "        self.merges_path = os.path.join(dalle_path, 'merges.txt')\n",
        "        self.encoder_params_path = os.path.join(dalle_path, 'encoder.pt')\n",
        "        self.decoder_params_path = os.path.join(dalle_path, 'decoder.pt')\n",
        "        self.detoker_params_path = os.path.join(vqgan_path, 'detoker.pt')\n",
        "\n",
        "        self.init_tokenizer()\n",
        "        if is_reusable:\n",
        "            self.init_encoder()\n",
        "            self.init_decoder()\n",
        "\n",
        "\n",
        "    def download_tokenizer(self):\n",
        "        if self.is_verbose: print(\"downloading tokenizer params\")\n",
        "        suffix = '' if self.is_mega else '_mini'\n",
        "        vocab = requests.get(MIN_DALLE_REPO + 'vocab{}.json'.format(suffix))\n",
        "        merges = requests.get(MIN_DALLE_REPO + 'merges{}.txt'.format(suffix))\n",
        "        with open(self.vocab_path, 'wb') as f: f.write(vocab.content)\n",
        "        with open(self.merges_path, 'wb') as f: f.write(merges.content)\n",
        "\n",
        "\n",
        "    def download_encoder(self):\n",
        "        if self.is_verbose: print(\"downloading encoder params\")\n",
        "        suffix = '' if self.is_mega else '_mini'\n",
        "        urli=MIN_DALLE_REPO + 'encoder{}.pt'.format(suffix)\n",
        "        !wget -O {self.encoder_params_path} {urli}\n",
        "\n",
        "\n",
        "    def download_decoder(self):\n",
        "        if self.is_verbose: print(\"downloading decoder params\")\n",
        "        suffix = '' if self.is_mega else '_mini'\n",
        "        urli=MIN_DALLE_REPO + 'decoder{}.pt'.format(suffix)\n",
        "        !wget -O {self.decoder_params_path} {urli}\n",
        "    \n",
        "\n",
        "    def init_tokenizer(self):\n",
        "        is_downloaded = os.path.exists(self.vocab_path)\n",
        "        is_downloaded &= os.path.exists(self.merges_path)\n",
        "        if not is_downloaded: self.download_tokenizer()\n",
        "        if self.is_verbose: print(\"intializing TextTokenizer\")\n",
        "        with open(self.vocab_path, 'r', encoding='utf8') as f:\n",
        "            vocab = json.load(f)\n",
        "        with open(self.merges_path, 'r', encoding='utf8') as f:\n",
        "            merges = f.read().split(\"\\n\")[1:-1]\n",
        "        self.tokenizer = TextTokenizer(vocab, merges)\n",
        "\n",
        "\n",
        "    def init_encoder_h(self):\n",
        "      with init_empty_weights():\n",
        "          self.encoder = DalleBartEncoder(\n",
        "              attention_head_count = self.attention_head_count,\n",
        "              embed_count = self.embed_count,\n",
        "              glu_embed_count = self.glu_embed_count,\n",
        "              text_token_count = self.text_token_count,\n",
        "              text_vocab_count = self.text_vocab_count,\n",
        "              layer_count = self.layer_count,\n",
        "              device=self.device\n",
        "          )\n",
        "      init2(self.encoder,self.encoder_params_path)\n",
        "      self.encoder = self.encoder.cuda()\n",
        "\n",
        "    def init_encoder(self):\n",
        "        is_downloaded = os.path.exists(self.encoder_params_path)\n",
        "        if not is_downloaded: self.download_encoder()\n",
        "        if self.is_verbose: print(\"initializing DalleBartEncoder\")\n",
        "        if self.device == 'cuda':\n",
        "          self.init_encoder_h()\n",
        "          return\n",
        "        self.encoder = DalleBartEncoder(\n",
        "            attention_head_count = self.attention_head_count,\n",
        "            embed_count = self.embed_count,\n",
        "            glu_embed_count = self.glu_embed_count,\n",
        "            text_token_count = self.text_token_count,\n",
        "            text_vocab_count = self.text_vocab_count,\n",
        "            layer_count = self.layer_count,\n",
        "            device=self.device\n",
        "        ).to(self.dtype).eval()\n",
        "        params = torch.load(self.encoder_params_path)\n",
        "        self.encoder.load_state_dict(params, strict=False)\n",
        "        del params\n",
        "        self.encoder = self.encoder.to(device=self.device)\n",
        "\n",
        "\n",
        "    def init_decoder_h(self):\n",
        "      with init_empty_weights():\n",
        "          self.decoder = DalleBartDecoder(\n",
        "              image_vocab_count = self.image_vocab_count,\n",
        "              attention_head_count = self.attention_head_count,\n",
        "              embed_count = self.embed_count,\n",
        "              glu_embed_count = self.glu_embed_count,\n",
        "              layer_count = self.layer_count,\n",
        "              device=self.device\n",
        "          )\n",
        "      init2(self.decoder,self.decoder_params_path)\n",
        "      self.decoder = self.decoder.cuda()\n",
        "\n",
        "    def init_decoder(self):\n",
        "        is_downloaded = os.path.exists(self.decoder_params_path)\n",
        "        if not is_downloaded: self.download_decoder()\n",
        "        if self.is_verbose: print(\"initializing DalleBartDecoder\")\n",
        "        if self.device == 'cuda':\n",
        "          self.init_decoder_h()\n",
        "          return\n",
        "        self.decoder = DalleBartDecoder(\n",
        "            image_vocab_count = self.image_vocab_count,\n",
        "            attention_head_count = self.attention_head_count,\n",
        "            embed_count = self.embed_count,\n",
        "            glu_embed_count = self.glu_embed_count,\n",
        "            layer_count = self.layer_count,\n",
        "            device=self.device\n",
        "        ).to(self.dtype).eval()\n",
        "        params = torch.load(self.decoder_params_path)\n",
        "        self.decoder.load_state_dict(params, strict=False)\n",
        "        del params\n",
        "        self.decoder = self.decoder.to(device=self.device)\n",
        "\n"
      ],
      "metadata": {
        "id": "l14d8tRdJ8ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def mkCBemb(seq):\n",
        "  with net.create_extractor() as ex:\n",
        "    ex.input(\"in0\", ncnn.Mat(seq).clone())\n",
        "    hrr, out0 = ex.extract(\"2\")\n",
        "  del ex\n",
        "  return out0\n",
        "\n",
        "def emb2img(emb):\n",
        "  with net.create_extractor() as ex:\n",
        "    ex.input(\"2\", emb)\n",
        "    hrr, out0 = ex.extract(\"out0\")\n",
        "  del ex\n",
        "  return Image.fromarray(np.array(out0).astype(np.uint8))\n",
        "\n",
        "def npmkCBemb(seq):\n",
        "  with net.create_extractor() as ex:\n",
        "    ex.input(\"in0\", ncnn.Mat(seq.astype(np.uint32)).clone())\n",
        "    hrr, out0 = ex.extract(\"2\")\n",
        "  del ex\n",
        "  return np.array(out0)\n",
        "\n",
        "\n",
        "def npemb2img(emb):\n",
        "  with net.create_extractor() as ex:\n",
        "    ex.input(\"2\", ncnn.Mat(emb).clone())\n",
        "    hrr, out0 = ex.extract(\"out0\")\n",
        "  del ex\n",
        "  return Image.fromarray(np.array(out0).astype(np.uint8))\n",
        "\n",
        "def pbla(step,scale):\n",
        "  ret=[]\n",
        "  mga=4-(4/scale)\n",
        "  k=step-1\n",
        "  for i in range(step):\n",
        "    ret.append(k+mga*( ((i**2)/k) - i ))\n",
        "  return ret\n",
        "\n",
        "\n",
        "def img_float():\n",
        "  with net.create_extractor() as ex:\n",
        "      ex.input(\"in0\", ncnn.Mat(curfull[:,sta:endo].reshape(16*(endo-sta)).astype(np.uint32)).clone())\n",
        "      hrr, out0 = ex.extract(\"252\")\n",
        "  del ex\n",
        "  return torch.FloatTensor(np.array(out0)).to(device).unsqueeze(0)\n",
        "\n",
        "\n",
        "def npimg_float(seq):\n",
        "  with net.create_extractor() as ex:\n",
        "      ex.input(\"in0\", ncnn.Mat(seq.astype(np.uint32)).clone())\n",
        "      hrr, out0 = ex.extract(\"252\")\n",
        "  del ex\n",
        "  return np.array(out0)\n",
        "\n",
        "\n",
        "def hstack(sta,n=16,crop=[]):\n",
        "  haf=[]\n",
        "  for k in range(n):\n",
        "    haf.append(dumped_seqs[sta+k].reshape((16,16))[:,:8])\n",
        "  if crop:\n",
        "    sta=crop[0]>>4\n",
        "    endo=(crop[0]+crop[1])>>4\n",
        "    return np.hstack(haf)[:,sta:endo].reshape(16*(endo-sta))\n",
        "  else:\n",
        "    return np.hstack(haf).reshape(128*n)\n",
        "    \n",
        "\n",
        "def fromsteps(liz):\n",
        "  li=len(liz)-1\n",
        "  haf=[]\n",
        "  for n in range(li):\n",
        "    haf.append(np.fromfile('/content/steps/s%d.bin'%n,dtype=np.uint16).astype(np.int32).reshape((-1,256))[liz[n]].reshape((16,16))[:,:8])\n",
        "  haf.append(np.fromfile('/content/steps/s%d.bin'%li,dtype=np.uint16).astype(np.int32).reshape((-1,256))[liz[li]].reshape((16,16)))\n",
        "  return np.hstack(haf).reshape(256 + 128*li)\n",
        "\n",
        "\n",
        "def mk3x3(idx):\n",
        "  rowz=[]\n",
        "  for y in range(3):\n",
        "    colz=[]\n",
        "    for x in range(3):\n",
        "      colz.append(dumped_seqs[idx[y*3+x]].reshape((16,16)))\n",
        "    rowz.append(np.hstack(colz).reshape(768))\n",
        "  return np.concatenate(rowz)\n",
        "\n",
        "def interpo(seq1,seq2,step=30,scale=1.21,outfmt='/content/avif/%02d.png'):\n",
        "  stp=step-1\n",
        "  divi=pbla(step,scale)\n",
        "  em1=npmkCBemb(seq1)\n",
        "  em2=npmkCBemb(seq2)\n",
        "  for i in range(step):\n",
        "    npemb2img((em1*i+em2*(stp-i))/divi[i]).save(outfmt%i)\n",
        "\n",
        "def interpoC():\n",
        "  !mkdir /content/avif\n",
        "  for n in range(candidate_count):\n",
        "    interpo(dumped_seqs[n],dumped_seqs[n+1])\n",
        "    !/tmp/ffmpeg/ffmpeg -framerate 18 -i /content/avif/%02d.png -sn -map_metadata -1 -map_chapters -1 -crf 10 -c:v libaom-av1 -aom-params enable-keyframe-filtering=0:enable-tpl-model=1 -lag-in-frames 48 -cpu-used 5 -row-mt 1 -tiles 1x1 -threads 2 -strict experimental -movflags +faststart -flags +cgop -pix_fmt yuv420p10le -c:a libopus -b:a 96k -ac 2 -f webm /content/avif/intp.webm\n",
        "    os.rename('/content/avif/intp.webm','/content/avif/v%02d.webm'%n)\n",
        "\n",
        "def showp(n, prt=False):\n",
        "  daaz=dumped_seqs[n]\n",
        "  with net.create_extractor() as ex:\n",
        "    ex.input(\"in0\", ncnn.Mat(daaz.astype(np.uint32)).clone())\n",
        "    hrr, out0 = ex.extract(\"out0\")\n",
        "  del ex\n",
        "  uz=Image.fromarray(np.array(out0).astype(np.uint8))\n",
        "  uz.save('/content/sample_data/%d.png'%n)\n",
        "  if prt:\n",
        "    display(uz)\n",
        "    return daaz\n",
        "  else:\n",
        "    return uz\n",
        "\n",
        "def showp2(seq):\n",
        "  with net.create_extractor() as ex:\n",
        "    ex.input(\"in0\", ncnn.Mat(seq.astype(np.uint32)).clone())\n",
        "    hrr, out0 = ex.extract(\"out0\")\n",
        "  del ex\n",
        "  uz=Image.fromarray(np.array(out0).astype(np.uint8))\n",
        "  uz.save('/content/sample_data/000.png')\n",
        "  return uz\n",
        "\n",
        "def hcopy(tk,left_sta=8):\n",
        "  len=16-left_sta\n",
        "  for y in range(16):\n",
        "    mae=1+y*16\n",
        "    tk[mae:mae+len]=tk[mae+left_sta:mae+16]\n",
        "\n",
        "def hcopy_dup(tk,sele,left_sta=8):\n",
        "  src=sele.expand(candidate_count,-1).T\n",
        "  len=16-left_sta\n",
        "  for y in range(16):\n",
        "    mae=1+y*16\n",
        "    tk[mae:mae+len]=src[mae+left_sta:mae+16]\n",
        "  for as0 in range(mindd.layer_count):\n",
        "    for p in range(4):\n",
        "      pkan=p*candidate_count\n",
        "      sle4=attention_state[as0][pkan+candidate_select]\n",
        "      for j in range(candidate_count):\n",
        "        attention_state[as0][pkan+j][:]=sle4[:]\n",
        "\n",
        "def hcopy_dst(src,tk,len=8):\n",
        "  for y in range(16):\n",
        "    mae=y*16\n",
        "    tk[1+mae:1+mae+len]=torch.from_numpy(src[mae:mae+len])\n",
        "\n",
        "def rumpla():\n",
        "  global attention_state\n",
        "  for row_index in range(   ROW_START   ,16):\n",
        "    print('%x:'%row_index, end='')\n",
        "    kt=16 * row_index\n",
        "    for col_index in range(COL_START,16):\n",
        "      i =  kt + col_index       \n",
        "      with torch.cuda.amp.autocast(dtype=mindd.dtype):\n",
        "          image_tokens[i + 1], attention_state = mindd.decoder.forward(\n",
        "              settings=settings,\n",
        "              attention_mask=attention_mask,\n",
        "              encoder_state=encoder_state,\n",
        "              attention_state=attention_state,\n",
        "              prev_tokens=image_tokens[i],\n",
        "              token_index=token_indices[[i]]\n",
        "          )\n",
        "\n",
        "\n",
        "def rumplaL(glist):\n",
        "  global attention_state\n",
        "  dr=16-len(glist)\n",
        "  for row_index in range(   dr   ,16):\n",
        "    print('%x:'%row_index, end='')\n",
        "    kt=16 * row_index\n",
        "    for col_index in range( glist[row_index-dr] ,16):\n",
        "      i =  kt + col_index       \n",
        "      with torch.cuda.amp.autocast(dtype=mindd.dtype):\n",
        "          image_tokens[i + 1], attention_state = mindd.decoder.forward(\n",
        "              settings=settings,\n",
        "              attention_mask=attention_mask,\n",
        "              encoder_state=encoder_state,\n",
        "              attention_state=attention_state,\n",
        "              prev_tokens=image_tokens[i],\n",
        "              token_index=token_indices[[i]]\n",
        "          )\n",
        "\n",
        "def rudallestuff():\n",
        "  if not os.path.isfile('/content/model-ru-latest.pt'):\n",
        "    !git clone https://github.com/sberbank-ai/Real-ESRGAN > /dev/null\n",
        "    !git clone https://github.com/Jack000/guided-diffusion > /dev/null\n",
        "    !pip install rudalle > /dev/null\n",
        "    !pip install -e ./guided-diffusion > /dev/null\n",
        "    !pip install -r Real-ESRGAN/requirements.txt > /dev/null\n",
        "    !wget https://huggingface.co/shonenkov/rudalle-utils/resolve/main/RealESRGAN_x2.pth > /dev/null\n",
        "    !wget https://huggingface.co/shonenkov/rudalle-utils/resolve/main/RealESRGAN_x4.pth > /dev/null\n",
        "    !wget https://dall-3.com/models/guided-diffusion/ru-dalle/model-ru-latest.pt > /dev/null\n",
        "\n",
        "def mksettings(top_k0,temperature0,supercondition_factor0):\n",
        "  return torch.tensor(\n",
        "    [temperature0, top_k0, supercondition_factor0], \n",
        "    dtype=torch.float32,\n",
        "    device=mindd.device\n",
        ")\n",
        "  \n",
        "def gen0(dr=0,dc=0):\n",
        "  global ROW_START\n",
        "  global COL_START\n",
        "  global dumped_seqs\n",
        "  ROW_START=dr\n",
        "  COL_START=dc\n",
        "  rumpla()\n",
        "  dumped_seqs=image_tokens[1:].T.to('cpu').numpy().astype(np.uint16)\n",
        "  with open(\"/content/ozv.bin\",mode='ba+') as f:\n",
        "    dumped_seqs.tofile(f)\n",
        "\n",
        "\n",
        "def gen0L(glist):\n",
        "  global dumped_seqs\n",
        "  rumplaL(glist)\n",
        "  dumped_seqs=image_tokens[1:].T.to('cpu').numpy().astype(np.uint16)\n",
        "  with open(\"/content/ozv.bin\",mode='ba+') as f:\n",
        "    dumped_seqs.tofile(f)\n",
        "\n",
        "\n",
        "def chkstz(stz):\n",
        "  lstz=len(stz)\n",
        "  if lstz < 2:\n",
        "    return True\n",
        "  if stz[1] == '':\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "\n",
        "def gen1():\n",
        "  torch.set_grad_enabled(False)\n",
        "  dr=ROW_START\n",
        "  dc=COL_START\n",
        "  useL=False\n",
        "  while os.path.isfile('/content/once.txt'):\n",
        "    if useL:\n",
        "      gen0L(glist)\n",
        "    else:\n",
        "      gen0(dr,dc)\n",
        "    try:\n",
        "      with open('/content/once.txt','rt') as f:\n",
        "        stz=f.read().replace(' ','').splitlines()\n",
        "      if stz[0] == '':\n",
        "        dr=dr\n",
        "      elif chkstz(stz):\n",
        "        useL=False\n",
        "        stz=stz[0].split('@')\n",
        "        lstz=len(stz)\n",
        "        if lstz > 0:\n",
        "          dr=int(stz[0])\n",
        "        if lstz > 1:\n",
        "          dc=int(stz[1])\n",
        "        if lstz > 2:\n",
        "          settings[1]=float(stz[2])\n",
        "          if lstz > 3:\n",
        "            settings[0]=float(stz[3])\n",
        "          if lstz > 4:\n",
        "            settings[2]=float(stz[4])\n",
        "      else:\n",
        "        glist=[int(x) for x in stz if x.isdigit()][:16]\n",
        "        useL=True\n",
        "    except:\n",
        "      dr=dr\n",
        "  os.rename('/content/-.txt','/content/once.txt')\n",
        "\n",
        "def prmp(filltoken=False):\n",
        "  global seed\n",
        "  global text_tokens\n",
        "  global image_tokens\n",
        "  global attention_mask\n",
        "  global encoder_state\n",
        "  global attention_state\n",
        "  if is_verbose: print(\"tokenizing text\")\n",
        "  tokens = mindd.tokenizer.tokenize(text, is_verbose=is_verbose)\n",
        "  if len(tokens) > mindd.text_token_count: \n",
        "      tokens = tokens[:mindd.text_token_count]\n",
        "  if is_verbose: print(\"{} text tokens\".format(len(tokens)), tokens)\n",
        "  text_tokens = np.ones((2, 64), dtype=np.int32)\n",
        "  text_tokens[0, :2] = [tokens[0], tokens[-1]]\n",
        "  text_tokens[1, :len(tokens)] = tokens\n",
        "  text_tokens = torch.tensor(\n",
        "      text_tokens, \n",
        "      dtype=torch.long, \n",
        "      device=mindd.device\n",
        "  )\n",
        "\n",
        "  if not mindd.is_reusable: mindd.init_encoder()\n",
        "  if is_verbose: print(\"encoding text tokens\")\n",
        "  with torch.cuda.amp.autocast(dtype=mindd.dtype):\n",
        "      encoder_state = mindd.encoder.forward(text_tokens)\n",
        "  if not mindd.is_reusable: del mindd.encoder\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  if not mindd.is_reusable: mindd.init_decoder()\n",
        "\n",
        "  with torch.cuda.amp.autocast(dtype=mindd.dtype):\n",
        "      expanded_indices = [0] * candidate_count + [1] * candidate_count\n",
        "      text_tokens = text_tokens[expanded_indices]\n",
        "      encoder_state = encoder_state[expanded_indices].to(token_indices.device)\n",
        "      attention_mask = text_tokens.not_equal(1).to(token_indices.device)\n",
        "      if filltoken:\n",
        "        attention_state = torch.zeros(\n",
        "            size=(\n",
        "                mindd.layer_count,\n",
        "                candidate_count * 4,\n",
        "                IMAGE_TOKEN_COUNT,\n",
        "                mindd.embed_count\n",
        "            ), \n",
        "            device=mindd.device\n",
        "        )\n",
        "        image_tokens = torch.full(\n",
        "            (IMAGE_TOKEN_COUNT + 1, candidate_count), \n",
        "            mindd.image_vocab_count,\n",
        "            dtype=torch.long,\n",
        "            device=mindd.device\n",
        "        )\n",
        "      \n",
        "      if seed == 0:\n",
        "        seed=random.randint(0, 2**32)\n",
        "        print('rndseed: '+str(seed)) \n",
        "      torch.manual_seed(seed)\n",
        "        \n",
        "\n",
        "newstart=True"
      ],
      "metadata": {
        "id": "Ujkb7IY3NSDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PlayGround<br>\n",
        "User-select outpainting (mini model)\n",
        "![mini](https://github.com/TabuaTambalam/DalleWebms/blob/main/docs/sbk-mini.png?raw=true)\n",
        "<br>\n",
        "mega model:\n",
        "![mega](https://github.com/TabuaTambalam/DalleWebms/blob/main/docs/sbk-mega.png?raw=true)"
      ],
      "metadata": {
        "id": "mDKKp_RiJ3Hs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "UseMega=True #@param {type:\"boolean\"}\n",
        "PrepareRuDalleStuff=False #@param {type:\"boolean\"}\n",
        "if newstart:\n",
        "  mindd = MinDalle(is_mega=UseMega, is_reusable=True)\n",
        "  token_indices = mindd.decoder.token_indices\n",
        "  import ncnn\n",
        "  import gc\n",
        "  net = ncnn.Net()\n",
        "  net.opt.use_vulkan_compute = False\n",
        "  net.load_param(  \"/tmp/vq.param\"   )  #   \"/content/vq3x3.txt\"\n",
        "  net.load_model(\"/tmp/vq.bin\")\n",
        "  newstart=False\n",
        "  !nvidia-smi\n",
        "if PrepareRuDalleStuff:\n",
        "  t1 = Thread(target = rudallestuff)\n",
        "  a1 = t1.start()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "NGoqCk3nLQRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make a prompt"
      ],
      "metadata": {
        "id": "bvizR5gUOACG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text =\"desert oasis merchant market high fantasy book cover painting\" #@param {type:\"string\"}\n",
        "candidate_count =  4#@param {type:\"integer\"}\n",
        "seed= 776677  #@param {type:\"integer\"}\n",
        "log_everything = False #@param {type:\"boolean\"}\n",
        "\n",
        "'''\n",
        "555\n",
        "'''\n",
        "\n",
        "is_verbose=False\n",
        "\n",
        "\n",
        "prmp(True)\n",
        "\n",
        "\n",
        "userselect=[]\n",
        "userselectN=[0]*128\n",
        "\n",
        "!rm -rf /content/steps\n",
        "!mkdir /content/steps\n",
        "!rm /content/sample_data/*.png\n",
        "\n",
        "if log_everything:\n",
        "  with open('/content/steps/prompt.txt','wt') as f:\n",
        "    f.write(text+'\\ntokens='+str(tokens))\n",
        "  torch.save(attention_mask, '/content/steps/attention_mask.pt')\n",
        "  torch.save(encoder_state, '/content/steps/encoder_state.pt')\n",
        "\n",
        "\n",
        "ROW_START =0\n",
        "COL_START =0\n",
        "step=0\n",
        "newprompt=True"
      ],
      "metadata": {
        "cellView": "form",
        "id": "zkSWZoJINcE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "change the prompt midway (optional, probably won't produce anything you want)"
      ],
      "metadata": {
        "id": "2jX0mpoTUgte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text =\"Change prompt here (without reset step)\" #@param {type:\"string\"}\n",
        "seed= 889988  #@param {type:\"integer\"}\n",
        "prmp()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SYJ8hrtgG3XG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate loop<br>\n",
        "(Run this cell multiple times,<br>changing candidate_select & other settings)"
      ],
      "metadata": {
        "id": "Ooc4r-kLR0Ay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_select  = 1 #@param {type:\"integer\"}\n",
        "top_k = 2048 #@param {type:\"integer\"}\n",
        "temperature = 3  #@param {type:\"integer\"}\n",
        "supercondition_factor = 64 #@param {type:\"integer\"}\n",
        "\n",
        "#better not change these\n",
        "ROW_START =0 \n",
        "COL_START =7 #@param {type:\"integer\"}\n",
        "PreviewLimit=2\n",
        "\n",
        "settings = torch.tensor(\n",
        "    [temperature, top_k, supercondition_factor], \n",
        "    dtype=torch.float32,\n",
        "    device=mindd.device\n",
        ")\n",
        "\n",
        "\n",
        "if newprompt:\n",
        "  ROW_START=0\n",
        "  COL_START=0\n",
        "elif candidate_select < candidate_count:\n",
        "  userselectN[step]+=candidate_select\n",
        "  userselect.append(dumped_seqs[candidate_select].reshape((16,16))[:,:8])\n",
        "  hcopy_dup(image_tokens,image_tokens.T[candidate_select])\n",
        "  step+=1\n",
        "else:\n",
        "  userselectN[step]+=candidate_count\n",
        "  if step == 0:\n",
        "    newprompt=True\n",
        "    ROW_START=0\n",
        "    COL_START=0\n",
        "\n",
        "\n",
        "rumpla()\n",
        "\n",
        "fnapfix='/content/steps/s%d'%step\n",
        "dumped_seqs=image_tokens[1:].T.to('cpu').numpy().astype(np.uint16)\n",
        "with open(fnapfix+'.bin',mode='ba+') as f:\n",
        "  dumped_seqs.tofile(f)\n",
        "\n",
        "\n",
        "if log_everything:\n",
        "  torch.save(settings, fnapfix+'_settings.pt')\n",
        "  #torch.save(attention_state, fnapfix+'_attention_state.pt')\n",
        "\n",
        "print('\\n')\n",
        "if newprompt:\n",
        "  newprompt=False\n",
        "  for n in range(candidate_count):\n",
        "    print('=====Init%d====='%n,end='')\n",
        "    display(showp(n))\n",
        "else:\n",
        "  syz=len(userselect)\n",
        "  if syz > PreviewLimit:\n",
        "    tview=userselect[syz-PreviewLimit:]\n",
        "    syz=PreviewLimit\n",
        "  else:\n",
        "    tview=userselect\n",
        "  bdfna='-'.join(str(x) for x in userselectN[:step])\n",
        "  for n in range(candidate_count):\n",
        "    print('=====Next%d====='%n,end='')\n",
        "    display(showp2( np.hstack(tview+[dumped_seqs[n].reshape((16,16))]).reshape(256+128*syz) ))\n",
        "    dfna='/content/sample_data/'+bdfna+'-'+str(n)+'.png'\n",
        "    if os.path.isfile(dfna):\n",
        "      os.remove(dfna)\n",
        "    os.rename('/content/sample_data/000.png',dfna)\n",
        "\n",
        "\n",
        "newprompt=False"
      ],
      "metadata": {
        "cellView": "form",
        "id": "OKlJ-HJtQAuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finish gen"
      ],
      "metadata": {
        "id": "o8i_Hg157FxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_select=1 #@param {type:\"integer\"}\n",
        "userselect.append(dumped_seqs[candidate_select].reshape((16,16)))\n",
        "userselectN[step]+=candidate_select\n",
        "step+=1\n",
        "\n",
        "\n",
        "userselectN=userselectN[:step]\n",
        "curfull=np.hstack(userselect).astype(np.uint32)\n",
        "print(userselectN)\n",
        "showp2(curfull.reshape(128+128*len(userselect)))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "5Ph5wQ827K1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "================<br>Crop & Re-decode"
      ],
      "metadata": {
        "id": "TD5z-6HnZU8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Left=219 #@param {type:\"integer\"}\n",
        "Width=355 #@param {type:\"integer\"}\n",
        "\n",
        "sta=Left>>4\n",
        "endo=(Left+Width)>>4\n",
        "showp2(curfull[:,sta:endo].reshape(16*(endo-sta)))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "8eZrnMlTZJDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tools"
      ],
      "metadata": {
        "id": "yn8qjo29URQE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reload ncnndec<br>\n",
        "(when memory leaks, and will make ncnn faster on GPU somethimes)<br>\n",
        "DO NOT use height_free for this interactive demo"
      ],
      "metadata": {
        "id": "KwNojuI9NA5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "UseVulkan=False #@param {type:\"boolean\"}\n",
        "Height_free=False #@param {type:\"boolean\"}\n",
        "\n",
        "try:\n",
        "  del net\n",
        "except:\n",
        "  import ncnn\n",
        "  import gc\n",
        "  print('load only')\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "net = ncnn.Net()\n",
        "net.opt.use_vulkan_compute = UseVulkan\n",
        "net.opt.use_fp16_packed = False\n",
        "net.opt.use_fp16_storage = False\n",
        "net.opt.use_fp16_arithmetic = False\n",
        "if Height_free:\n",
        "  net.load_param(\"/tmp/vq_vert.param\")\n",
        "else:\n",
        "  net.load_param(  \"/tmp/vq.param\"   )  #   \"/content/vq3x3.txt\"\n",
        "net.load_model(\"/tmp/vq.bin\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4gROBCYAM-8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pack steps"
      ],
      "metadata": {
        "id": "uu2uesW9B__A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!7z a /content/pk.7z /content/steps\n",
        "if len(userselectN) < 128:\n",
        "  dfna=','.join(str(x) for x in userselectN)\n",
        "  os.rename('/content/pk.7z','/content/'+dfna+'  .7z')"
      ],
      "metadata": {
        "id": "0oyn0L18CD_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!7z x pk.7z\n",
        "curfull=fromsteps([ 6,5,1,1,2,2,2,2,2,2,1,2    ])\n",
        "display(showp2(curfull))\n",
        "curfull=curfull.reshape((16,-1))"
      ],
      "metadata": {
        "id": "5p5mXcfjHqSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%html\n",
        "<html><body><style>\n",
        "FL { \n",
        "position: relative;\n",
        "display: inline-block;\n",
        "background-color: #808;\n",
        "width: 1024px;\n",
        "min-height: 1024px;\n",
        "}\n",
        "\n",
        "NG { \n",
        "min-width: 64px;\n",
        "min-height: 64px;\n",
        "position: relative;\n",
        "display: inline-block;\n",
        "text-align: center;\n",
        "line-height : 60px;\n",
        "font-weight: bold;\n",
        "font-size: 140%;\n",
        "color: #f00;\n",
        "}\n",
        "\n",
        ".ckb {\n",
        "\tposition: absolute;\n",
        "\ttop: 0px;\n",
        "\tleft: 0px;\n",
        "\tmin-width: 1024px;\n",
        "\t-webkit-mask: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAQAAAAEAAQMAAABmvDolAAAABlBMVEUAAAD///+l2Z/dAAAAAXRSTlMAQObYZgAAAGhJREFUaN7t2rENgDAMRUFvyTgMxDwswRCEDT5IpEiis9voCuuVqfYyXx7scavdaQldhTx1HXnr3PLqYS5BDwQ9EPRA0ANBDwQ9EPQw2iXXEPRA0ANBDwQ9EPRA0ANBD6Ndcg3h5++FB6kp5Ka9HRXlAAAAAElFTkSuQmCC') 0% 0% / 100%;\n",
        "}\n",
        "</style>\n",
        "<FL><img class=\"ckb\" src=\"https://localhost:8233/-1.png\"></FL><br>\n",
        "</body>\n",
        "<script>\n",
        "var nfo=new Array(256); \n",
        "// Token space visualizer =False  #@param {type:\"boolean\"}\n",
        "\n",
        "function asstok()\n",
        "{\n",
        "\tvar atr=this.value.split(' ').filter(element => element);\n",
        "\tfor(var i=0;i<256;i++)\n",
        "\t{\n",
        "\t\tnfo[i].innerHTML=atr[i];\n",
        "\t}\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "function setup()\n",
        "{\n",
        "\tvar bz=document.getElementsByTagName('FL')[0];\n",
        "\n",
        "\tfor(var y=0;y<16;y++)\n",
        "\t{\n",
        "\t\tvar yp=y*16;\n",
        "\t\tfor(var x=0;x<16;x++)\n",
        "\t\t{\n",
        "\t\t\tvar idx=yp+x;\n",
        "\t\t\tvar ngp=document.createElement('NG');\n",
        "\t\t\tngp.innerHTML=idx;\n",
        "\t\t\tnfo[idx]=ngp;\n",
        "\t\t\tbz.appendChild(ngp);\n",
        "\t\t\t\n",
        "\t\t}\n",
        "\t\t\n",
        "\t}\n",
        "\n",
        "\tvar txta=document.createElement('textarea');\n",
        "\ttxta.cols=150;\n",
        "\ttxta.rows=30;\n",
        "\tdocument.body.appendChild(txta);\n",
        "\ttxta.ondblclick=asstok;\n",
        "\t\n",
        "\n",
        "\t\n",
        "\n",
        "}\n",
        "setup()\n",
        "\n",
        "</script></html>"
      ],
      "metadata": {
        "cellView": "form",
        "id": "tqHxT9pn5nwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import output\n",
        "!nohup python3 -m http.server -d /content/sample_data/ 8233 > izh.txt &\n",
        "#output.serve_kernel_port_as_window(8233, path='')"
      ],
      "metadata": {
        "id": "fqLcB2rpEJBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Omake: Lock toprows/leftcols generate with candidates\n",
        "Lock top rows: \n",
        "![interpo mk2plus](https://github.com/TabuaTambalam/DalleWebms/blob/main/docs/room.gif?raw=true)\n",
        "mini model: \n",
        "![interpo superman, mini model](https://github.com/TabuaTambalam/DalleWebms/blob/main/docs/interpo_mini.gif?raw=true)\n"
      ],
      "metadata": {
        "id": "efxj_wzv3rwf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lock top rows"
      ],
      "metadata": {
        "id": "i123RgUvHv4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_select=11 #@param {type:\"integer\"}\n",
        "top_k = 2048 #@param {type:\"integer\"}\n",
        "temperature = 3  #@param {type:\"integer\"}\n",
        "supercondition_factor = 64 #@param {type:\"integer\"}\n",
        "\n",
        "settings = torch.tensor(\n",
        "    [temperature, top_k, supercondition_factor], \n",
        "    dtype=torch.float32,\n",
        "    device=mindd.device\n",
        ")\n",
        "\n",
        "stepping = 2 #@param {type:\"integer\"}\n",
        "r_ROW_START =-1 #@param {type:\"integer\"}\n",
        "r_COL_START =-1 #@param {type:\"integer\"}\n",
        "\n",
        "if ROW_START > 15:\n",
        "  print('No more')\n",
        "  assert False\n",
        "\n",
        "if newprompt:\n",
        "  ROW_START=0\n",
        "  COL_START=0\n",
        "else:\n",
        "  if r_ROW_START >=0:\n",
        "    ROW_START=r_ROW_START>>4\n",
        "  if r_COL_START >=0:\n",
        "    COL_START=r_COL_START>>4\n",
        "  if candidate_select < candidate_count:\n",
        "    image_tokens[:]=image_tokens.T[candidate_select].expand(candidate_count,-1).T[:]\n",
        "    for as0 in range(mindd.layer_count):\n",
        "      for p in range(4):\n",
        "        pkan=p*candidate_count\n",
        "        sle4=attention_state[as0][pkan+candidate_select]\n",
        "        for j in range(candidate_count):\n",
        "          attention_state[as0][pkan+j][:]=sle4[:]\n",
        "    fna='/content/sample_data/'+str(candidate_select)\n",
        "    dfna=fna+'h'+str(ROW_START<<4)+'.png'\n",
        "    if os.path.isfile(dfna):\n",
        "      os.remove(dfna)\n",
        "    os.rename(fna+'.png',dfna)\n",
        "  else:\n",
        "    ROW_START-=stepping\n",
        "\n",
        "\n",
        "rumpla()\n",
        "ROW_START+=stepping\n",
        "  \n",
        "\n",
        "\n",
        "dumped_seqs=image_tokens[1:].T.to('cpu').numpy().astype(np.uint16)\n",
        "with open(\"/content/ozv.bin\",mode='ba+') as f:\n",
        "  dumped_seqs.tofile(f)\n",
        "\n",
        "\n",
        "msg='=== U rike dis top'+str(ROW_START<<4)+'x256? %d===='\n",
        "\n",
        "print('\\n')\n",
        "newprompt=False\n",
        "for n in range(candidate_count):\n",
        "  print(msg%n,end='')\n",
        "  display(showp(n))\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mU7SE_sQ33Vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load external seq (probably won't produce anything you want)"
      ],
      "metadata": {
        "id": "pDXO8uOD4Wmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://github.com/TabuaTambalam/DalleWebms/releases/download/0.1/ozv.bin"
      ],
      "metadata": {
        "id": "l1crl7Wo4V2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_bin =\"/content/ozv.bin\" #@param {type:\"string\"}\n",
        "seq_idx =  21#@param {type:\"integer\"}\n",
        "seq=np.fromfile(seq_bin,dtype=np.uint16).astype(np.int64).reshape((-1,256))[seq_idx]\n",
        "image_tokens[1:]=torch.from_numpy(seq).expand(candidate_count,-1).T\n",
        "newprompt=False\n",
        "showp2(seq)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "op0fipuA4oBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "P.S. Actually has no knowledge about pytorch,<br>\n",
        "hope someone can make this shorter/less operates/less copying:<br>\n",
        "```\n",
        "image_tokens[ : ]=image_tokens.T[candidate_select].expand(candidate_count,-1).T[ : ]\n",
        "```\n",
        "\n",
        "And this less loop:\n",
        "```\n",
        "  for as0 in range(mindd.layer_count):\n",
        "    for p in range(4):\n",
        "      pkan=p*candidate_count\n",
        "      sle4=attention_state[as0][pkan+candidate_select]\n",
        "      for j in range(candidate_count):\n",
        "        attention_state[as0][pkan+j][:]=sle4[:]\n",
        "```\n"
      ],
      "metadata": {
        "id": "Dobt8Lhb_koK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lock left cols"
      ],
      "metadata": {
        "id": "DKUnPTpIG3uI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_select=0 #@param {type:\"integer\"}\n",
        "top_k = 2048 #@param {type:\"integer\"}\n",
        "temperature = 3  #@param {type:\"integer\"}\n",
        "supercondition_factor = 64 #@param {type:\"integer\"}\n",
        "\n",
        "settings = torch.tensor(\n",
        "    [temperature, top_k, supercondition_factor], \n",
        "    dtype=torch.float32,\n",
        "    device=mindd.device\n",
        ")\n",
        "\n",
        "stepping = 2 #@param {type:\"integer\"}\n",
        "r_ROW_START =-1 #@param {type:\"integer\"}\n",
        "r_COL_START =-1 #@param {type:\"integer\"}\n",
        "\n",
        "if COL_START > 15:\n",
        "  print('No more')\n",
        "  assert False\n",
        "\n",
        "\n",
        "\n",
        "if newprompt:\n",
        "  ROW_START=0\n",
        "  COL_START=0\n",
        "else:\n",
        "  if r_ROW_START >=0:\n",
        "    ROW_START=r_ROW_START>>4\n",
        "  if r_COL_START >=0:\n",
        "    COL_START=r_COL_START>>4\n",
        "  if candidate_select < candidate_count:\n",
        "    image_tokens[:]=image_tokens.T[candidate_select].expand(candidate_count,-1).T[:]\n",
        "    for as0 in range(mindd.layer_count):\n",
        "      for p in range(4):\n",
        "        pkan=p*candidate_count\n",
        "        sle4=attention_state[as0][pkan+candidate_select]\n",
        "        for j in range(candidate_count):\n",
        "          attention_state[as0][pkan+j][:]=sle4[:]\n",
        "    fna='/content/sample_data/'+str(candidate_select)\n",
        "    dfna=fna+'w'+str(COL_START<<4)+'.png'\n",
        "    if os.path.isfile(dfna):\n",
        "      os.remove(dfna)\n",
        "    os.rename(fna+'.png',dfna)\n",
        "  else:\n",
        "    COL_START-=stepping\n",
        "\n",
        "\n",
        "\n",
        "rumpla()\n",
        "COL_START+=stepping\n",
        "\n",
        "\n",
        "dumped_seqs=image_tokens[1:].T.to('cpu').numpy().astype(np.uint16)\n",
        "with open(\"/content/ozv.bin\",mode='ba+') as f:\n",
        "  dumped_seqs.tofile(f)\n",
        "\n",
        "\n",
        "msg='=== U rike dis left256x'+str(COL_START<<4)+'? %d===='\n",
        "\n",
        "print('\\n')\n",
        "newprompt=False\n",
        "for n in range(candidate_count):\n",
        "  print(msg%n,end='')\n",
        "  display(showp(n))\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cz0Zj3JiGKqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Infinite similar gen"
      ],
      "metadata": {
        "id": "I_F-NdAXmM0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_select=0 #@param {type:\"integer\"}\n",
        "top_k = 2048 #@param {type:\"integer\"}\n",
        "temperature = 3  #@param {type:\"integer\"}\n",
        "supercondition_factor = 64 #@param {type:\"integer\"}\n",
        "\n",
        "\n",
        "r_ROW_START =35 #@param {type:\"integer\"}\n",
        "r_COL_START =0 #@param {type:\"integer\"}\n",
        "\n",
        "settings=mksettings(top_k,temperature,supercondition_factor)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if newprompt:\n",
        "  newprompt=False\n",
        "  gen0()\n",
        "  for n in range(candidate_count):\n",
        "    print('init%d'%n,end='')\n",
        "    display(showp(n))\n",
        "elif candidate_select < candidate_count:\n",
        "  image_tokens[:]=image_tokens.T[candidate_select].expand(candidate_count,-1).T[:]\n",
        "  for as0 in range(mindd.layer_count):\n",
        "    for p in range(4):\n",
        "      pkan=p*candidate_count\n",
        "      sle4=attention_state[as0][pkan+candidate_select]\n",
        "      for j in range(candidate_count):\n",
        "        attention_state[as0][pkan+j][:]=sle4[:]\n",
        "  ROW_START=r_ROW_START>>4\n",
        "  COL_START=r_COL_START>>4\n",
        "  print(\"infinite gen started in thread\")\n",
        "  t1 = Thread(target = gen1)\n",
        "  a1 = t1.start()\n",
        "else:\n",
        "  gen0()\n",
        "  for n in range(candidate_count):\n",
        "    print('init%d'%n,end='')\n",
        "    display(showp(n))\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "hD7bsI-nrvfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(infinite gen) Show last (0~candidate_count) gen"
      ],
      "metadata": {
        "id": "w-dJ0RcpC4HJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "showp(-1, True)"
      ],
      "metadata": {
        "id": "9EEIcwV203wS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(infinite gen) change top_k .etc midway<br>\n",
        "change the prompt midway with that `889988` one<br>\n",
        "rename `once.txt` to `-.txt` to stop the infinite gen<br>\n",
        "change `(skip top n rows)@(skip left n cols)` in token space in `once.txt` e.g. `4@0` or `0@2`"
      ],
      "metadata": {
        "id": "CUOXyS7WDEWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_k = 2048 #@param {type:\"integer\"}\n",
        "temperature = 3  #@param {type:\"integer\"}\n",
        "supercondition_factor = 64 #@param {type:\"integer\"}\n",
        "settings[0]=temperature\n",
        "settings[1]=top_k\n",
        "settings[2]=supercondition_factor"
      ],
      "metadata": {
        "cellView": "form",
        "id": "qbDxHRaYDPAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RuDalle's ESRGAN"
      ],
      "metadata": {
        "id": "OV3gL7ybZah1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rudallestuff()\n",
        "sys.path.append('./Real-ESRGAN')\n",
        "sys.path.append('./guided-diffusion')\n",
        "\n",
        "UseGuidedDiffusion=True #@param {type:\"boolean\"}\n",
        "save_path='/content/sample_data/'  #@param {type:\"string\"}\n",
        "\n",
        "import os\n",
        "from realesrgan import RealESRGAN\n",
        "from torch.nn import functional as F\n",
        "from rrdbnet_arch import RRDBNet\n",
        "from utils_sr import *\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('device:', device)\n",
        "\n",
        "if UseGuidedDiffusion:\n",
        "  torch.set_grad_enabled(True)\n",
        "  model_params = {\n",
        "      'attention_resolutions': '32, 16, 8',\n",
        "      'class_cond': False,\n",
        "      'diffusion_steps': 2000,\n",
        "      'rescale_timesteps': True,\n",
        "      'timestep_respacing': '', #'ddim2000',\n",
        "      'image_size': 256,\n",
        "      'learn_sigma': True,\n",
        "      'noise_schedule': 'linear',\n",
        "      'num_channels': 256,\n",
        "      'num_head_channels': 64,\n",
        "      'num_res_blocks': 2,\n",
        "      'resblock_updown': True,\n",
        "      'use_fp16': True,\n",
        "      'use_scale_shift_norm': True,\n",
        "      'emb_condition': True\n",
        "  }\n",
        "  import PIL\n",
        "  from einops import rearrange\n",
        "  from rudalle import get_vae\n",
        "  from guided_diffusion.script_util import create_model_and_diffusion, model_and_diffusion_defaults\n",
        "  import torch.nn as nn\n",
        "  from torch.nn import functional as F\n",
        "  from torchvision import transforms as T\n",
        "  from torchvision.transforms import functional as TF\n",
        "  import torchvision\n",
        "\n",
        "  resr = RealESRGAN(device, scale=2)\n",
        "  resr.load_weights('/content/RealESRGAN_x2.pth')\n",
        "\n",
        "\n",
        "  if not torch.cuda.is_available():\n",
        "    model_params['use_fp16']=False\n",
        "\n",
        "  model_config = model_and_diffusion_defaults()\n",
        "  model_config.update(model_params)\n",
        "\n",
        "  diffusion_decoder_model, diffusion = create_model_and_diffusion(**model_config)\n",
        "  diffusion_decoder_model.load_state_dict(torch.load('/content/model-ru-latest.pt', map_location='cpu'))\n",
        "  diffusion_decoder_model.requires_grad_(False).eval().to(device)\n",
        "\n",
        "  for name, param in diffusion_decoder_model.named_parameters():\n",
        "      if 'qkv' in name or 'norm' in name or 'proj' in name:\n",
        "          param.requires_grad_()\n",
        "\n",
        "  if model_config['use_fp16']:\n",
        "      diffusion_decoder_model.convert_to_fp16()\n",
        "\n",
        "  vae = get_vae(dwt=False).to(device)\n",
        "  embedding = nn.Embedding.from_pretrained(vae.model.quantize.embed.weight)\n",
        "else:\n",
        "  resr = RealESRGAN(device, scale=4)\n",
        "  resr.load_weights('/content/RealESRGAN_x4.pth')\n",
        "\n",
        "\n",
        "def preprocess(img, target_image_size=256):\n",
        "    s = min(img.size)\n",
        "    if s < target_image_size:\n",
        "        raise ValueError(f'min dim for image {s} < {target_image_size}')\n",
        "    r = target_image_size / s\n",
        "    s = (round(r * img.size[1]), round(r * img.size[0]))\n",
        "    img = TF.resize(img, s, interpolation=PIL.Image.LANCZOS)\n",
        "    img = TF.center_crop(img, output_size=2 * [target_image_size])\n",
        "    img = torch.unsqueeze(T.ToTensor()(img), 0)\n",
        "    return img\n",
        "\n",
        "def postesrgan(res):\n",
        "  sr_image = res.permute((0,2,3,1)).clamp_(0, 1).cpu()\n",
        "  sr_img=Image.fromarray((sr_image[0].numpy()*255).astype(np.uint8))\n",
        "  sr_img.save(save_path+'000_x4.png')\n",
        "  return sr_img\n",
        "\n",
        "def Thrd_show(pp,fna):\n",
        "  ymg = Image.fromarray(((pp.transpose((1,2,0))+1)*127.5).clip(0,255).astype(np.uint8))\n",
        "  ymg.save(save_path+fna+'.png')\n",
        "  display(ymg)\n",
        "  np.savez(save_path+fna+'.npz',pp)\n",
        "\n",
        "def diffusion_decode(z,seed=None,stop_at = 1000,logstep=50):\n",
        "    res = []\n",
        "    image_index = 1          \n",
        "    diffusion_batch_size = 1\n",
        "    diffusion_num_batches = 1\n",
        "    \n",
        "    cblen=z.shape[1]\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "  \n",
        "    \n",
        "\n",
        "    target_embeds, weights = [], []\n",
        "    img_seq = z.to(device)\n",
        "\n",
        "    b, n = 1, cblen # z.shape\n",
        "    z = embedding(img_seq)\n",
        "\n",
        "    embeds = rearrange(z, 'b (h w) c -> b c h w', h = 32)\n",
        "    side_y=embeds.shape[2]*16\n",
        "    side_x=embeds.shape[3]*16\n",
        "    embeds = embeds.repeat(diffusion_batch_size, 1, 1, 1)\n",
        "    \n",
        "\n",
        "    cur_t = None\n",
        "\n",
        "    if model_config['timestep_respacing'].startswith('ddim'):\n",
        "        sample_fn = diffusion.ddim_sample_loop_progressive\n",
        "    else:\n",
        "        sample_fn = diffusion.p_sample_loop_progressive\n",
        "\n",
        "    for i in range(diffusion_num_batches):\n",
        "        cur_t = diffusion.num_timesteps - 1\n",
        "\n",
        "        samples = sample_fn(\n",
        "            diffusion_decoder_model,\n",
        "            (diffusion_batch_size, 3, side_y, side_x),\n",
        "            clip_denoised=False,\n",
        "            model_kwargs={'image_embeds': embeds},\n",
        "            cond_fn=None,\n",
        "            progress=True,\n",
        "        )\n",
        "        for j, sample in enumerate(samples):\n",
        "          cur_t -= 1\n",
        "          isending=(cur_t == -1 or j > stop_at)\n",
        "          if j % logstep == 0 or isending:\n",
        "              for k, image in enumerate(sample['pred_xstart']):\n",
        "                t1=Thread(target=Thrd_show, args=(image.cpu().numpy(),'%d_%d'%(j,k),)) \n",
        "                a1 = t1.start()\n",
        "                if isending:\n",
        "                  res.append(image.add(1).div(2))\n",
        "              if isending:\n",
        "                return res\n",
        "    return res"
      ],
      "metadata": {
        "id": "V3kuHQxbZguj",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "works with `curfull` loaded"
      ],
      "metadata": {
        "id": "w0uO-7fiTCx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Left=0 #@param {type:\"integer\"}\n",
        "Width=640 #@param {type:\"integer\"}\n",
        "\n",
        "sta=Left>>4\n",
        "endo=(Left+Width)>>4\n",
        "\n",
        "with torch.no_grad():\n",
        "  res = resr.model(img_float())\n",
        "\n",
        "postesrgan(res)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "oiDnUpvsS_bW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "works with any image"
      ],
      "metadata": {
        "id": "JBg3-qb0TLDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FileName='/content/sample_data/7-21-11.png' #@param {type:\"string\"}\n",
        "lr_image=np.array(Image.open(FileName).convert('RGB'))\n",
        "batch_size=4\n",
        "patches_size=200\n",
        "padding=28\n",
        "pad_size=28\n",
        "\n",
        "scale = resr.scale\n",
        "device = resr.device\n",
        "lr_image = np.array(lr_image)\n",
        "lr_image = pad_reflect(lr_image, pad_size)\n",
        "\n",
        "patches, p_shape = split_image_into_overlapping_patches(lr_image, patch_size=patches_size, \n",
        "                                                        padding_size=padding)\n",
        "img = torch.FloatTensor(patches/255).permute((0,3,1,2)).to(device).detach()\n",
        "\n",
        "with torch.no_grad():\n",
        "    res = resr.model(img[0:batch_size])\n",
        "    for i in range(batch_size, img.shape[0], batch_size):\n",
        "        res = torch.cat((res, resr.model(img[i:i+batch_size])), 0)\n",
        "\n",
        "sr_image = res.permute((0,2,3,1)).clamp_(0, 1).cpu()\n",
        "np_sr_image = sr_image.numpy()\n",
        "\n",
        "padded_size_scaled = tuple(np.multiply(p_shape[0:2], scale)) + (3,)\n",
        "scaled_image_shape = tuple(np.multiply(lr_image.shape[0:2], scale)) + (3,)\n",
        "np_sr_image = stich_together(np_sr_image, padded_image_shape=padded_size_scaled,\n",
        "                        target_shape=scaled_image_shape, padding_size=padding * scale)\n",
        "sr_img = (np_sr_image*255).astype(np.uint8)\n",
        "sr_img = unpad_image(sr_img, pad_size*scale)\n",
        "sr_img=Image.fromarray(sr_img)\n",
        "sr_img.save(FileName[:-4]+\"_x4.png\")\n",
        "sr_img"
      ],
      "metadata": {
        "id": "_nwF25daZjOz",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rudalle GD"
      ],
      "metadata": {
        "id": "lu8LCmIB7IL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Left=0 #@param {type:\"integer\"}\n",
        "Width=512 #@param {type:\"integer\"}\n",
        "StopAt=2000 #@param {type:\"integer\"}\n",
        "LogAt=50 #@param {type:\"integer\"}\n",
        "\n",
        "UseSeq=-999999  #@param {type:\"integer\"}\n",
        "\n",
        "if UseSeq > -999990:\n",
        "  codebooks = vae.get_codebook_indices(torch.FloatTensor(npimg_float(dumped_seqs[UseSeq])).to(device).unsqueeze(0))\n",
        "else:\n",
        "  sta=Left>>4\n",
        "  endo=(Left+Width)>>4\n",
        "  codebooks = vae.get_codebook_indices(img_float())\n",
        "resGD = diffusion_decode(codebooks,seed=None,stop_at=StopAt,logstep=LogAt)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jKRttFYV7LH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "postesrgan(resr.model(resGD[-1].unsqueeze(0)))"
      ],
      "metadata": {
        "id": "H5fT_7Ded3rS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "release ram if necessary"
      ],
      "metadata": {
        "id": "pyuQeYW77kVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del mindd\n",
        "del net\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "4e7qXu0o7mr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tool: load dumped_seqs"
      ],
      "metadata": {
        "id": "lD7z-5LL1CHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seqbin='ozv_lock.bin' #@param {type:\"string\"}\n",
        "dumped_seqs=np.fromfile(seqbin,dtype=np.uint16).astype(np.int32).reshape((-1,256))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Pi7ICroc1Lr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tool: test cb"
      ],
      "metadata": {
        "id": "f2smpCxw7qMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "torchvision.transforms.functional.to_pil_image(vae.decode(codebooks)[0]).convert('RGB')"
      ],
      "metadata": {
        "id": "TMIOUooG7n2r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}